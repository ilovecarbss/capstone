timestamp,raw,final_label,severity,owner_team,tags,cluster_id,t5_template,cluster_template,auto_label
2025-09-03T07:52:15.236,"[2025-09-03T07:52:15.236] Gres Name=gpu Type=l4 Count=8 Flags=HAS_FILE,HAS_TYPE,ENV_NVML,ENV_RSMI,ENV_ONEAPI,ENV_OPENCL,ENV_DEFAULT",gpu_inventory_update,info,infra,"gpu,l4,inventory",1,[2025-09-03T07:52:15.236] Gres [2025-09-03T07:52:15.236] Gres [2025-09-03T07:52:15.236] Gres [2025-09-03T07:52:,<*> Gres Name=gpu Type=l4 <*> <*>,gpu_inventory_update
2025-09-03T07:52:15.241,"[2025-09-03T07:52:15.241] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,HAS_TYPE,ENV_NVML,ENV_RSMI,ENV_ONEAPI,ENV_OPENCL,ENV_DEFAULT",gpu_inventory_update,info,infra,"gpu,l4,inventory",1,[2025-09-03T07:52:15.241] Gres [2025-09-03T07:52:15.241] Gres [2025-09-03T07:52:15.241] Gres [2025-09-03T07:52:,<*> Gres Name=gpu Type=l4 <*> <*>,gpu_inventory_update
2025-09-03T07:52:15.242,"[2025-09-03T07:52:15.242] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,HAS_TYPE,ENV_NVML,ENV_RSMI,ENV_ONEAPI,ENV_OPENCL,ENV_DEFAULT",gpu_inventory_update,info,infra,"gpu,l4,inventory",1,[2025-09-03T07:52:15.242] Gres [2025-09-03T07:52:15.242] Gres [2025-09-03T07:52:15.242] Gres [2025-09-03T07,<*> Gres Name=gpu Type=l4 <*> <*>,gpu_inventory_update
2025-09-03T07:52:15.242,"[2025-09-03T07:52:15.242] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,HAS_TYPE,ENV_NVML,ENV_RSMI,ENV_ONEAPI,ENV_OPENCL,ENV_DEFAULT",gpu_inventory_update,info,infra,"gpu,l4,inventory",1,[2025-09-03T07:52:15.242] Gres [2025-09-03T07:52:15.242] Gres [2025-09-03T07:52:15.242] Gres [2025-09-03T07,<*> Gres Name=gpu Type=l4 <*> <*>,gpu_inventory_update
2025-09-03T07:52:15.243,"[2025-09-03T07:52:15.243] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,HAS_TYPE,ENV_NVML,ENV_RSMI,ENV_ONEAPI,ENV_OPENCL,ENV_DEFAULT",gpu_inventory_update,info,infra,"gpu,l4,inventory",1,[2025-09-03T07:52:15.243] Gres [2025-09-03T07:52:15.243] Gres [2025-09-03T07:52:15.243] Gres [2025-09-03T07:52:,<*> Gres Name=gpu Type=l4 <*> <*>,gpu_inventory_update
2025-09-03T07:52:15.243,"[2025-09-03T07:52:15.243] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,HAS_TYPE,ENV_NVML,ENV_RSMI,ENV_ONEAPI,ENV_OPENCL,ENV_DEFAULT",gpu_inventory_update,info,infra,"gpu,l4,inventory",1,[2025-09-03T07:52:15.243] Gres [2025-09-03T07:52:15.243] Gres [2025-09-03T07:52:15.243] Gres [2025-09-03T07:52:,<*> Gres Name=gpu Type=l4 <*> <*>,gpu_inventory_update
2025-09-03T07:52:15.244,[2025-09-03T07:52:15.244] select/cons_tres: part_data_create_array: select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,2,[2025-09-03T07:52:15.244] select/cons_tres: preparing for 19 partitions [2025-09-03T07:52:15.244] select/cons_tres: part_data_create_array: select/cons_,[2025-09-03T07:52:15.244] select/cons_tres: part_data_create_array: select/cons_tres: preparing for 19 partitions,unknown
2025-09-03T07:52:15.244,[2025-09-03T07:52:15.244] Recovered state of 0 reservations,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",3,[2025-09-03T07:52:15.244] Recovered state of 0 reservations. [2025-09-03T07:52:15.244] [2025-09-03T07:52:15.244] Recovered state of 0 reservations. [,[2025-09-03T07:52:15.244] Recovered state of 0 reservations,cluster_state_recovered_no_reservations
2025-09-03T07:52:15.244,[2025-09-03T07:52:15.244] State of 0 triggers recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",4,[2025-09-03T07:52:15.244].] [2025-09-03T07:52:15.244]] [2025-09-03T07:52:15.244] [2025-09-03T07:52:,[2025-09-03T07:52:15.244] State of 0 triggers recovered,cluster_state_recovered_no_reservations
2025-09-03T07:52:15.244,[2025-09-03T07:52:15.244] read_slurm_conf: backup_controller not specified,controller_backup_not_configured,warning,infra,"controller,backup,not_configured",5,[2025-09-03T07:52:15.244] read_slurm_conf: backup_controller not specified [2025-09-03T07:52:15.244] read_slurm_conf: backup_controller not specified [2025-0,[2025-09-03T07:52:15.244] read_slurm_conf: backup_controller not specified,controller_backup_not_configured
2025-09-03T07:52:15.244,[2025-09-03T07:52:15.244] select/cons_tres: select_p_reconfigure: select/cons_tres: reconfigure,unknown,info,unknown,,6,[2025-09-03T07:52:15.244] select/cons_tres: select_p_reconfigure: select/cons_tres: reconfigure [2025-09-03T07:52:15.244] select/cons_tres: recon,[2025-09-03T07:52:15.244] select/cons_tres: select_p_reconfigure: select/cons_tres: reconfigure,unknown
2025-09-03T07:52:15.244,[2025-09-03T07:52:15.244] select/cons_tres: part_data_create_array: select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,2,[2025-09-03T07:52:15.244] select/cons_tres: preparing for 19 partitions [2025-09-03T07:52:15.244] select/cons_tres: part_data_create_array: select/cons_,[2025-09-03T07:52:15.244] select/cons_tres: part_data_create_array: select/cons_tres: preparing for 19 partitions,unknown
2025-09-03T07:52:15.245,[2025-09-03T07:52:15.245] Running as primary controller,controller_primary,info,infra,"controller,primary",7,[2025-09-03T07:52:15.245] Running as primary controller. [2025-09-03T07:52:15.245] Running as primary controller.,[2025-09-03T07:52:15.245] Running as primary controller,controller_primary
2025-09-03T07:53:16.000,"[2025-09-03T07:53:16.000] SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320,bf_interval=60,bf_continue,bf_busy_nodes,bf_job_part_count_reserve=5,bf_min_age_reserve=60",scheduler_parameters,info,scheduler,"scheduler,config",8,[2025-09-03T07:53:16.000] [2025-09-03T07:53:16.000] [2025-09-03T07:53:16.000] [2025-09-03T07:53:16.000] [202,"[2025-09-03T07:53:16.000] SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320,bf_interval=60,bf_continue,bf_busy_nodes,bf_job_part_count_reserve=5,bf_min_age_reserve=60",scheduler_parameters
2025-09-03T07:54:51.273,[2025-09-03T07:54:51.273] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T07:54:51.273] POWER: Power save mode: 334 nodes [2025-09-03T07:54:51.273] POWER: Power save mode: 334 nodes [2025-09-03T07:54:,[2025-09-03T07:54:51.273] POWER: Power save mode: 334 nodes,cluster_power_save_mode
2025-09-03T08:05:11.347,[2025-09-03T08:05:11.347] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T08:05:11.347] POWER: Power save mode: 334 nodes [2025-09-03T08:05:11.347] POWER: Power save mode: 334 nodes [2025-09-03T08:05:,<*> POWER: Power save mode: 334 nodes,cluster_power_save_mode
2025-09-03T08:15:31.423,[2025-09-03T08:15:31.423] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T08:15:31.423] POWER: Power save mode: 334 nodes [2025-09-03T08:15:31.423] POWER: Power save mode: 334 nodes [2025-09-03T08:15:,<*> POWER: Power save mode: 334 nodes,cluster_power_save_mode
2025-09-03T08:25:51.498,[2025-09-03T08:25:51.498] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T08:25:51.498] POWER: Power save mode: 334 nodes: [2025-09-03T08:25:51.498] POWER:. [2025-09-03T08:25:51.4,<*> POWER: Power save mode: 334 nodes,cluster_power_save_mode
2025-09-03T08:36:11.572,[2025-09-03T08:36:11.572] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T08:36:11.572] POWER: Power save mode: 334 nodes [2025-09-03T08:36:11.572] POWER: Power save mode: 334 nodes [2025-09-03T08:36:,<*> POWER: Power save mode: 334 nodes,cluster_power_save_mode
2025-09-03T08:46:31.642,[2025-09-03T08:46:31.642] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T08:46:31.642] POWER: Power save mode: 334 nodes [2025-09-03T08:46:31.642] POWER: Power save mode: 334 nodes [2025-09-03T08:46:,<*> POWER: Power save mode: 334 nodes,cluster_power_save_mode
2025-09-03T08:56:51.715,[2025-09-03T08:56:51.715] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T08:56:51.715] POWER: Power save mode: 334 nodes [2025-09-03T08:56:51.715] POWER: Power save mode: 334 nodes [2025-09-03T08:56:,<*> POWER: Power save mode: 334 nodes,cluster_power_save_mode
2025-09-03T09:07:11.790,[2025-09-03T09:07:11.790] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T09:07:11.790] POWER: Power save mode: 334 nodes [2025-09-03T09:07:11.790] POWER: Power save mode: 334 nodes [2025-09-03T09:07:,<*> POWER: Power save mode: 334 nodes,cluster_power_save_mode
2025-09-03T09:17:31.865,[2025-09-03T09:17:31.865] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T09:17:31.865] POWER: Power save mode: 334 nodes [2025-09-03T09:17:31.865] POWER: Power save mode: 334 nodes [2025-09-03T09:17:,<*> POWER: Power save mode: 334 nodes,cluster_power_save_mode
2025-09-03T09:27:51.938,[2025-09-03T09:27:51.938] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T09:27:51.938] POWER: Power save mode: 334 nodes [2025-09-03T09:27:51.938] POWER: Power save mode: 334 nodes [2025-09-03T09:27:,<*> POWER: Power save mode: 334 nodes,cluster_power_save_mode
2025-09-03T09:38:11.010,[2025-09-03T09:38:11.010] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T09:38:11.010] POWER: Power save mode: 334 nodes [2025-09-03T09:38:11.010] POWER: Power save mode: 334 nodes [2025-09-03T09:38:,<*> POWER: Power save mode: 334 nodes,cluster_power_save_mode
2025-09-03T09:38:11.413,[2025-09-03T09:38:11.413] _slurm_rpc_submit_batch_job: JobId=958 InitPrio=35869 usec=6139,job_submit_batch,info,scheduler,"job,submit,batch",10,[2025-09-03T09:38:11.413] _slurm_rpc_submit_batch_job: JobId=958 InitPrio=35869 usec=6139 [2025-09-03T09:38,[2025-09-03T09:38:11.413] _slurm_rpc_submit_batch_job: JobId=958 InitPrio=35869 usec=6139,job_submit_batch
2025-09-03T09:38:12.000,[2025-09-03T09:38:12.000] sched: Allocate JobId=958 NodeList=cpu-8c-std-dy-c7i4xlarge-1 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T09:38:12.000] sched: [2025-09-03T09:38:12.000] sched: [2025-09-03T09:38:12.000] sched: [2025-09-03T09,[2025-09-03T09:38:12.000] sched: Allocate JobId=958 NodeList=cpu-8c-std-dy-c7i4xlarge-1 #CPUs=8 Partition=cpu-8c-std,job_allocate
2025-09-03T09:38:42.013,[2025-09-03T09:38:42.013] POWER: no more nodes to resume for job JobId=958,cluster_power_save_complete,info,infra,"power,save,complete",12,[2025-09-03T09:38:42.013] POWER: no more nodes to resume for job JobId=958 JobId=958 JobId=958 JobId=958 JobId=958 JobId=95,[2025-09-03T09:38:42.013] POWER: no more nodes to resume for job JobId=958,cluster_power_save_complete
2025-09-03T09:38:42.013,[2025-09-03T09:38:42.013] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-1,cluster_power_save_wake,info,infra,"power,save,wakeup",13,cpu_save: waking nodes cpu-std-dy-c7i4xlarge-1_save: waking nodes-dy-c7i4xlarge-1--- POWER,[2025-09-03T09:38:42.013] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-1,cluster_power_save_wake
2025-09-03T09:42:37.720,[2025-09-03T09:42:37.720] Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-1 now responding. [2025-09-03T09:42:37.720]. [2025-09-03T09:42:37.720]--c7i4xlarge-1 now responding. [2025-09-,[2025-09-03T09:42:37.720] Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding
2025-09-03T09:42:37.720,"[2025-09-03T09:42:37.720] POWER: Node cpu-8c-std-dy-c7i4xlarge-1/cpu-8c-std-dy-c7i4xlarge-1/192.168.13.70 powered up with instance_id=i-0c30e29de39a401f6, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,-1/192.168.13.70-1 and node_name=c7i.4xlarge-1. [2025-09-03T09:42:37.720] POWER: Node0] [2025-09-03T09:42:37.720,"[2025-09-03T09:42:37.720] POWER: Node cpu-8c-std-dy-c7i4xlarge-1/cpu-8c-std-dy-c7i4xlarge-1/192.168.13.70 powered up with instance_id=i-0c30e29de39a401f6, instance_type=c7i.4xlarge",cluster_power_node_online
2025-09-03T09:42:45.001,[2025-09-03T09:42:45.001] job_time_limit: Configuration for JobId=958 complete,job_time_limit_configured,info,scheduler,"job,time_limit,config",16,[2025-09-03T09:42:01.001] job_time_limit: Configuration for JobId=958 complete [2025-09-03T09:42:02.001] job_time_limit: Configuration for JobId=958 complete [2025-0,[2025-09-03T09:42:45.001] job_time_limit: Configuration for JobId=958 complete,job_time_limit_configured
2025-09-03T09:42:45.001,[2025-09-03T09:42:45.001] Resetting JobId=958 start time for node power up,job_reset_start_time,info,scheduler,"job,reset_start_time,node_power_up",17,[2025-09-03T09:42:45.001]. [2025-09-03T09:42:45.001] [2025-09-03T09:42:45.001] Resetting JobId=958 start time for node,[2025-09-03T09:42:45.001] Resetting JobId=958 start time for node power up,job_reset_start_time
2025-09-03T09:48:31.084,[2025-09-03T09:48:31.084] POWER: Power save mode: 333 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T09:48:31.084] POWER: Power save mode: 333 nodes [2025-09-03T09:48:31.084] POWER: Power save mode: 333 nodes [2025-09-03T09:48:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T09:49:06.084,[2025-09-03T09:49:06.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T09:49:06.084] POWER: Power save mode: 334 nodes [2025-09-03T09:49:06.084] POWER: Power save mode: 334 nodes [2025-09-03T09:49:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T09:49:41.084,[2025-09-03T09:49:41.084] _slurm_rpc_submit_batch_job: JobId=961 InitPrio=30001 usec=5001,job_submit_batch,info,scheduler,"job,submit,batch",18,[2025-09-03T09:49:41.084] _slurm_rpc_submit_batch_job: [2025-09-03T09:49:41.084] _slurm_rpc_submit_,[2025-09-03T09:49:41.084] _slurm_rpc_submit_batch_job: JobId=961 InitPrio=30001 usec=5001,job_submit_batch
2025-09-03T09:50:16.084,[2025-09-03T09:50:16.084] sched: Allocate JobId=962 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T09:50:16.084] sched: [2025-09-03T09:50:16.084] sched: [2025-09-03T09:50:16.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2025-09-03T09:50:51.084,[2025-09-03T09:50:51.084] POWER: no more nodes to resume for job JobId=963,cluster_power_save_complete,info,infra,"power,save,complete",12,POWER: no more nodes to resume for job [2025-09-03T09:50:51.084] POWER: no more nodes to resume for job JobId=963 [2025-09-03T09:50:51.084] POWER:,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2025-09-03T09:51:26.084,[2025-09-03T09:51:26.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-3,cluster_power_save_wake,info,infra,"power,save,wakeup",13,_save: waking nodes_save: waking nodes_save: waking nodes-c7i4xlarge-3-c7i4--2025-09-03T09:51:26.084,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2025-09-03T09:52:01.084,[2025-09-03T09:52:01.084] Node cpu-8c-std-dy-c7i4xlarge-4 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-4 now responding. [2025-09-03T09:52:01.084]. [2025-09-03T09:52:01.084]--c7i4xlarge-4 now responding. [2025-09-,<*> Node <*> now responding,node_now_responding
2025-09-03T09:52:36.084,"[2025-09-03T09:52:36.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-5/cpu-8c-std-dy-c7i4xlarge-5/192.168.13.71 powered up with instance_id=i-0c30e29de39a40001, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"_id=i-0c30e29de39a40001, instance_type=c7i.4xlarge-5.-5.00-5/cpu-8c-std-dy-c7i4xlarge-5/192.168",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2025-09-03T09:53:11.084,[2025-09-03T09:53:11.084] AUTH: user user1 failed password authentication from 10.1.1.101,auth_failure,high,security,"auth,security,login_failure",19,[2025-09-03T09:53:11.084] AUTH: user user1 failed password authentication from 10.1.1.101. [2025-09-03T09:53:11.084] AUTH: user user1 failed password authentication from 10.1.1.101.,[2025-09-03T09:53:11.084] AUTH: user user1 failed password authentication from 10.1.1.101,auth_failure
2025-09-03T09:53:46.084,[2025-09-03T09:53:46.084] DISK: /var is 89% full on node cpu-16c-std-2,disk_usage_high,warning,storage,"disk,usage,high",20,[2025-09-03T09:53:46.084] DISK: [2025-09-03T09:53:46.084] [2025-09-03T09:53:46.084] [2025-09-03T09:53:46.0,[2025-09-03T09:53:46.084] DISK: /var is 89% full on node cpu-16c-std-2,disk_usage_high
2025-09-03T09:54:21.084,[2025-09-03T09:54:21.084] NETWORK: latency to storage cluster increased to 81ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T09:54:21.084] [2025-09-03T09:54:21.084] [2025-09-03T09:54:21.084] [2025-09-03T09:54:21.084] [202,[2025-09-03T09:54:21.084] NETWORK: latency to storage cluster increased to 81ms (no SLA breach),network_latency_storage_cluster
2025-09-03T09:54:56.084,[2025-09-03T09:54:56.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T09:54:56.084] POWER: Power save mode: 334 nodes [2025-09-03T09:54:56.084] POWER: Power save mode: 334 nodes [2025-09-03T09:54:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T09:55:31.084,[2025-09-03T09:55:31.084] POWER: Power save mode: 335 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T09:55:31.084] POWER: Power save mode: 335 nodes [2025-09-03T09:55:31.084] POWER: Power save mode: 335 nodes [2025-09-03T09:55:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T09:56:06.084,[2025-09-03T09:56:06.084] _slurm_rpc_submit_batch_job: JobId=971 InitPrio=30011 usec=5011,job_submit_batch,info,scheduler,"job,submit,batch",22,[2025-09-03T09:56:06.084] _slurm_rpc_submit_batch_job: JobId=971 InitPrio=30011 usec=5011 [2025-09-03T09:56:,[2025-09-03T09:56:06.084] _slurm_rpc_submit_batch_job: JobId=971 InitPrio=30011 usec=5011,job_submit_batch
2025-09-03T09:56:41.084,[2025-09-03T09:56:41.084] sched: Allocate JobId=972 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T09:56:41.084] sched: [2025-09-03T09:56:41.084] sched: [2025-09-03T09:56:41.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2025-09-03T09:57:16.084,[2025-09-03T09:57:16.084] POWER: no more nodes to resume for job JobId=973,cluster_power_save_complete,info,infra,"power,save,complete",12,[2025-09-03T09:57:16.084] POWER: no more nodes to resume for job [2025-09-03T09:57:16.084] POWER: no more nodes to resume for job.:0.:,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2025-09-03T09:57:51.084,[2025-09-03T09:57:51.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",13,_save: waking nodes_save: waking nodes_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4-c7i4xlarge-4--,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2025-09-03T09:58:26.084,[2025-09-03T09:58:26.084] Node cpu-8c-std-dy-c7i4xlarge-5 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-5 [2025-09-03T09:58:26.084].-c7i4xlarge-5-c7i4xlarge-5 [2025-09-03T09:58:26.084] [2025-09,<*> Node <*> now responding,node_now_responding
2025-09-03T09:59:01.084,"[2025-09-03T09:59:01.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-1/cpu-8c-std-dy-c7i4xlarge-1/192.168.13.72 powered up with instance_id=i-0c30e29de39a40002, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"-1/cpu-8c-std-dy-c7i4xlarge-1/192.168.13.72, instance_type=c7i.4xlarge-1.]0-1/cpu-8c-std-dy-",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2025-09-03T09:59:36.084,[2025-09-03T09:59:36.084] AUTH: user user2 failed password authentication from 10.1.2.102,auth_failure,high,security,"auth,security,login_failure",19,[2025-09-03T09:59:36.084] AUTH: user user2 failed password authentication from 10.1.2.102. [2025-09-03T09:59:36.084] AUTH: user user2 failed password authentication from 10.1.2.102.,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
2025-09-03T10:00:11.084,[2025-09-03T10:00:11.084] DISK: /var is 90% full on node cpu-16c-std-3,disk_usage_high,warning,storage,"disk,usage,high",20,: [2025-09-03T10:00:11.084] DISK: [2025-09-03T10:00:11.084] DISK:-std-3-node-node.]:,<*> DISK: /var is <*> full on node <*>,disk_usage_high
2025-09-03T10:00:46.084,[2025-09-03T10:00:46.084] NETWORK: latency to storage cluster increased to 82ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T10:00:46.084] [2025-09-03T10:00:46.084] [2025-09-03T10:00:46.084] [2025-09-03T10:00:46.084] [2025-09-03,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
2025-09-03T10:01:21.084,[2025-09-03T10:01:21.084] POWER: Power save mode: 333 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:01:21.084] POWER: Power save mode: 333 nodes: [2025-09-03T10:01:21.084] POWER: [2025-09-03T10:01:21.084] POWER:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T10:01:56.084,[2025-09-03T10:01:56.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:01:56.084] POWER: Power save mode: 334 nodes [2025-09-03T10:01:56.084] POWER: Power save mode: 334 nodes [2025-09-03T10:01:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T10:02:31.084,[2025-09-03T10:02:31.084] _slurm_rpc_submit_batch_job: JobId=981 InitPrio=30021 usec=5021,job_submit_batch,info,scheduler,"job,submit,batch",23,[2025-09-03T10:02:31.084] _slurm_rpc_submit_batch_job: JobId=981 InitPrio=30021 usec=5021 [2025-09-03T10:02:,[2025-09-03T10:02:31.084] _slurm_rpc_submit_batch_job: JobId=981 InitPrio=30021 usec=5021,job_submit_batch
2025-09-03T10:03:06.084,[2025-09-03T10:03:06.084] sched: Allocate JobId=982 NodeList=cpu-8c-std-dy-c7i4xlarge-4 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T10:03:06.084] sched: [2025-09-03T10:03:06.084] [2025-09-03T10:03:06.084] sched: [2025-09-03T10:,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2025-09-03T10:03:41.084,[2025-09-03T10:03:41.084] POWER: no more nodes to resume for job JobId=983,cluster_power_save_complete,info,infra,"power,save,complete",12,[2025-09-03T10:03:41.084] POWER: no more nodes to resume for job [2025-09-03T10:03:41.084] POWER: no more nodes to resume for job:983:41.084,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2025-09-03T10:04:16.084,[2025-09-03T10:04:16.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-5,cluster_power_save_wake,info,infra,"power,save,wakeup",13,_save: waking nodes-save: waking nodes_save: waking nodes cpu-8c-std-dy-c7i4xlarge-5-c7i4x-c7i4,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2025-09-03T10:04:51.084,[2025-09-03T10:04:51.084] Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-1.-c7i4xlarge-1-c7i4xlarge-1 [2025-09-03T10:04:51.084] [2025-09-03T10:04:51.084] [2025-09,<*> Node <*> now responding,node_now_responding
2025-09-03T10:05:26.084,"[2025-09-03T10:05:26.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-2/cpu-8c-std-dy-c7i4xlarge-2/192.168.13.73 powered up with instance_id=i-0c30e29de39a40003, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"-2/192.168.13.73-2, instance_name=c7i.4xlarge-2, instance_type=c7i.4xlarge-2, instance_id=i-0c30e29de39a40003, instance_type=c7i",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2025-09-03T10:06:01.084,[2025-09-03T10:06:01.084] AUTH: user user3 failed password authentication from 10.1.3.103,auth_failure,high,security,"auth,security,login_failure",19,[2025-09-03T10:06:01.084] AUTH: user user3 failed password authentication from 10.1.3.103. [2025-09-03T10:06:01.084] AUTH: user user3 failed password authentication from 10.1.3.103.,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
2025-09-03T10:06:36.084,[2025-09-03T10:06:36.084] DISK: /var is 91% full on node cpu-16c-std-4,disk_usage_high,warning,storage,"disk,usage,high",20,:36.084] [2025-09-03T10:06:36.084] [2025-09-03T10:06:36.084] [2025-09-03T10:06:36.084] [2025-09-03T10:06:3,<*> DISK: /var is <*> full on node <*>,disk_usage_high
2025-09-03T10:07:11.084,[2025-09-03T10:07:11.084] NETWORK: latency to storage cluster increased to 83ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T10:07:11.084] NETWORK: latency to storage cluster increased to 83ms (no SLA breach). [2025-09-03T10:07:11.084] NETWORK: latency to storage cluster increased,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
2025-09-03T10:07:46.084,[2025-09-03T10:07:46.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:07:46.084] POWER: Power save mode: 334 nodes [2025-09-03T10:07:46.084] POWER: Power save mode: 334 nodes [2025-09-03T10:07:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T10:08:21.084,[2025-09-03T10:08:21.084] POWER: Power save mode: 335 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:08:21.084] POWER: Power save mode: 335 nodes: [2025-09-03T10:08:21.084] POWER: [2025-09-03T10:08:21.084] POWER:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T10:08:56.084,[2025-09-03T10:08:56.084] _slurm_rpc_submit_batch_job: JobId=991 InitPrio=30031 usec=5031,job_submit_batch,info,scheduler,"job,submit,batch",24,[2025-09-03T10:08:56.084] _slurm_rpc_submit_batch_job: JobId=991 InitPrio=30031 usec=5031 [2025-09-03T10:08:,[2025-09-03T10:08:56.084] _slurm_rpc_submit_batch_job: JobId=991 InitPrio=30031 usec=5031,job_submit_batch
2025-09-03T10:09:31.084,[2025-09-03T10:09:31.084] sched: Allocate JobId=992 NodeList=cpu-8c-std-dy-c7i4xlarge-5 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T10:09:31.084] sched: [2025-09-03T10:09:31.084] sched: [2025-09-03T10:09:31.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2025-09-03T10:10:06.084,[2025-09-03T10:10:06.084] POWER: no more nodes to resume for job JobId=993,cluster_power_save_complete,info,infra,"power,save,complete",12,[2025-09-03T10:10:06.084] POWER: no more nodes to resume for job [2025-09-03T10:10:06.084] POWER: no more nodes to resume for job. POWER: no more no,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2025-09-03T10:10:41.084,[2025-09-03T10:10:41.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-1,cluster_power_save_wake,info,infra,"power,save,wakeup",13,cpu-8c-std-dy-c7i4xlarge-1 cpu-8c-std-dy-c7i4xlarge-1_save: waking nodes-c7i4xlarge-1-,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2025-09-03T10:11:16.084,[2025-09-03T10:11:16.084] Node cpu-8c-std-dy-c7i4xlarge-2 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-2 [2025-09-03T10:11:16.084]. [2025-09-03T10:11:16.084]-c7i4xlarge-2-c7i4xlarge-2 now responding-,<*> Node <*> now responding,node_now_responding
2025-09-03T10:11:51.084,"[2025-09-03T10:11:51.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-3/cpu-8c-std-dy-c7i4xlarge-3/192.168.13.74 powered up with instance_id=i-0c30e29de39a40004, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"-c7i4xlarge-3, instance_id=i-0c30e29de39a40004, instance_type=c7i.4xlarge-3.-3, instance_type=c7i.4xlarge-3.00.0:5",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2025-09-03T10:12:26.084,[2025-09-03T10:12:26.084] AUTH: user user4 failed password authentication from 10.1.4.104,auth_failure,high,security,"auth,security,login_failure",19,[2025-09-03T10:12:26.084] AUTH: user user4 failed password authentication from 10.1.4.104.,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
2025-09-03T10:13:01.084,[2025-09-03T10:13:01.084] DISK: /var is 92% full on node cpu-16c-std-1,disk_usage_high,warning,storage,"disk,usage,high",20,[2025-09-03T10:13:01.084] DISK: [2025-09-03T10:13:01.084] DISK: [2025-09-03T10:13:01.084] DISK: [2025-09-,<*> DISK: /var is <*> full on node <*>,disk_usage_high
2025-09-03T10:13:36.084,[2025-09-03T10:13:36.084] NETWORK: latency to storage cluster increased to 84ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T10:13:36.084] NETWORK: latency to storage cluster increased to 84ms (no SLA breach) [2025-09-03T10:13:36.084] NETWORK: latency to storage cluster increased to,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
2025-09-03T10:14:11.084,[2025-09-03T10:14:11.084] POWER: Power save mode: 333 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:14:11.084] POWER: Power save mode: 333 nodes [2025-09-03T10:14:11.084] POWER: Power save mode: 333 nodes [2025-09-03T10:14:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T10:14:46.084,[2025-09-03T10:14:46.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:14:46.084] POWER: Power save mode: 334 nodes [2025-09-03T10:14:46.084] POWER: Power save mode: 334 nodes [2025-09-03T10:14:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T10:15:21.084,[2025-09-03T10:15:21.084] _slurm_rpc_submit_batch_job: JobId=1001 InitPrio=30041 usec=5041,job_submit_batch,info,scheduler,"job,submit,batch",25,[2025-09-03T10:15:21.084] _slurm_rpc_submit_batch_job: JobId=1001 InitPrio=30041 usec=5041 [2025-09-03T10:15:,[2025-09-03T10:15:21.084] _slurm_rpc_submit_batch_job: JobId=1001 InitPrio=30041 usec=5041,job_submit_batch
2025-09-03T10:15:56.084,[2025-09-03T10:15:56.084] sched: Allocate JobId=1002 NodeList=cpu-8c-std-dy-c7i4xlarge-1 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T10:15:56.084] sched: [2025-09-03T10:15:56.084] sched: [2025-09-03T10:15:56.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2025-09-03T10:16:31.084,[2025-09-03T10:16:31.084] POWER: no more nodes to resume for job JobId=1003,cluster_power_save_complete,info,infra,"power,save,complete",12,nodes to resume for job [2025-09-03T10:16:31.084] POWER: no more nodes to resume for job JobId=1003 [2025-09-03T10:16:31.084] POWER: no more nodes to,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2025-09-03T10:17:06.084,[2025-09-03T10:17:06.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-2,cluster_power_save_wake,info,infra,"power,save,wakeup",13,_save: waking nodes_save: waking nodes_save: waking nodes cpu-8c-std-dy-c7i4xlarge-2-c7i4xlarge-2-,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2025-09-03T10:17:41.084,[2025-09-03T10:17:41.084] Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-3 [2025-09-03T10:17:41.084]. [2025-09-03T10:17:41.084]-c7i4xlarge-3-c7i4xlarge-3 [2025-09,<*> Node <*> now responding,node_now_responding
2025-09-03T10:18:16.084,"[2025-09-03T10:18:16.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-4/cpu-8c-std-dy-c7i4xlarge-4/192.168.13.75 powered up with instance_id=i-0c30e29de39a40005, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"_id=i-0c30e29de39a40005, instance_type=c7i.4xlarge-4.-4.00.-4/cpu-8c-std-dy-c7i4xlarge-4/192.1",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2025-09-03T10:18:51.084,[2025-09-03T10:18:51.084] AUTH: user user5 failed password authentication from 10.1.5.105,auth_failure,high,security,"auth,security,login_failure",19,failed password authentication from 10.1.5.105. [2025-09-03T10:18:51.084] AUTH: user user5 failed password authentication from 10.1.5.105. [2025-09-03T10:18:51.084] [2025-09-03T,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
2025-09-03T10:19:26.084,[2025-09-03T10:19:26.084] DISK: /var is 93% full on node cpu-16c-std-2,disk_usage_high,warning,storage,"disk,usage,high",20,[2025-09-03T10:19:26.084] DISK: /var is 93% full on node cpu-16c-std-2. [2025-09-03T10:19:26.084] [2025-09-03T10,<*> DISK: /var is <*> full on node <*>,disk_usage_high
2025-09-03T10:20:01.084,[2025-09-03T10:20:01.084] NETWORK: latency to storage cluster increased to 85ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T10:20:01.084] NETWORK: latency to storage cluster increased to 85ms (no SLA breach) [2025-09-03T10:20:01.084] NETWORK: latency to storage cluster increased to,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
2025-09-03T10:20:36.084,[2025-09-03T10:20:36.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:20:36.084] POWER: Power save mode: 334 nodes [2025-09-03T10:20:36.084] POWER: Power save mode: 334 nodes [2025-09-03T10:20:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T10:21:11.084,[2025-09-03T10:21:11.084] POWER: Power save mode: 335 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:21:11.084] POWER: Power save mode: 335 nodes [2025-09-03T10:21:11.084] POWER: Power save mode: 335 nodes [2025-09-03T10:21:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T10:21:46.084,[2025-09-03T10:21:46.084] _slurm_rpc_submit_batch_job: JobId=1011 InitPrio=30051 usec=5051,job_submit_batch,info,scheduler,"job,submit,batch",26,[2025-09-03T10:21:46.084] _slurm_rpc_submit_batch_job: JobId=1011 InitPrio=30051 usec=5051 [2025-09-03T10:21:,[2025-09-03T10:21:46.084] _slurm_rpc_submit_batch_job: JobId=1011 InitPrio=30051 usec=5051,job_submit_batch
2025-09-03T10:22:21.084,[2025-09-03T10:22:21.084] sched: Allocate JobId=1012 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T10:22:21.084] sched: [2025-09-03T10:22:21.084] sched: [2025-09-03T10:22:21.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2025-09-03T10:22:56.084,[2025-09-03T10:22:56.084] POWER: no more nodes to resume for job JobId=1013,cluster_power_save_complete,info,infra,"power,save,complete",12,nodes to resume for job [2025-09-03T10:22:56.084] POWER: no more nodes to resume for job [2025-09-03T10:22:56.084] POWER: no more nodes to resume for job:10,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2025-09-03T10:23:31.084,[2025-09-03T10:23:31.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-3,cluster_power_save_wake,info,infra,"power,save,wakeup",13,: waking nodes cpu-std-dy-c7i4xlarge-3_save: waking nodes-c7i4xlarge-3-c7i4--2025-09-03T10:,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2025-09-03T10:24:06.084,[2025-09-03T10:24:06.084] Node cpu-8c-std-dy-c7i4xlarge-4 now responding,node_now_responding,info,infra,"node,recovered,online",14,node cpu-8c-std-dy-c7i4xlarge-4 now responding..-c7i4xlarge-4-c7i4xlarge-4 [2025-09-03T10:24:06.084] [202,<*> Node <*> now responding,node_now_responding
2025-09-03T10:24:41.084,"[2025-09-03T10:24:41.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-5/cpu-8c-std-dy-c7i4xlarge-5/192.168.13.76 powered up with instance_id=i-0c30e29de39a40006, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"_id=i-0c30e29de39a40006, instance_type=c7i.4xlarge-5.-5.41.084] [2025-09-03T10:24:41.084] [2025-09-03T10:",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2025-09-03T10:25:16.084,[2025-09-03T10:25:16.084] AUTH: user user6 failed password authentication from 10.1.6.106,auth_failure,high,security,"auth,security,login_failure",19,[2025-09-03T10:25:16.084] AUTH: user user6 failed password authentication from 10.1.6.106.,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
2025-09-03T10:25:51.084,[2025-09-03T10:25:51.084] DISK: /var is 94% full on node cpu-16c-std-3,disk_usage_high,warning,storage,"disk,usage,high",20,[2025-09-03T10:25:51.084] DISK: [2025-09-03T10:25:51.084] [2025-09-03T10:25:51.084] [2025-09-03T10:25:51.0,<*> DISK: /var is <*> full on node <*>,disk_usage_high
2025-09-03T10:26:26.084,[2025-09-03T10:26:26.084] NETWORK: latency to storage cluster increased to 86ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T10:26:26.084] NETWORK: latency to storage cluster increased to 86ms (no SLA breach) [2025-09-03T10:26:26.084] NETWORK: latency to storage cluster increased to,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
2025-09-03T10:27:01.084,[2025-09-03T10:27:01.084] POWER: Power save mode: 333 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:27:01.084] POWER: Power save mode: 333 nodes [2025-09-03T10:27:01.084] POWER: Power save mode: 333 nodes [2025-09-03T10:27:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T10:27:36.084,[2025-09-03T10:27:36.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:27:36.084] POWER: Power save mode: 334 nodes [2025-09-03T10:27:36.084] POWER: Power save mode: 334 nodes [2025-09-03T10:27:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T10:28:11.084,[2025-09-03T10:28:11.084] _slurm_rpc_submit_batch_job: JobId=1021 InitPrio=30061 usec=5061,job_submit_batch,info,scheduler,"job,submit,batch",27,[2025-09-03T10:28:11.084] _slurm_rpc_submit_batch_job: JobId=1021 InitPrio=30061 usec=5061 [2025-09-03T10:28:,[2025-09-03T10:28:11.084] _slurm_rpc_submit_batch_job: JobId=1021 InitPrio=30061 usec=5061,job_submit_batch
2025-09-03T10:28:46.084,[2025-09-03T10:28:46.084] sched: Allocate JobId=1022 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T10:28:46.084] sched: [2025-09-03T10:28:46.084] sched: [2025-09-03T10:28:46.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2025-09-03T10:29:21.084,[2025-09-03T10:29:21.084] POWER: no more nodes to resume for job JobId=1023,cluster_power_save_complete,info,infra,"power,save,complete",12,nodes to resume for job [2025-09-03T10:29:21.084] POWER: no more nodes to resume for job [2025-09-03T10:29:21.084] POWER: no more nodes to resume for job JobId=,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2025-09-03T10:29:56.084,[2025-09-03T10:29:56.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",13,: waking nodes cpu-std-dy-c7i4xlarge-4_save: waking nodes-c7i4xlarge-4 cpu-8c-std-dy-c7i4xlarge,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2025-09-03T10:30:31.084,[2025-09-03T10:30:31.084] Node cpu-8c-std-dy-c7i4xlarge-5 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-5 [2025-09-03T10:30:31.084].-c7i4xlarge-5-c7i4xlarge-5 [2025-09-03T10:30:31.084] [2025-09,<*> Node <*> now responding,node_now_responding
2025-09-03T10:31:06.084,"[2025-09-03T10:31:06.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-1/cpu-8c-std-dy-c7i4xlarge-1/192.168.13.77 powered up with instance_id=i-0c30e29de39a40007, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"-1/cpu-8c-std-dy-c7i4xlarge-1/192.168.13.77, instance_type=c7i.4xlarge-1.000:00+0c30e29de39a40007, instance_",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2025-09-03T10:31:41.084,[2025-09-03T10:31:41.084] AUTH: user user0 failed password authentication from 10.1.0.107,auth_failure,high,security,"auth,security,login_failure",19,failed password authentication from 10.1.0.107. [2025-09-03T10:31:41.084] [2025-09-03T10:31:41.084] [2025-09-03T10:31:41.084] [2025-09-03T10,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
2025-09-03T10:32:16.084,[2025-09-03T10:32:16.084] DISK: /var is 95% full on node cpu-16c-std-4,disk_usage_high,warning,storage,"disk,usage,high",20,[2025-09-03T10:32:16.084] DISK: [2025-09-03T10:32:16.084] [2025-09-03T10:32:16.084] [2025-09-03T10:32:16.0,<*> DISK: /var is <*> full on node <*>,disk_usage_high
2025-09-03T10:32:51.084,[2025-09-03T10:32:51.084] NETWORK: latency to storage cluster increased to 87ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T10:32:51.084] [2025-09-03T10:32:51.084] [2025-09-03T10:32:51.084] [2025-09-03T10:32:51.084] [202,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
2025-09-03T10:33:26.084,[2025-09-03T10:33:26.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:33:26.084] POWER: Power save mode: 334 nodes [2025-09-03T10:33:26.084] POWER: Power save mode: 334 nodes [2025-09-03T10:33:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T10:34:01.084,[2025-09-03T10:34:01.084] POWER: Power save mode: 335 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:34:01.084] POWER: Power save mode: 335 nodes [2025-09-03T10:34:01.084] POWER: Power save mode: 335 nodes [2025-09-03T10:34:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T10:34:36.084,[2025-09-03T10:34:36.084] _slurm_rpc_submit_batch_job: JobId=1031 InitPrio=30071 usec=5071,job_submit_batch,info,scheduler,"job,submit,batch",28,[2025-09-03T10:34:36.084] _slurm_rpc_submit_batch_job: JobId=1031 InitPrio=30071 usec=5071 [2025-09-03T10:34:,[2025-09-03T10:34:36.084] _slurm_rpc_submit_batch_job: JobId=1031 InitPrio=30071 usec=5071,job_submit_batch
2025-09-03T10:35:11.084,[2025-09-03T10:35:11.084] sched: Allocate JobId=1032 NodeList=cpu-8c-std-dy-c7i4xlarge-4 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T10:35:11.084] sched: [2025-09-03T10:35:11.084] sched: [2025-09-03T10:35:11.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2025-09-03T10:35:46.084,[2025-09-03T10:35:46.084] POWER: no more nodes to resume for job JobId=1033,cluster_power_save_complete,info,infra,"power,save,complete",12,[2025-09-03T10:35:46.084] POWER: no more nodes to resume for job JobId=1033 [2025-09-03T10:35:46.084] POWER: no more nodes to resume for job [2025-0,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2025-09-03T10:36:21.084,[2025-09-03T10:36:21.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-5,cluster_power_save_wake,info,infra,"power,save,wakeup",13,_save: waking nodes-save: waking nodes_save: waking nodes cpu-8c-std-dy-c7i4xlarge-5-c7i4x-c7i4,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2025-09-03T10:36:56.084,[2025-09-03T10:36:56.084] Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-1 now responding.. [2025-09-03T10:36:56.084]-c7i4xlarge-1-c7i4xlarge-1 now responding. [2025-09-03T10:36:56.0,<*> Node <*> now responding,node_now_responding
2025-09-03T10:37:31.084,"[2025-09-03T10:37:31.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-2/cpu-8c-std-dy-c7i4xlarge-2/192.168.13.78 powered up with instance_id=i-0c30e29de39a40008, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"-c7i4xlarge-2/192.168.13.78-2, instance_type=c7i.4xlarge-2, instance_id=i-0c30e29de39a40008, instance_type=c7i.4xlarge-2.",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2025-09-03T10:38:06.084,[2025-09-03T10:38:06.084] AUTH: user user1 failed password authentication from 10.1.1.108,auth_failure,high,security,"auth,security,login_failure",19,[2025-09-03T10:38:06.084] AUTH: user user1 failed password authentication from 10.1.1.108 from 10.1.1.108. [2025-09-03T10:38:06.084] AUTH: user user1 failed password authentication from 10.,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
2025-09-03T10:38:41.084,[2025-09-03T10:38:41.084] DISK: /var is 96% full on node cpu-16c-std-1,disk_usage_high,warning,storage,"disk,usage,high",20,[2025-09-03T10:38:41.084] [2025-09-03T10:38:41.084] [2025-09-03T10:38:41.084] [2025-09-03T10:38:41.084] [202,<*> DISK: /var is <*> full on node <*>,disk_usage_high
2025-09-03T10:39:16.084,[2025-09-03T10:39:16.084] NETWORK: latency to storage cluster increased to 88ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T10:39:16.084] NETWORK: latency to storage cluster increased to 88ms (no SLA breach). [2025-09-03T10:39:16.084] NETWORK: latency to storage cluster increased,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
2025-09-03T10:39:51.084,[2025-09-03T10:39:51.084] POWER: Power save mode: 333 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:39:51.084] POWER: Power save mode: 333 nodes [2025-09-03T10:39:51.084] POWER: Power save mode: 333 nodes [2025-09-03T10:39:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T10:40:26.084,[2025-09-03T10:40:26.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:40:26.084] POWER: Power save mode: 334 nodes [2025-09-03T10:40:26.084] POWER: Power save mode: 334 nodes [2025-09-03T10:40:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T10:41:01.084,[2025-09-03T10:41:01.084] _slurm_rpc_submit_batch_job: JobId=1041 InitPrio=30081 usec=5081,job_submit_batch,info,scheduler,"job,submit,batch",29,[2025-09-03T10:41:01.084] _slurm_rpc_submit_batch_job: JobId=1041 InitPrio=30081 usec=5081 [2025-09-03T10:41:,[2025-09-03T10:41:01.084] _slurm_rpc_submit_batch_job: JobId=1041 InitPrio=30081 usec=5081,job_submit_batch
2025-09-03T10:41:36.084,[2025-09-03T10:41:36.084] sched: Allocate JobId=1042 NodeList=cpu-8c-std-dy-c7i4xlarge-5 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T10:41:36.084] sched: [2025-09-03T10:41:36.084] sched: [2025-09-03T10:41:36.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2025-09-03T10:42:11.084,[2025-09-03T10:42:11.084] POWER: no more nodes to resume for job JobId=1043,cluster_power_save_complete,info,infra,"power,save,complete",12,[2025-09-03T10:42:11.084] POWER: no more nodes to resume for job JobId=1043 JobId=1043 JobId=1043 JobId=1043:10 POWER: no more nodes,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2025-09-03T10:42:46.084,[2025-09-03T10:42:46.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-1,cluster_power_save_wake,info,infra,"power,save,wakeup",13,cpu_save: waking nodes cpu-8c-std-dy-c7i4xlarge-1_save: waking nodes_save: waking nodes-c7i4xlarge-1,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2025-09-03T10:43:21.084,[2025-09-03T10:43:21.084] Node cpu-8c-std-dy-c7i4xlarge-2 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-2 [2025-09-03T10:43:21.084]. [2025-09-03T10:43:21.084]-c7i4xlarge-2-c7i4xlarge-2 now responding. [,<*> Node <*> now responding,node_now_responding
2025-09-03T10:43:56.084,"[2025-09-03T10:43:56.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-3/cpu-8c-std-dy-c7i4xlarge-3/192.168.13.79 powered up with instance_id=i-0c30e29de39a40009, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"-3/192.168.13.79-3, instance_type=c7i.4xlarge-3, instance_id=i-0c30e29de39a40009, instance_type=c7i.4xlarge-3.0.0.",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2025-09-03T10:44:31.084,[2025-09-03T10:44:31.084] AUTH: user user2 failed password authentication from 10.1.2.109,auth_failure,high,security,"auth,security,login_failure",19,[2025-09-03T10:44:31.084] AUTH: user user2 failed password authentication from 10.1.2.109 from 10.1.2.109. [2025-09-03T10:44:31.084] AUTH: user user2 failed password authentication from 10.,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
2025-09-03T10:45:06.084,[2025-09-03T10:45:06.084] DISK: /var is 97% full on node cpu-16c-std-2,disk_usage_high,warning,storage,"disk,usage,high",20,DISK: [2025-09-03T10:45:06.084] DISK: [2025-09-03T10:45:06.084] [2025-09-03T10:45:06.084] [2025-09-03T10:,<*> DISK: /var is <*> full on node <*>,disk_usage_high
2025-09-03T10:45:41.084,[2025-09-03T10:45:41.084] NETWORK: latency to storage cluster increased to 89ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T10:45:41.084] NETWORK: latency to storage cluster increased to 89ms (no SLA breach) [2025-09-03T10:45:41.084] NETWORK: latency to storage cluster increased,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
2025-09-03T10:46:16.084,[2025-09-03T10:46:16.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:46:16.084] POWER: Power save mode: 334 nodes [2025-09-03T10:46:16.084] POWER: Power save mode: 334 nodes [2025-09-03T10:46:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T10:46:51.084,[2025-09-03T10:46:51.084] POWER: Power save mode: 335 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:46:51.084] POWER: Power save mode: 335 nodes [2025-09-03T10:46:51.084] POWER: Power save mode: 335 nodes [2025-09-03T10:46:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T10:47:26.084,[2025-09-03T10:47:26.084] _slurm_rpc_submit_batch_job: JobId=1051 InitPrio=30091 usec=5091,job_submit_batch,info,scheduler,"job,submit,batch",30,[2025-09-03T10:47:26.084] _slurm_rpc_submit_batch_job: JobId=1051 InitPrio=30091 usec=5091 [2025-09-03T10:47:,[2025-09-03T10:47:26.084] _slurm_rpc_submit_batch_job: JobId=1051 InitPrio=30091 usec=5091,job_submit_batch
2025-09-03T10:48:01.084,[2025-09-03T10:48:01.084] sched: Allocate JobId=1052 NodeList=cpu-8c-std-dy-c7i4xlarge-1 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T10:48:01.084] sched: [2025-09-03T10:48:01.084] sched: [2025-09-03T10:48:01.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2025-09-03T10:48:36.084,[2025-09-03T10:48:36.084] POWER: no more nodes to resume for job JobId=1053,cluster_power_save_complete,info,infra,"power,save,complete",12,[2025-09-03T10:48:36.084] POWER: no more nodes to resume for job [2025-09-03T10:48:36.084] POWER: no more nodes to resume for job10:36.084],<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2025-09-03T10:49:11.084,[2025-09-03T10:49:11.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-2,cluster_power_save_wake,info,infra,"power,save,wakeup",13,_save: waking nodes_save: waking nodes_save: waking nodes-c7i4xlarge-2-c7i4xlarge-2 cpu-8c-std-dy-c,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2025-09-03T10:49:46.084,[2025-09-03T10:49:46.084] Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-3 [2025-09-03T10:49:46.084].-c7i4xlarge-3-c7i4xlarge-3 [2025-09-03T10:49:46.084] [2025-09,<*> Node <*> now responding,node_now_responding
2025-09-03T10:50:21.084,"[2025-09-03T10:50:21.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-4/cpu-8c-std-dy-c7i4xlarge-4/192.168.13.80 powered up with instance_id=i-0c30e29de39a40010, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"-c7i4xlarge-4, instance_type=c7i.4xlarge-4.-4, instance_type=c7i.4xlarge-4. [2025-09-03T10:50:21.084] POWER: Node00",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2025-09-03T10:50:56.084,[2025-09-03T10:50:56.084] AUTH: user user3 failed password authentication from 10.1.3.110,auth_failure,high,security,"auth,security,login_failure",19,[2025-09-03T10:50:56.084] AUTH: user user3 failed password authentication from 10.1.3.110 from 10.1.3.110. [2025-09-03T10:50:56.084] AUTH: user user3 failed password authentication from 10.,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
2025-09-03T10:51:31.084,[2025-09-03T10:51:31.084] DISK: /var is 98% full on node cpu-16c-std-3,disk_usage_high,warning,storage,"disk,usage,high",20,[2025-09-03T10:51:31.084] [2025-09-03T10:51:31.084] [2025-09-03T10:51:31.084] [2025-09-03T10:51:31.084] [202,<*> DISK: /var is <*> full on node <*>,disk_usage_high
2025-09-03T10:52:06.084,[2025-09-03T10:52:06.084] NETWORK: latency to storage cluster increased to 90ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T10:52:06.084] NETWORK: latency to storage cluster increased to 90ms (no SLA breach) [2025-09-03T10:52:06.084] NETWORK: latency to storage cluster increased to,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
2025-09-03T10:52:41.084,[2025-09-03T10:52:41.084] POWER: Power save mode: 333 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:52:41.084] POWER: Power save mode: 333 nodes [2025-09-03T10:52:41.084] POWER: Power save mode: 333 nodes [2025-09-03T10:52:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T10:53:16.084,[2025-09-03T10:53:16.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:53:16.084] POWER: Power save mode: 334 nodes [2025-09-03T10:53:16.084] POWER: Power save mode: 334 nodes [2025-09-03T10:53:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T10:53:51.084,[2025-09-03T10:53:51.084] _slurm_rpc_submit_batch_job: JobId=1061 InitPrio=30101 usec=5101,job_submit_batch,info,scheduler,"job,submit,batch",31,[2025-09-03T10:53:51.084] _slurm_rpc_submit_batch_job: [2025-09-03T10:53:51.084] _slurm_rpc_submit_,[2025-09-03T10:53:51.084] _slurm_rpc_submit_batch_job: JobId=1061 InitPrio=30101 usec=5101,job_submit_batch
2025-09-03T10:54:26.084,[2025-09-03T10:54:26.084] sched: Allocate JobId=1062 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T10:54:26.084] sched: [2025-09-03T10:54:26.084] sched: [2025-09-03T10:54:26.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2025-09-03T10:55:01.084,[2025-09-03T10:55:01.084] POWER: no more nodes to resume for job JobId=1063,cluster_power_save_complete,info,infra,"power,save,complete",12,[2025-09-03T10:55:01.084] POWER: no more nodes to resume for job [2025-09-03T10:55:01.084] POWER: no more nodes to resume for job: POWER: no more no,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2025-09-03T10:55:36.084,[2025-09-03T10:55:36.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-3,cluster_power_save_wake,info,infra,"power,save,wakeup",13,_save: waking nodes_save: waking nodes_save: waking nodes-c7i4xlarge-3-c7i4-c7i4xlarge-3-c7i4 POWER,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2025-09-03T10:56:11.084,[2025-09-03T10:56:11.084] Node cpu-8c-std-dy-c7i4xlarge-4 now responding,node_now_responding,info,infra,"node,recovered,online",14,node cpu-8c-std-dy-c7i4xlarge-4 now responding..-c7i4xlarge-4-c7i4xlarge-4 [2025-09-03T10:56:11.084] [202,<*> Node <*> now responding,node_now_responding
2025-09-03T10:56:46.084,"[2025-09-03T10:56:46.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-5/cpu-8c-std-dy-c7i4xlarge-5/192.168.13.81 powered up with instance_id=i-0c30e29de39a40011, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"_id=i-0c30e29de39a40011, instance_type=c7i.4xlarge-5.-5, instance_type=c7i.4xlarge-5.46.084] [2025-09-03T10:56:",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2025-09-03T10:57:21.084,[2025-09-03T10:57:21.084] AUTH: user user4 failed password authentication from 10.1.4.111,auth_failure,high,security,"auth,security,login_failure",19,[2025-09-03T10:57:21.084] AUTH: user user4 failed password authentication from 10.1.4.111. [2025-09-03T10:57:21.084] AUTH: user user4 failed password authentication from 10.1.4.111.,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
2025-09-03T10:57:56.084,[2025-09-03T10:57:56.084] DISK: /var is 99% full on node cpu-16c-std-4,disk_usage_high,warning,storage,"disk,usage,high",20,[2025-09-03T10:57:56.084] DISK: [2025-09-03T10:57:56.084] [2025-09-03T10:57:56.084] [2025-09-03T10:57:56.0,<*> DISK: /var is <*> full on node <*>,disk_usage_high
2025-09-03T10:58:31.084,[2025-09-03T10:58:31.084] NETWORK: latency to storage cluster increased to 91ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T10:58:31.084] NETWORK: latency to storage cluster increased to 91ms (no SLA breach). [2025-09-03T10:58:31.084] NETWORK: latency to storage cluster increased,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
2025-09-03T10:59:06.084,[2025-09-03T10:59:06.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:59:06.084] POWER: Power save mode: 334 nodes [2025-09-03T10:59:06.084] POWER: Power save mode: 334 nodes [2025-09-03T10:59:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T10:59:41.084,[2025-09-03T10:59:41.084] POWER: Power save mode: 335 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:59:41.084] POWER: Power save mode: 335 nodes [2025-09-03T10:59:41.084] POWER: Power save mode: 335 nodes [2025-09-03T10:59:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T11:00:16.084,[2025-09-03T11:00:16.084] _slurm_rpc_submit_batch_job: JobId=1071 InitPrio=30111 usec=5111,job_submit_batch,info,scheduler,"job,submit,batch",32,[2025-09-03T11:00:16.084] _slurm_rpc_submit_batch_job: JobId=1071 InitPrio=30111 usec=5111 [2025-09-03T11:00:16.0,[2025-09-03T11:00:16.084] _slurm_rpc_submit_batch_job: JobId=1071 InitPrio=30111 usec=5111,job_submit_batch
2025-09-03T11:00:51.084,[2025-09-03T11:00:51.084] sched: Allocate JobId=1072 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T11:00:51.084] sched: [2025-09-03T11:00:51.084] sched: [2025-09-03T11:00:51.084] sched: [2025-09-03T1,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2025-09-03T11:01:26.084,[2025-09-03T11:01:26.084] POWER: no more nodes to resume for job JobId=1073,cluster_power_save_complete,info,infra,"power,save,complete",12,[2025-09-03T11:01:26.084] [2025-09-03T11:01:26.084] [2025-09-03T11:01:26.084] [2025-09-03T11:01:26.084] [202,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2025-09-03T11:02:01.084,[2025-09-03T11:02:01.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",13,power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4_save: waking nodes cpu-std-dy-c7i4xlarge-4-,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2025-09-03T11:02:36.084,[2025-09-03T11:02:36.084] Node cpu-8c-std-dy-c7i4xlarge-5 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-5 [2025-09-03T11:02:36.084].-c7i4xlarge-5-c7i4xlarge-5 [2025-09-03T11:02:36.084] [2025-09,<*> Node <*> now responding,node_now_responding
2025-09-03T11:03:11.084,"[2025-09-03T11:03:11.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-1/cpu-8c-std-dy-c7i4xlarge-1/192.168.13.82 powered up with instance_id=i-0c30e29de39a40012, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"-std-dy-c7i4xlarge-1/192.168.13.82-1, instance_type=c7i.4xlarge-1.0] [2025-09-03T11:03:11.084] [2025-09-03",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2025-09-03T11:03:46.084,[2025-09-03T11:03:46.084] AUTH: user user5 failed password authentication from 10.1.5.112,auth_failure,high,security,"auth,security,login_failure",19,[2025-09-03T11:03:46.084] AUTH: user user5 failed password authentication from 10.1.5.112 user user5 failed password authentication from 10.1.5.112 user user5 failed password authentication from 10.1.5.112 user user5 failed password authentication from,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
2025-09-03T11:04:21.084,[2025-09-03T11:04:21.084] DISK: /var is 88% full on node cpu-16c-std-1,disk_usage_high,warning,storage,"disk,usage,high",20,[2025-09-03T11:04:21.084] DISK: /var is 88% full on node cpu-16c-std-1. [2025-09-03T11:04:21.084] DISK: /var is,<*> DISK: /var is <*> full on node <*>,disk_usage_high
2025-09-03T11:04:56.084,[2025-09-03T11:04:56.084] NETWORK: latency to storage cluster increased to 92ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T11:04:56.084] NETWORK: latency to storage cluster increased to 92ms (no SLA breach). [2025-09-03T11:04:56.084] NETWORK: latency to storage cluster increased,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
2025-09-03T11:05:31.084,[2025-09-03T11:05:31.084] POWER: Power save mode: 333 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T11:05:31.084] POWER: Power save mode: 333 nodes: [2025-09-03T11:05:31.084] POWER: [2025-09-03T11:05:31.084] POWER:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T11:06:06.084,[2025-09-03T11:06:06.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T11:06:06.084] POWER: Power save mode: 334 nodes [2025-09-03T11:06:06.084] POWER: Power save mode: 334 nodes [2025-09-03T11:06:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T11:06:41.084,[2025-09-03T11:06:41.084] _slurm_rpc_submit_batch_job: JobId=1081 InitPrio=30121 usec=5121,job_submit_batch,info,scheduler,"job,submit,batch",33,[2025-09-03T11:06:41.084] _slurm_rpc_submit_batch_job: [2025-09-03T11:06:41.084] _slurm_rpc_submit_,[2025-09-03T11:06:41.084] _slurm_rpc_submit_batch_job: JobId=1081 InitPrio=30121 usec=5121,job_submit_batch
2025-09-03T11:07:16.084,[2025-09-03T11:07:16.084] sched: Allocate JobId=1082 NodeList=cpu-8c-std-dy-c7i4xlarge-4 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T11:07:16.084] sched: [2025-09-03T11:07:16.084] sched: [2025-09-03T11:07:16.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2025-09-03T11:07:51.084,[2025-09-03T11:07:51.084] POWER: no more nodes to resume for job JobId=1083,cluster_power_save_complete,info,infra,"power,save,complete",12,POWER: no more nodes to resume for job [2025-09-03T11:07:51.084] POWER: no more nodes to resume for job JobId=1083 [2025-09-03T11:07:51.084] POWER:,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2025-09-03T11:08:26.084,[2025-09-03T11:08:26.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-5,cluster_power_save_wake,info,infra,"power,save,wakeup",13,_save: waking nodes_save: waking nodes_save: waking nodes cpu-8c-std-dy-c7i4xlarge-5-c7i4x- cpu,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2025-09-03T11:09:01.084,[2025-09-03T11:09:01.084] Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",14,cpu 8c-std-dy-c7i4xlarge-1 now responding. [2025-09-03T11:09:01.084]..-c7i4xlarge-1 now responding. [2025-09-03T11,<*> Node <*> now responding,node_now_responding
2025-09-03T11:09:36.084,"[2025-09-03T11:09:36.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-2/cpu-8c-std-dy-c7i4xlarge-2/192.168.13.83 powered up with instance_id=i-0c30e29de39a40013, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"-2/192.168.13.83-2, instance_type=c7i.4xlarge-2.00-2/cpu-8c-std-dy-c7i4xlarge-2/192.168.13.83-2.168",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2025-09-03T11:10:11.084,[2025-09-03T11:10:11.084] AUTH: user user6 failed password authentication from 10.1.6.113,auth_failure,high,security,"auth,security,login_failure",19,[2025-09-03T11:10:11.084] AUTH: user user6 failed password authentication from 10.1.6.113. [2025-09-03T11:10:11.084] AUTH: user user6 failed password authentication from 10.1.6.113.,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
2025-09-03T11:10:46.084,[2025-09-03T11:10:46.084] DISK: /var is 89% full on node cpu-16c-std-2,disk_usage_high,warning,storage,"disk,usage,high",20,[2025-09-03T11:10:46.084] DISK: /var is 89% full on node cpu-16c-std-2. [2025-09-03T11:10:46.084] DISK: /var is,<*> DISK: /var is <*> full on node <*>,disk_usage_high
2025-09-03T11:11:21.084,[2025-09-03T11:11:21.084] NETWORK: latency to storage cluster increased to 93ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T11:11:21.084] NETWORK: latency to storage cluster increased to 93ms (no SLA breach) [2025-09-03T11:11:21.084] NETWORK: latency to storage cluster increased,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
2025-09-03T11:11:56.084,[2025-09-03T11:11:56.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T11:11:56.084] POWER: Power save mode: 334 nodes [2025-09-03T11:11:56.084] POWER: Power save mode: 334 nodes [2025-09-03T11:11:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T11:12:31.084,[2025-09-03T11:12:31.084] POWER: Power save mode: 335 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T11:12:31.084] POWER: Power save mode: 335 nodes [2025-09-03T11:12:31.084] POWER: Power save mode: 335 nodes [2025-09-03T11:12:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T11:13:06.084,[2025-09-03T11:13:06.084] _slurm_rpc_submit_batch_job: JobId=1091 InitPrio=30131 usec=5131,job_submit_batch,info,scheduler,"job,submit,batch",34,[2025-09-03T11:13:06.084] _slurm_rpc_submit_batch_job: JobId=1091 InitPrio=30131 usec=5131 [2025-09-03T11:13:,[2025-09-03T11:13:06.084] _slurm_rpc_submit_batch_job: JobId=1091 InitPrio=30131 usec=5131,job_submit_batch
2025-09-03T11:13:41.084,[2025-09-03T11:13:41.084] sched: Allocate JobId=1092 NodeList=cpu-8c-std-dy-c7i4xlarge-5 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T11:13:41.084] sched: [2025-09-03T11:13:41.084] sched: [2025-09-03T11:13:41.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2025-09-03T11:14:16.084,[2025-09-03T11:14:16.084] POWER: no more nodes to resume for job JobId=1093,cluster_power_save_complete,info,infra,"power,save,complete",12,POWER: no more nodes to resume for job [2025-09-03T11:14:16.084] POWER: no more nodes to resume for job JobId=1093 [2025-09-03T11:14:16.084] POWER:,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2025-09-03T11:14:51.084,[2025-09-03T11:14:51.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-1,cluster_power_save_wake,info,infra,"power,save,wakeup",13,cpu-8c-std-dy-c7i4xlarge-1 cpu-8c-std-dy-c7i4xlarge-1_save: waking nodes-- cpu--,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2025-09-03T11:15:21.084,[2025-09-03T11:15:21.084] Node cpu-8c-std-dy-c7i4xlarge-2 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-2 now responding. [2025-09-03T11:15:21.084]. [2025-09-03T11:15:21.084]-c7i4xlarge-2 now responding.-c7i4x,<*> Node <*> now responding,node_now_responding
