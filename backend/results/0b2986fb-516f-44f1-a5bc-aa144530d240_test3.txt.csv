timestamp,raw,final_label,severity,owner_team,tags,cluster_id,t5_template,cluster_template,auto_label
2025-09-03T10:58:31.084,[2025-09-03T10:58:31.084] NETWORK: latency to storage cluster increased to 91ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T10:58:31.084] NETWORK: latency to storage cluster increased to 91ms (no SLA breach). [2025-09-03T10:58:31.084] NETWORK: latency to storage cluster increased,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
2025-09-03T10:59:06.084,[2025-09-03T10:59:06.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:59:06.084] POWER: Power save mode: 334 nodes [2025-09-03T10:59:06.084] POWER: Power save mode: 334 nodes [2025-09-03T10:59:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T10:59:41.084,[2025-09-03T10:59:41.084] POWER: Power save mode: 335 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:59:41.084] POWER: Power save mode: 335 nodes [2025-09-03T10:59:41.084] POWER: Power save mode: 335 nodes [2025-09-03T10:59:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T11:00:16.084,[2025-09-03T11:00:16.084] _slurm_rpc_submit_batch_job: JobId=1071 InitPrio=30111 usec=5111,job_submit_batch,info,scheduler,"job,submit,batch",32,[2025-09-03T11:00:16.084] _slurm_rpc_submit_batch_job: JobId=1071 InitPrio=30111 usec=5111 [2025-09-03T11:00:16.0,[2025-09-03T11:00:16.084] _slurm_rpc_submit_batch_job: JobId=1071 InitPrio=30111 usec=5111,job_submit_batch
2025-09-03T11:00:51.084,[2025-09-03T11:00:51.084] sched: Allocate JobId=1072 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T11:00:51.084] sched: [2025-09-03T11:00:51.084] sched: [2025-09-03T11:00:51.084] sched: [2025-09-03T1,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2025-09-03T11:01:26.084,[2025-09-03T11:01:26.084] POWER: no more nodes to resume for job JobId=1073,cluster_power_save_complete,info,infra,"power,save,complete",12,[2025-09-03T11:01:26.084] [2025-09-03T11:01:26.084] [2025-09-03T11:01:26.084] [2025-09-03T11:01:26.084] [202,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2025-09-03T11:02:01.084,[2025-09-03T11:02:01.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",13,power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4_save: waking nodes cpu-std-dy-c7i4xlarge-4-,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2025-09-03T11:02:36.084,[2025-09-03T11:02:36.084] Node cpu-8c-std-dy-c7i4xlarge-5 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-5 [2025-09-03T11:02:36.084].-c7i4xlarge-5-c7i4xlarge-5 [2025-09-03T11:02:36.084] [2025-09,<*> Node <*> now responding,node_now_responding
2025-09-03T11:03:11.084,"[2025-09-03T11:03:11.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-1/cpu-8c-std-dy-c7i4xlarge-1/192.168.13.82 powered up with instance_id=i-0c30e29de39a40012, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"-std-dy-c7i4xlarge-1/192.168.13.82-1, instance_type=c7i.4xlarge-1.0] [2025-09-03T11:03:11.084] [2025-09-03",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2025-09-03T11:03:46.084,[2025-09-03T11:03:46.084] AUTH: user user5 failed password authentication from 10.1.5.112,auth_failure,high,security,"auth,security,login_failure",19,[2025-09-03T11:03:46.084] AUTH: user user5 failed password authentication from 10.1.5.112 user user5 failed password authentication from 10.1.5.112 user user5 failed password authentication from 10.1.5.112 user user5 failed password authentication from,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
2025-09-03T11:04:21.084,[2025-09-03T11:04:21.084] DISK: /var is 88% full on node cpu-16c-std-1,disk_usage_high,warning,storage,"disk,usage,high",20,[2025-09-03T11:04:21.084] DISK: /var is 88% full on node cpu-16c-std-1. [2025-09-03T11:04:21.084] DISK: /var is,<*> DISK: /var is <*> full on node <*>,disk_usage_high
2025-09-03T11:04:56.084,[2025-09-03T11:04:56.084] NETWORK: latency to storage cluster increased to 92ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T11:04:56.084] NETWORK: latency to storage cluster increased to 92ms (no SLA breach). [2025-09-03T11:04:56.084] NETWORK: latency to storage cluster increased,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
2025-09-03T11:05:31.084,[2025-09-03T11:05:31.084] POWER: Power save mode: 333 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T11:05:31.084] POWER: Power save mode: 333 nodes: [2025-09-03T11:05:31.084] POWER: [2025-09-03T11:05:31.084] POWER:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T11:06:06.084,[2025-09-03T11:06:06.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T11:06:06.084] POWER: Power save mode: 334 nodes [2025-09-03T11:06:06.084] POWER: Power save mode: 334 nodes [2025-09-03T11:06:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T11:06:41.084,[2025-09-03T11:06:41.084] _slurm_rpc_submit_batch_job: JobId=1081 InitPrio=30121 usec=5121,job_submit_batch,info,scheduler,"job,submit,batch",33,[2025-09-03T11:06:41.084] _slurm_rpc_submit_batch_job: [2025-09-03T11:06:41.084] _slurm_rpc_submit_,[2025-09-03T11:06:41.084] _slurm_rpc_submit_batch_job: JobId=1081 InitPrio=30121 usec=5121,job_submit_batch
2025-09-03T11:07:16.084,[2025-09-03T11:07:16.084] sched: Allocate JobId=1082 NodeList=cpu-8c-std-dy-c7i4xlarge-4 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T11:07:16.084] sched: [2025-09-03T11:07:16.084] sched: [2025-09-03T11:07:16.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2025-09-03T11:07:51.084,[2025-09-03T11:07:51.084] POWER: no more nodes to resume for job JobId=1083,cluster_power_save_complete,info,infra,"power,save,complete",12,POWER: no more nodes to resume for job [2025-09-03T11:07:51.084] POWER: no more nodes to resume for job JobId=1083 [2025-09-03T11:07:51.084] POWER:,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2025-09-03T11:08:26.084,[2025-09-03T11:08:26.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-5,cluster_power_save_wake,info,infra,"power,save,wakeup",13,_save: waking nodes_save: waking nodes_save: waking nodes cpu-8c-std-dy-c7i4xlarge-5-c7i4x- cpu,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2025-09-03T11:09:01.084,[2025-09-03T11:09:01.084] Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",14,cpu 8c-std-dy-c7i4xlarge-1 now responding. [2025-09-03T11:09:01.084]..-c7i4xlarge-1 now responding. [2025-09-03T11,<*> Node <*> now responding,node_now_responding
2025-09-03T11:09:36.084,"[2025-09-03T11:09:36.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-2/cpu-8c-std-dy-c7i4xlarge-2/192.168.13.83 powered up with instance_id=i-0c30e29de39a40013, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"-2/192.168.13.83-2, instance_type=c7i.4xlarge-2.00-2/cpu-8c-std-dy-c7i4xlarge-2/192.168.13.83-2.168",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2025-09-03T11:10:11.084,[2025-09-03T11:10:11.084] AUTH: user user6 failed password authentication from 10.1.6.113,auth_failure,high,security,"auth,security,login_failure",19,[2025-09-03T11:10:11.084] AUTH: user user6 failed password authentication from 10.1.6.113. [2025-09-03T11:10:11.084] AUTH: user user6 failed password authentication from 10.1.6.113.,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
2025-09-03T11:10:46.084,[2025-09-03T11:10:46.084] DISK: /var is 89% full on node cpu-16c-std-2,disk_usage_high,warning,storage,"disk,usage,high",20,[2025-09-03T11:10:46.084] DISK: /var is 89% full on node cpu-16c-std-2. [2025-09-03T11:10:46.084] DISK: /var is,<*> DISK: /var is <*> full on node <*>,disk_usage_high
2025-09-03T11:11:21.084,[2025-09-03T11:11:21.084] NETWORK: latency to storage cluster increased to 93ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T11:11:21.084] NETWORK: latency to storage cluster increased to 93ms (no SLA breach) [2025-09-03T11:11:21.084] NETWORK: latency to storage cluster increased,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
2025-09-03T11:11:56.084,[2025-09-03T11:11:56.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T11:11:56.084] POWER: Power save mode: 334 nodes [2025-09-03T11:11:56.084] POWER: Power save mode: 334 nodes [2025-09-03T11:11:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T11:12:31.084,[2025-09-03T11:12:31.084] POWER: Power save mode: 335 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T11:12:31.084] POWER: Power save mode: 335 nodes [2025-09-03T11:12:31.084] POWER: Power save mode: 335 nodes [2025-09-03T11:12:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2025-09-03T11:13:06.084,[2025-09-03T11:13:06.084] _slurm_rpc_submit_batch_job: JobId=1091 InitPrio=30131 usec=5131,job_submit_batch,info,scheduler,"job,submit,batch",34,[2025-09-03T11:13:06.084] _slurm_rpc_submit_batch_job: JobId=1091 InitPrio=30131 usec=5131 [2025-09-03T11:13:,[2025-09-03T11:13:06.084] _slurm_rpc_submit_batch_job: JobId=1091 InitPrio=30131 usec=5131,job_submit_batch
2025-09-03T11:13:41.084,[2025-09-03T11:13:41.084] sched: Allocate JobId=1092 NodeList=cpu-8c-std-dy-c7i4xlarge-5 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T11:13:41.084] sched: [2025-09-03T11:13:41.084] sched: [2025-09-03T11:13:41.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2025-09-03T11:14:16.084,[2025-09-03T11:14:16.084] POWER: no more nodes to resume for job JobId=1093,cluster_power_save_complete,info,infra,"power,save,complete",12,POWER: no more nodes to resume for job [2025-09-03T11:14:16.084] POWER: no more nodes to resume for job JobId=1093 [2025-09-03T11:14:16.084] POWER:,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2025-09-03T11:14:51.084,[2025-09-03T11:14:51.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-1,cluster_power_save_wake,info,infra,"power,save,wakeup",13,cpu-8c-std-dy-c7i4xlarge-1 cpu-8c-std-dy-c7i4xlarge-1_save: waking nodes-- cpu--,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2025-09-03T11:15:21.084,[2025-09-03T11:15:21.084] Node cpu-8c-std-dy-c7i4xlarge-2 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-2 now responding. [2025-09-03T11:15:21.084]. [2025-09-03T11:15:21.084]-c7i4xlarge-2 now responding.-c7i4x,<*> Node <*> now responding,node_now_responding
