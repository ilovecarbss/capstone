timestamp,raw,final_label,severity,owner_team,tags,cluster_id,t5_template,cluster_template,auto_label
,[2025-09-03T02:09:33.0000 POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete,info,infra,"power,save,complete",1,] [2025-09-03T02:09:33.0000 POWER: no more nodes resume for job] [2025-09-03T02:09:33.0000 POWER: no more nodes resume for job] POWER: no more nodes:,[2025-09-03T02:09:33.0000 POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete
,[2025-09-03T15:09:31.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold...].] [2025-09-03T15:09:31.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T15:09:31.0000 NETWORK,[2025-09-03T15:09:31.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,"[2025-09-03T15:00:57.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T15:00:57.0000] [2025-09-03T15:00:57.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-0","[2025-09-03T15:00:57.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
,[2025-09-03T11:46:13.0000 _slurm_rpc_submit_batch_job: JobId=973 InitPrio=30459 usec=6689,job_submit_batch,info,scheduler,"job,submit,batch",4,[2025-09-03T11:46:13.0000 _slurm_rpc_submit_batch_job: [2025-09-03T11:46:13.0000 _slurm_rpc_submit_batch,[2025-09-03T11:46:13.0000 _slurm_rpc_submit_batch_job: JobId=973 InitPrio=30459 usec=6689,job_submit_batch
,[2025-09-03T19:18:59.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T19:18:59.0000.2025-09-03T19:18:59.0000 State of 0 reservations recovered recovered.0000] [2025-09-03T19:18:59.0000 State of 0 reservations recovered recovered.,[2025-09-03T19:18:59.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations
,"[2025-09-03T03:47:41.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09T03:47:41.0000] [2025-09-03T03:47:41.0000,bf_resolution=60,bf_window=4320]5-09-03T03:47:41.0000] [202","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
,[2025-09-03T21:31:32.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T21:31:32.0000 Running as primary controller.] [2025-09-03T21:31:32.0000 Running as primary controller.,[2025-09-03T21:31:32.0000 Running as primary controller,controller_primary
,[2025-09-03T02:13:11.0000 POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete,info,infra,"power,save,complete",1,] [2025-09-03T02:13:11.0000 POWER: no more nodes resume for job] [2025-09-03T02:13:11.0000 POWER: no more nodes resume for job] POWER: no more nodes resume for job,<*> POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete
,[2025-09-03T19:57:47.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T19:57:47.0000 AUTH: authentication failure for user admin] [2025-09-03T19:57:47.0000 AUTH: authentication failure for user admin] [2025-09-03T19:57:47.0000,[2025-09-03T19:57:47.0000 AUTH: authentication failure for user admin,auth_failure
,[2025-09-03T08:13:32.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T08:13:32.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding now now]-c7i4xlarge-3]-c7i4x-,[2025-09-03T08:13:32.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding
,[2025-09-03T17:27:32.0000 _slurm_rpc_submit_batch_job: JobId=1008 InitPrio=35944 usec=6535,job_submit_batch,info,scheduler,"job,submit,batch",9,5-09-03T17:27:32.0000 _slurm_rpc_submit_batch_job: [2025-09-03T17:27:32.0000 _slurm_rpc_submit_batch_,[2025-09-03T17:27:32.0000 _slurm_rpc_submit_batch_job: JobId=1008 InitPrio=35944 usec=6535,job_submit_batch
,[2025-09-03T17:38:06.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-3 now responding.]-c7i4xlarge-3] [2025-09-03T17:38:06.0000 Node cpu-8c-std-d,<*> Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding
,[2025-09-03T21:13:50.0000 _slurm_rpc_submit_batch_job: JobId=991 InitPrio=32548 usec=5610,job_submit_batch,info,scheduler,"job,submit,batch",10,[2025-09-03T21:13:13.0000 _slurm_rpc_submit_batch_job: JobId=991 InitPrio=32548 usec=5610 _slurm_rpc_,[2025-09-03T21:13:50.0000 _slurm_rpc_submit_batch_job: JobId=991 InitPrio=32548 usec=5610,job_submit_batch
,[2025-09-03T22:27:40.0000 POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete,info,infra,"power,save,complete",1,] [2025-09-03T22:27:40.0000 POWER: no more nodes resume for job] [2025-09-03T22:27:40.0000 POWER: no more nodes resume for job]:40.0000 POWER: no,<*> POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete
,[2025-09-03T13:20:13.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T13:20:13.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding. [2025-09-03T13:20:13.0000 Node cpu-8c-st,<*> Node <*> now responding,node_now_responding
,[2025-09-03T01:27:18.0000 _slurm_rpc_submit_batch_job: JobId=976 InitPrio=35529 usec=5381,job_submit_batch,info,scheduler,"job,submit,batch",11,[2025-09-03T01:27:18.0000 _slurm_rpc_submit_batch_job: [2025-09-03T01:27:18.0000 _slurm_rpc_submit_batch,[2025-09-03T01:27:18.0000 _slurm_rpc_submit_batch_job: JobId=976 InitPrio=35529 usec=5381,job_submit_batch
,[2025-09-03T13:15:14.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T13:15:14.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding now now]-c7i4xlarge-5] [2025-09-03T13:,<*> Node <*> now responding,node_now_responding
,"[2025-09-03T16:53:25.0000 Gres Name=gpu Type=l4 Count=7 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T16:53:25.0000 Gres] [2025-09-03T16:53:25.0000 Gres] [2025-09-03T16:53:25.0000 Gres] [2025-09-03T16:53:25,"[2025-09-03T16:53:25.0000 Gres Name=gpu Type=l4 Count=7 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,[2025-09-03T13:18:20.0000 sched: Allocate JobId=1011 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T13:18:20.0000 sched:-dy-c7i4xlarge-2]d-dy-c7i4xlarge-2] [2025-09-03T13:18:20.0000 sched,[2025-09-03T13:18:20.0000 sched: Allocate JobId=1011 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T21:30:46.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error,high,storage,"disk,scrubber,error",14,] [2025-09T21:30:46.0000 DISK: scrubber error while cleaning temp files..]..]. [2025-09T21:30:46.0000 DISK: scrubber error while cleaning temp files.05,[2025-09-03T21:30:46.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error
,"[2025-09-03T16:18:49.0000 Gres Name=gpu Type=l4 Count=7 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T16:18:49.0000] [2025-09-03T16:18:49.0000 Gres [2025-09-03T16:18:49.0000 Gres [2025-09-03T16:18:49.0000 Gres,"<*> Gres Name=gpu Type=l4 Count=7 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,[2025-09-03T00:43:49.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error,high,storage,"disk,scrubber,error",14,] [2025-09-03T00:43:40.00 DISK: scrubber error while cleaning temp files..]] [2025-09-03T00:43:40.00 DISK: scrubber error while cleaning temp files..] [202,<*> DISK: scrubber error while cleaning temp files,disk_scrubber_error
,[2025-09-03T23:08:09.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold.. latency to storage cluster exceeded threshold..] [2025-09-03T23:08:09.0000 NETWORK: latency to storage cluster exceeded threshold.. Storage cluster exceeded threshold. Storage cluster exceeded threshold.:,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,"[2025-09-03T22:39:32.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T22:39:32.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T22:39:32.0000] [","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
,"[2025-09-03T18:34:05.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T18:34:05.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T18:34:05.0000 [202","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
,[2025-09-03T19:46:44.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T19:46:44.0000.2025-09-03T19:46:44.0000 State of 0 reservations recovered recovered.0000] [2025-09-03T19:46:44.0000 State of 0 reservations recovered recovered.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
,[2025-09-03T15:10:17.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,_id=i-1985197] [2025-09-03T15:10:10.0000] [2025-09-03T15:10:10.0000]-c7i4xlarge-4-c7i4xlarge-4-c7i,[2025-09-03T15:10:17.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
,[2025-09-03T14:25:09.0000 sched: Allocate JobId=960 NodeList=cpu-8c-std-dy-c7i4xlarge-5 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T14:25:09.0000 sched:-dy-c7i4xlarge-5]dy-c7i4xlarge-5] [2025-09-03T14:25:09.0000 sched: Allocate,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,"[2025-09-03T16:34:22.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T16:34:22.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T16:34:22.0000 [202","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
,[2025-09-03T14:54:38.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error,high,storage,"disk,scrubber,error",14,[2025-09-03T14:54:38.0000 DISK: scrubber error while cleaning temp files.] [2025-09-03T14:54:38.0000 DISK: scrubber error while cleaning temp files.,<*> DISK: scrubber error while cleaning temp files,disk_scrubber_error
,[2025-09-03T15:11:34.0000 sched: Allocate JobId=964 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T15:11:34.0000 sched:-dy-c7i4xlarge-3]d-dy-c7i4xlarge-3] [2025-09-03T15:11:34.0000 sched,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T01:55:12.0000 Node cpu-8c-std-dy-c7i4xlarge-2 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-2 now responding.]-c7i4xlarge-2 now responding-c7i4-c75-09-03T01:55:12.00005-09,<*> Node <*> now responding,node_now_responding
,"[2025-09-03T06:25:23.0000 Gres Name=gpu Type=l4 Count=4 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T06:25:23.0000 Gres] [2025-09-03T06:25:23.0000 Gres] [2025-09-03T06:25:23.0000 Gres] [2025-09-03T06:25:23,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,[2025-09-03T02:30:47.0000 _slurm_rpc_submit_batch_job: JobId=989 InitPrio=32046 usec=6827,job_submit_batch,info,scheduler,"job,submit,batch",16,[2025-09-03T02:30:47.0000 _slurm_rpc_submit_batch_job: [2025-09-03T02:30:47.0000 _slurm_rpc_submit_batch_job,[2025-09-03T02:30:47.0000 _slurm_rpc_submit_batch_job: JobId=989 InitPrio=32046 usec=6827,job_submit_batch
,[2025-09-03T23:57:16.0000 _slurm_rpc_submit_batch_job: JobId=956 InitPrio=35326 usec=6136,job_submit_batch,info,scheduler,"job,submit,batch",17,5-09-03T23:57:16.0000 [2025-09-03T23:57:16.0000 _slurm_rpc_submit_batch_job:]] [2025-09-03T23:57:16.,[2025-09-03T23:57:16.0000 _slurm_rpc_submit_batch_job: JobId=956 InitPrio=35326 usec=6136,job_submit_batch
,[2025-09-03T04:16:20.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,cpu=8c-std-dy-c7i4xlarge-4] [2025-09-03T04:16:20.0000]-c7i4xlarge-4]-c7i4xlarge-4]-,<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
,[2025-09-03T18:45:04.0000 POWER: Power save mode: 338 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,] [2025-09-03T18:45:04.0000 POWER: Power save mode: 338 nodes]2025-09-03T18:45:04.0000 POWER:: Power save mode: 338 nodes] [2025-09-03T18,[2025-09-03T18:45:04.0000 POWER: Power save mode: 338 nodes,cluster_power_save_mode
,[2025-09-03T05:34:01.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-3 now responding.]c7i4xlarge-3]-c7i4xlarge-3]-c7i4xlarge-3]-5-09T05,<*> Node <*> now responding,node_now_responding
,"[2025-09-03T05:54:09.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T05:54:09.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T05:54:09.0000] [2025-0","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
,[2025-09-03T09:50:03.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold....] [2025-09-03T09:50:03.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T09:50:03.0000 NETWORK: latency,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T10:31:17.0000 sched: Allocate JobId=1027 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,] [2025-09-03T10:31:17.0000 sched: Allocate JobId=1027-dy-c7i4xlarge-3]-dy-c7i4xlarge-3] [2025-09-03T,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T17:58:56.0000 POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",19,] [2025-09-03T17:58:56.0000 POWER: power_save: waking nodes]] [2025-09-03T17:58:56.0000 POWER: power_save: waking nodes],[2025-09-03T17:58:56.0000 POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake
,[2025-09-03T01:48:13.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T01:48:13.0000 Running as primary controller.] [2025-09-03T01:48:13.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
,[2025-09-03T13:51:07.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold..].] [2025-09-03T13:51:07.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T13:51:07.0000 NETWORK: latency,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T13:11:43.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09-03T13:11:43.0000 select/cons_tres: preparing for 19 partitions. [2025-09-03T13:11:43.0000 select/cons_tres: preparing for 19 partitions.,[2025-09-03T13:11:43.0000 select/cons_tres: preparing for 19 partitions,unknown
,[2025-09-03T16:34:16.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T14:27:03.0000 sched: Allocate JobId=1021 NodeList=cpu-8c-std-dy-c7i4xlarge-5 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T14:27:03.0000 sched: Allocate JobId=1021-dy-c7i4xlarge-5]-dy-c7i4xlarge-5] [2025-09-03T14:27,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T03:43:14.0000 POWER: Power save mode: 330 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T03:43:14.0000 POWER: Power save mode: 330 nodes. [2025-09-03T03:43:14.0000 POWER: Power save mode: 330 nodes.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T03:01:27.0000 _slurm_rpc_submit_batch_job: JobId=1010 InitPrio=33871 usec=5060,job_submit_batch,info,scheduler,"job,submit,batch",21,[2025-09-03T03:01:27.0000 _slurm_rpc_submit_batch_job: [2025-09-03T03:01:27.0000 _slurm_rpc_submit_batch,[2025-09-03T03:01:27.0000 _slurm_rpc_submit_batch_job: JobId=1010 InitPrio=33871 usec=5060,job_submit_batch
,[2025-09-03T16:49:05.0000 POWER: Power save mode: 338 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T16:49:05.0000 POWER: Power save mode: 338 nodes. [2025-09-03T16:49:05.0000 POWER: Power save mode: 338 nodes.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,"[2025-09-03T00:51:52.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,2025-09-03T00:51:52.0000] [2025-09-03T00:51:52.0000 [2025-09-03T00:51:52.0000 [2025-09-03T00:51:52.0000] [2025-09-,"<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
,[2025-09-03T06:20:19.0000 _slurm_rpc_submit_batch_job: JobId=955 InitPrio=33633 usec=5519,job_submit_batch,info,scheduler,"job,submit,batch",22,[2025-09-03T06:20:19.0000 _slurm_rpc_submit_batch_job: [2025-09-03T06:20:19.0000 _slurm_rpc_submit_batch,[2025-09-03T06:20:19.0000 _slurm_rpc_submit_batch_job: JobId=955 InitPrio=33633 usec=5519,job_submit_batch
,[2025-09-03T09:20:54.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T09:20:54.0000 Running as primary controller.] [2025-09-03T09:20:54.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
,[2025-09-03T19:12:05.0000 sched: Allocate JobId=1002 NodeList=cpu-8c-std-dy-c7i4xlarge-4 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,[2025-09-03T19:12:05.0000 sched: Allocate JobId=1002-dy-c7i4xlarge-4]-dy-c7i4xlarge-4] [2025-09-03T19,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T17:05:57.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T17:05:57.0000 Running as primary controller.] [2025-09-03T17:05:57.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
,[2025-09-03T01:16:28.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,[2025-09-03T01:16:28.0000 State of 0 reservations recovered recovered. [2025-09-03T01:16:28.0000 State of 0 reservations recovered.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
,[2025-09-03T22:10:50.0000 sched: Allocate JobId=985 NodeList=cpu-8c-std-dy-c7i4xlarge-1 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-1-dy-c7i4xlarge-1]-dy-c7i4xlarge-1] [2025-09-03T22:10:10.0000 s,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T00:41:12.0000 AUTH: failed password for ubuntu,auth_failure,high,security,"auth,security,login_failure",23,2025-09-03T00:41:12.0000 AUTH: failed password for ubuntu] [2025-09-03T00:41:12.0000 AUTH: failed password for ubuntu]] [2025-09-03T,[2025-09-03T00:41:12.0000 AUTH: failed password for ubuntu,auth_failure
,[2025-09-03T09:23:42.0000 POWER: Power save mode: 335 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T09:23:42.0000 POWER: Power save mode: 335 nodes: [2025-09-03T09:23:42.0000 POWER: [2025-09-03T09:23:42.0000 POWER: Power save mode,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T12:45:32.0000 sched: Allocate JobId=959 NodeList=cpu-8c-std-dy-c7i4xlarge-5 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T12:45:32.0000 sched:-dy-c7i4xlarge-5]dy-c7i4xlarge-5] [2025-09-03T12:45:32.0000 sched:,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T02:06:39.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,_id=i-1985197] [2025-09-03T02:06:39.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4]]-c7i4xlarge-4],<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
,[2025-09-03T21:05:55.0000 POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",19,] [2025-09-03T21:05:05.0000 POWER: power_save: waking nodes]]-c7i4xlarge-4] [2025-09-03T21:05:05.0000 POWER: power_s,<*> POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake
,[2025-09-03T23:35:28.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T20:48:23.0000 POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete,info,infra,"power,save,complete",1,] [2025-09-03T20:48:23.0000 POWER: no more nodes resume for job] [2025-09-03T20:48:23.0000 POWER: no more nodes resume for job] POWER: no more nodes:,<*> POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete
,[2025-09-03T22:23:17.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T22:23:17.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding now now]-c7i4xlarge-5] [2025-09-03T22:,<*> Node <*> now responding,node_now_responding
,[2025-09-03T23:07:33.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-1 now responding.]-c7i4xlarge-1 now responding-c7i4-c7i45-09-03T23:07:33.0000,<*> Node <*> now responding,node_now_responding
,[2025-09-03T00:05:10.0000 sched: Allocate JobId=978 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T00:05:10.0000 sched:-dy-c7i4xlarge-2] [2025-09-03T00:05:10.0000 sched: Allocate JobId=978] [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T09:49:25.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-5 now responding now now [2025-09-03T09:49:25.0000 Node cpu-c7i4xlarge-5 now responding now-c7i,<*> Node <*> now responding,node_now_responding
,[2025-09-03T01:21:32.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T01:21:32.0000 select/cons_tres: preparing for 19 partitions..]. [2025-09-03T01:21:32.0000 select/cons_tres: preparing for 19 partitions..,<*> select/cons_tres: preparing for 19 partitions,unknown
,[2025-09-03T13:19:03.0000 POWER: Power save mode: 331 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T13:19:03.0000 POWER: Power save mode: 331 nodes [2025-09-03T13:19:03.0000 POWER: Power save mode: 331 nodes [2025-09-03T13:19:03.0000 POWER,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T22:12:36.0000 POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete,info,infra,"power,save,complete",1,] [2025-09-03T22:12:36.0000 POWER: no more nodes resume for job] [2025-09-03T22:12:36.0000 POWER: no more nodes resume for job]:36.0000 POWER: no,<*> POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete
,[2025-09-03T22:44:48.0000 DISK: /var is 97% full on node cpu-8c-std,disk_usage_high,warning,storage,"disk,usage,high",24,[2025-09-03T22:44:40.00 DISK: /var is 97% full on node cpu-8c-std]]] [2025-09-03T22:44:40.00 DISK:],[2025-09-03T22:44:48.0000 DISK: /var is 97% full on node cpu-8c-std,disk_usage_high
,[2025-09-03T12:25:16.0000 POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete,info,infra,"power,save,complete",1,] [2025-09-03T12:25:16.0000 POWER: no more nodes resume for job] [2025-09-03T12:25:16.0000 POWER: no more nodes resume for job POWER: no more nodes resume for job,<*> POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete
,[2025-09-03T12:16:33.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T08:42:11.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T08:42:11.0000 Running as primary controller.] [2025-09-03T08:42:11.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
,[2025-09-03T10:06:09.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,[2025-09-03T10:06:09.0000 State of 0 reservations recovered recovered. [2025-09-03T10:06:09.0000 State of 0 reservations recovered.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
,[2025-09-03T11:34:43.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T11:34:43.0000 AUTH: authentication failure for user admin] [2025-09-03T11:34:43.0000 AUTH: authentication failure for user admin] [2025-09-03T11:34:43.0000,<*> AUTH: authentication failure for user admin,auth_failure
,[2025-09-03T13:05:52.0000 _slurm_rpc_submit_batch_job: JobId=987 InitPrio=33692 usec=5263,job_submit_batch,info,scheduler,"job,submit,batch",25,[2025-09-03T13:05:52.0000 _slurm_rpc_submit_batch_job: [2025-09-03T13:05:52.0000 _slurm_rpc_submit_batch,[2025-09-03T13:05:52.0000 _slurm_rpc_submit_batch_job: JobId=987 InitPrio=33692 usec=5263,job_submit_batch
,[2025-09-03T23:07:39.0000 DISK: /var is 96% full on node cpu-8c-std,disk_usage_high,warning,storage,"disk,usage,high",24,[2025-09-03T23:07:39.0000 DISK: /var is 96% full on node cpu-8c-std] [2025-09-03T23:07:39.0000 DISK: /var is 96% full,<*> DISK: /var is <*> full on node cpu-8c-std,disk_usage_high
,[2025-09-03T19:51:56.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T19:51:56.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding now now]-c7i4xlarge-5] [2025-09-03T19:,<*> Node <*> now responding,node_now_responding
,"[2025-09-03T08:29:54.0000 Gres Name=gpu Type=l4 Count=7 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T08:29:54.0000 Gres] [2025-09-03T08:29:54.0000 Gres] [2025-09-03T08:29:54.0000 Gres] [2025-09-03T08:29:54,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,[2025-09-03T22:06:07.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,[2025-09-03T22:06:07.0000 NETWORK: latency to storage cluster exceeded threshold.] [2025-09-03T22:06:07.0000 NETWORK: latency to storage cluster exceeded threshold.,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T21:09:01.0000 POWER: Power save mode: 336 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T21:09:01.0000 POWER: Power save mode: 336 nodes [2025-09-03T21:09:01.0000 POWER: Power save mode: 336 nodes [2025-09-03T21:09:01.0000 POWER,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T10:43:10.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,cpu=8c-std-dy-c7i4xlarge-4] [2025-09-03T10:43:10.0000]]-c7i4xlarge-4-x2025-09-03T10:,<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
,[2025-09-03T22:03:30.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,2025-09T22:03:30.0000].]..2025-09T22:03:30.0000 Running as primary controller] [2025-09-03T22:03:30.0000 [2025-09-03T22:03:30.0000,<*> Running as primary controller,controller_primary
,[2025-09-03T07:46:53.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T07:46:53.0000.2025-09-03T07:46:53.0000 State of 0 reservations recovered recovered.0000] [2025-09-03T07:46:53.0000 State of 0 reservations recovered recovered.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
,[2025-09-03T03:14:10.0000 POWER: Power save mode: 328 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T03:14:10.0000 POWER: Power save mode: 328 nodes [2025-09-03T03:14:10.0000 POWER: Power save mode: 328 nodes [2025-09-03T03:14:10.0000 POWER,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T07:34:34.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,..].] [2025-09T07:34:34.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T07:34:34.0000 NETWORK: latency to storage cluster exceeded threshold.,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T15:40:27.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-1 now responding now now]-c7i4xlarge-1 now responding now-c7i4xlarge-1]-c7i4-5-09T15,<*> Node <*> now responding,node_now_responding
,[2025-09-03T01:31:10.0000 _slurm_rpc_submit_batch_job: JobId=971 InitPrio=35714 usec=5350,job_submit_batch,info,scheduler,"job,submit,batch",26,[2025-09-03T01:31:10.0000 _slurm_rpc_submit_batch_job: [2025-09-03T01:31:10.0000 _slurm_rpc_submit_batch_job,[2025-09-03T01:31:10.0000 _slurm_rpc_submit_batch_job: JobId=971 InitPrio=35714 usec=5350,job_submit_batch
,[2025-09-03T11:05:09.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold.. latency to storage cluster exceeded threshold..] [2025-09-03T11:05:09.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T11:05:09.,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,"[2025-09-03T09:30:15.0000 Gres Name=gpu Type=l4 Count=7 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09T09:30:15.0000] [2025-09-03T09:30:15.0000 Gres Name=gpu Type=l4 Count=74]2025-09-03T09:30:15.0000]] [2025-09-,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,[2025-09-03T07:57:15.0000 _slurm_rpc_submit_batch_job: JobId=983 InitPrio=33373 usec=5191,job_submit_batch,info,scheduler,"job,submit,batch",27,5-09-03T07:57:15.0000 [2025-09-03T07:57:15.0000 _slurm_rpc_submit_batch_job: [2025-09-03T07:57:15.0000 _s,[2025-09-03T07:57:15.0000 _slurm_rpc_submit_batch_job: JobId=983 InitPrio=33373 usec=5191,job_submit_batch
,[2025-09-03T08:24:28.0000 _slurm_rpc_submit_batch_job: JobId=1019 InitPrio=34218 usec=5937,job_submit_batch,info,scheduler,"job,submit,batch",28,5-09-03T08:24:28.0000 _slurm_rpc_submit_batch_job: [2025-09-03T08:24:28.0000 _slurm_rpc_submit_batch_,[2025-09-03T08:24:28.0000 _slurm_rpc_submit_batch_job: JobId=1019 InitPrio=34218 usec=5937,job_submit_batch
,[2025-09-03T21:51:34.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T21:51:34.0000 select/cons_tres: preparing for 19 partitions..]] [2025-09-03T21:51:34.0000 select/cons_tres: preparing for 19 partitions.,<*> select/cons_tres: preparing for 19 partitions,unknown
,[2025-09-03T18:47:37.0000 Node cpu-8c-std-dy-c7i4xlarge-4 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-4 now responding now now [2025-09-03T18:47:37.0000 Node cpu-c7i4xlarge-4 now responding now-c7i,<*> Node <*> now responding,node_now_responding
,[2025-09-03T09:48:04.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,2025-09-03T09:48:04.0000 select/cons_tres: preparing for 19 partitions.s...s..s.2025-09-03T09:48:04.00005-09-03T09:,<*> select/cons_tres: preparing for 19 partitions,unknown
,[2025-09-03T03:36:01.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-5 now responding.]-c7i4xlarge-5] [2025-09T03:36:01.0000 Node cpu-8c-std-dy-,<*> Node <*> now responding,node_now_responding
,"[2025-09-03T05:37:09.0000 Gres Name=gpu Type=l4 Count=8 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T05:37:09.0000 Gres] [2025-09-03T05:37:09.0000 Gres] [2025-09-03T05:37:09.0000 Gres] [2025-09-03T05:37:09.0000 Gres,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,"[2025-09-03T21:55:32.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T21:55:32.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T21:55:32.0000] [","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
,[2025-09-03T16:45:36.0000 POWER: Power save mode: 339 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T16:45:36.0000 POWER: Power save mode: 339 nodes. [2025-09-03T16:45:36.0000 POWER: Power save mode: 339 nodes.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T21:06:33.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T21:06:33.0000 AUTH: authentication failure for user admin] [2025-09-03T21:06:33.0000 AUTH: authentication failure for user admin] [2025-09-03T21:06:33.0000,<*> AUTH: authentication failure for user admin,auth_failure
,"[2025-09-03T00:22:55.0000 Gres Name=gpu Type=l4 Count=4 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09T00:22:55.0000] [2025-09-03T00:22:55.0000 Gres4] [2025-09-03T00:22:55.0000 Gres55]0] [2025-09-03T00:,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,[2025-09-03T11:20:48.0000 _slurm_rpc_submit_batch_job: JobId=1003 InitPrio=32729 usec=6885,job_submit_batch,info,scheduler,"job,submit,batch",29,[2025-09-03T11:20:48.48.0000 _slurm_rpc_submit_batch_job: [2025-09-03T11:20:48.48.0000 _slurm_rpc_sub,[2025-09-03T11:20:48.0000 _slurm_rpc_submit_batch_job: JobId=1003 InitPrio=32729 usec=6885,job_submit_batch
,[2025-09-03T10:38:31.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T10:38:31.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding. [2025-09-03T10:38:31.0000 Node cpu-8c-st,<*> Node <*> now responding,node_now_responding
,[2025-09-03T09:13:08.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold.2025-09-03T09:13:08.0000.. latency to storage cluster exceeded threshold..2025-09-03T09:13:08.0000]5-09-03T09:13:08.,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,"[2025-09-03T10:31:39.0000 Gres Name=gpu Type=l4 Count=5 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T10:31:3.0000 Gres] [2025-09-03T10:31:3.0000 Gres] [2025-09-03T10:31:3.0000 Gres] [2025-09-03T10:31:03.0000 Gres,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,[2025-09-03T13:00:54.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,cpu=8c-std-dy-c7i4xlarge-4] [2025-09-03T13:00:54.0000]-c7i4xlarge-4---5-09-03T13:,<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
,[2025-09-03T00:32:42.0000 POWER: Power save mode: 338 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T00:32:42.0000 POWER: Power save mode: 338 nodes. [2025-09-03T00:32:42.0000 POWER: Power save mode: 338 nodes.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T14:49:00.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,cpu=8c-std-dy-c7i4xlarge-4] [2025-09T14:49:00.0000]-c7i4xlarge-4]---5-09-03T14:49,<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
,[2025-09-03T06:07:47.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold..] [2025-09-03T06:07:47.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T06:07:47.0000 NETWORK: latency to,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T15:01:58.0000 sched: Allocate JobId=961 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T15:01:58.0000 sched:-dy-c7i4xlarge-3]d-dy-c7i4xlarge-3] [2025-09-03T15:01:58.0000 sched,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T22:35:25.0000 POWER: Power save mode: 331 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T22:35:25.0000 POWER: Power save mode: 331 nodes: [2025-09-03T22:35:25.0000 POWER: [2025-09-03T22:35:25.0000 POWER: Power save mode,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T05:42:34.0000 sched: Allocate JobId=953 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T05:42:34.0000 sched:-dy-c7i4xlarge-2]d-dy-c7i4xlarge-2] [2025-09-03T05:42:34.0000 sched,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T05:33:32.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T05:33:32.0000 Running as primary controller.] [2025-09-03T05:33:32.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
,[2025-09-03T01:27:01.0000 AUTH: user root failed password authentication,auth_failure,high,security,"auth,security,login_failure",30,[2025-09-03T01:27:01.0000 AUTH: user root failed password authentication.] [2025-09-03T01:27:01.0000 AUTH: user root failed password authentication.,[2025-09-03T01:27:01.0000 AUTH: user root failed password authentication,auth_failure
,[2025-09-03T18:34:02.0000 _slurm_rpc_submit_batch_job: JobId=953 InitPrio=30192 usec=5880,job_submit_batch,info,scheduler,"job,submit,batch",31,[2025-09-03T18:34:02.0000 _slurm_rpc_submit_batch_job: JobId=953 InitPrio=30192 usec=5880 [2025-09-03T18:34:02.0000,[2025-09-03T18:34:02.0000 _slurm_rpc_submit_batch_job: JobId=953 InitPrio=30192 usec=5880,job_submit_batch
,[2025-09-03T01:31:37.0000 DISK: /var is 93% full on node cpu-8c-std,disk_usage_high,warning,storage,"disk,usage,high",24,] [2025-09-03T01:31:37.0000 DISK: /var is 93% full on node cpu-8c-std]]] [2025-09-03T01:31:37.0000 DISK:,<*> DISK: /var is <*> full on node cpu-8c-std,disk_usage_high
,[2025-09-03T20:43:55.0000 POWER: Power save mode: 330 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T20:43:55.0000 POWER: Power save mode: 330 nodes [2025-09-03T20:43:55.0000 POWER: Power save mode: 330 nodes [2025-09-03T20:43:55.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,"[2025-09-03T03:06:32.0000 Gres Name=gpu Type=l4 Count=7 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09T03:06:32.0000 Gres] [2025-09-03T03:06:32.0000 Gres] [2025-09-03T03:06:32.0000 Gres] [2025-09-03T03:06:32.0000,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,"[2025-09-03T22:07:01.0000 Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T22:07:01.0000 Gres] [2025-09-03T22:07:01.0000 Gres] [2025-09-03T22:07:01.0000 Gres] [2025-09-03T22:07:01.0000 Gres,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,[2025-09-03T22:54:07.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold.. latency to storage cluster exceeded threshold...] [2025-09-03T22:54:07.0000 NETWORK: latency to storage cluster exceeded threshold..2025-09-03T22:54,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T01:29:03.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,[2025-09-03T01:29:03.0000 State of 0 reservations recovered recovered. [2025-09-03T01:29:03.0000 State of 0 reservations recovered.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
,[2025-09-03T15:07:05.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T15:07:05.0000 select/cons_tres: preparing for 19 partitions..]. [2025-09-03T15:07:05.0000 select/cons_tres: preparing for 19 partitions..,<*> select/cons_tres: preparing for 19 partitions,unknown
,[2025-09-03T02:49:47.0000 _slurm_rpc_submit_batch_job: JobId=1021 InitPrio=34758 usec=6066,job_submit_batch,info,scheduler,"job,submit,batch",32,[2025-09-03T02:49:47.0000 _slurm_rpc_submit_batch_job: [2025-09-03T02:49:47.0000 _slurm_rpc_submit_batch,[2025-09-03T02:49:47.0000 _slurm_rpc_submit_batch_job: JobId=1021 InitPrio=34758 usec=6066,job_submit_batch
,[2025-09-03T19:56:11.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T19:56:11.0000 Running as primary controller. [2025-09-03T19:56:11.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
,[2025-09-03T16:53:46.0000 _slurm_rpc_submit_batch_job: JobId=978 InitPrio=32923 usec=5007,job_submit_batch,info,scheduler,"job,submit,batch",33,[2025-09-03T16:53:46.0000 _slurm_rpc_submit_batch_job: [2025-09-03T16:53:46.0000 _slurm_rpc_submit_batch,[2025-09-03T16:53:46.0000 _slurm_rpc_submit_batch_job: JobId=978 InitPrio=32923 usec=5007,job_submit_batch
,[2025-09-03T07:01:14.0000 sched: Allocate JobId=1000 NodeList=cpu-8c-std-dy-c7i4xlarge-4 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T07:01:14.0000 sched:-dy-c7i4xlarge-4]d-dy-c7i4xlarge-4] [2025-09-03T07:01:14.0000 sched,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T08:44:29.0000 DISK: /var is 97% full on node cpu-8c-std,disk_usage_high,warning,storage,"disk,usage,high",24,] [2025-09-03T08:44:29.0000 DISK:] [2025-09-03T08:44:29.0000 DISK:]] [2025-09-03T08:44:29.0000 DISK:,<*> DISK: /var is <*> full on node cpu-8c-std,disk_usage_high
,[2025-09-03T14:06:26.0000 _slurm_rpc_submit_batch_job: JobId=986 InitPrio=31967 usec=5035,job_submit_batch,info,scheduler,"job,submit,batch",34,[2025-09-03T14:06:26.0000 _slurm_rpc_submit_batch_job: [2025-09-03T14:06:26.0000 _slurm_rpc_submit_batch,[2025-09-03T14:06:26.0000 _slurm_rpc_submit_batch_job: JobId=986 InitPrio=31967 usec=5035,job_submit_batch
,[2025-09-03T11:41:58.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold..].] [2025-09-03T11:41:58.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T11:41:58.0000.0000,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T03:41:25.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T03:41:25.0000 AUTH: authentication failure for user admin] [2025-09-03T03:41:25.0000 AUTH: authentication failure for user admin] [2025-09-03T03:41:25.0000,<*> AUTH: authentication failure for user admin,auth_failure
,[2025-09-03T04:26:45.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error,high,storage,"disk,scrubber,error",14,2025-09-03T04:26:06.0000 DISK: scrubber error while cleaning temp files..]..5-09-03T04:26:0000.5-09-03T04:26:00005-09-03T,<*> DISK: scrubber error while cleaning temp files,disk_scrubber_error
,[2025-09-03T09:40:24.0000 sched: Allocate JobId=1008 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-2-dy-c7i4xlarge-2]d-dy-c7i4xlarge-2] [2025-09-03T09:40:24.0000,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T16:54:48.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T16:54:48.0000 select/cons_tres: preparing for 19 partitions..]. [2025-09-03T16:54:48.0000 select/cons_tres: preparing for 19 partitions..,<*> select/cons_tres: preparing for 19 partitions,unknown
,[2025-09-03T05:07:59.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T05:07:59.0000 Running as primary controller.] [2025-09-03T05:07:59.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
,[2025-09-03T10:27:52.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T10:27:52.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding. [2025-09-03T10:27:52.0000 Node cpu-8c-st,<*> Node <*> now responding,node_now_responding
,[2025-09-03T21:59:39.0000 sched: Allocate JobId=991 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-3-dy-c7i4xlarge-3]dy-c7i4xlarge-3] [2025-09-03T21:59:59.0000 sched,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T13:58:22.0000 DISK: /var is 86% full on node cpu-8c-std,disk_usage_high,warning,storage,"disk,usage,high",24,] [2025-09-03T13:58:22.0000 DISK:]]] [2025-09-03T13:58:22.0000 DISK: /var is 86% full on node cpu-8c-std,<*> DISK: /var is <*> full on node cpu-8c-std,disk_usage_high
,[2025-09-03T22:59:27.0000 POWER: Power save mode: 326 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T22:59:27.0000 POWER: Power save mode: 326 nodes: [2025-09-03T22:59:27.0000 POWER: [2025-09-03T22:59:27.0000 POWER: Power save mode,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T13:58:39.0000 POWER: Power save mode: 322 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T13:58:39.0000 POWER: Power save mode: 322 nodes. [2025-09-03T13:58:39.0000 POWER: Power save mode: 322 nodes.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T22:51:10.0000 sched: Allocate JobId=967 NodeList=cpu-8c-std-dy-c7i4xlarge-4 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T22:51:10.0000 sched: Allocate JobId=967-dy-c7i4xlarge-4]dy-c7i4xlarge-4] [2025-09-03T22:51:,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T09:45:43.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T09:45:43.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding now now]-c7i4xlarge-3]-c7i4x-,<*> Node <*> now responding,node_now_responding
,[2025-09-03T02:29:10.0000 POWER: Power save mode: 333 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T02:29:10.0000 POWER: Power save mode: 333 nodes [2025-09-03T02:29:10.0000 POWER: Power save mode: 333 nodes [2025-09-03T02:29:10.0000 POWER,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T06:35:42.0000 Node cpu-8c-std-dy-c7i4xlarge-2 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T06:35:42.0000 Node cpu-8c-std-dy-c7i4xlarge-2 now responding now now]-c7i4xlarge-2 now responding-c7i4-,<*> Node <*> now responding,node_now_responding
,"[2025-09-03T06:29:02.0000 Gres Name=gpu Type=l4 Count=3 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T06:29:02.0000 Gres] [2025-09-03T06:29:02.0000 Gres] [2025-09-03T06:29:02.0000 Gres] [2025-09-03T06:29:02.0000 Gres,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,[2025-09-03T00:52:43.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T00:52:43.0000 AUTH: authentication failure for user admin] [2025-09-03T00:52:43.0000 AUTH: authentication failure for user admin] [2025-09-03T00:52:43.0000,<*> AUTH: authentication failure for user admin,auth_failure
,[2025-09-03T08:45:45.0000 sched: Allocate JobId=956 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-2-dy-c7i4xlarge-2]d-dy-c7i4xlarge-2] [2025-09-03T08:45:45.0000,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T13:22:55.0000 Node cpu-8c-std-dy-c7i4xlarge-2 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-2 now responding.]-c7i4xlarge-2 now responding-c7i4xlarge-2]-c7i4-5-09T13:22,<*> Node <*> now responding,node_now_responding
,[2025-09-03T12:55:01.0000 POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete,info,infra,"power,save,complete",1,] [2025-09-03T12:55:01.0000 POWER: no more nodes resume for job] [2025-09-03T12:55:01.0000 POWER: no more nodes resume for job] POWER: no more nodes resume for job,<*> POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete
,[2025-09-03T18:06:37.0000 _slurm_rpc_submit_batch_job: JobId=1007 InitPrio=30631 usec=5078,job_submit_batch,info,scheduler,"job,submit,batch",35,[2025-09-03T18:06:37.0000 _slurm_rpc_submit_batch_job: [2025-09-03T18:06:37.0000 _slurm_rpc_submit_batch,[2025-09-03T18:06:37.0000 _slurm_rpc_submit_batch_job: JobId=1007 InitPrio=30631 usec=5078,job_submit_batch
,[2025-09-03T22:42:46.0000 _slurm_rpc_submit_batch_job: JobId=998 InitPrio=32442 usec=6064,job_submit_batch,info,scheduler,"job,submit,batch",36,5-09-03T22:42:46.0000 [2025-09-03T22:42:46.0000 _slurm_rpc_submit_batch_job: [2025-09-03T22:42:46.0000 _s,[2025-09-03T22:42:46.0000 _slurm_rpc_submit_batch_job: JobId=998 InitPrio=32442 usec=6064,job_submit_batch
,[2025-09-03T05:08:23.0000 Node cpu-8c-std-dy-c7i4xlarge-4 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-4 now responding now now]-c7i4xlarge-4] [2025-09-03T05:08:23.0000 Node cpu-c7i4,<*> Node <*> now responding,node_now_responding
,[2025-09-03T23:04:55.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",8,] [2025-09-03T23:04:55.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding.]-c7i4xlarge-1 now responding-c7i4xlarge,<*> Node <*> now responding,node_now_responding
,[2025-09-03T03:39:53.0000 POWER: Power save mode: 320 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T03:39:53.0000 POWER: Power save mode: 320 nodes. [2025-09-03T03:39:53.0000 POWER: Power save mode: 320 nodes.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T16:31:49.0000 POWER: Power save mode: 329 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T16:31:49.0000 POWER: Power save mode: 329 nodes: [2025-09-03T16:31:49.0000 POWER: [2025-09-03T16:31:49.0000 POWER: Power save mode,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T03:19:06.0000 sched: Allocate JobId=1012 NodeList=cpu-8c-std-dy-c7i4xlarge-1 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T03:19:06.0000 sched: Allocate JobId=1012-dy-c7i4xlarge-1]-dy-c7i4xlarge-1] [2025-09-03T03:19,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T17:48:49.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold...].] [2025-09-03T17:48:49.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T17:48:49.0000.,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T03:50:43.0000 _slurm_rpc_submit_batch_job: JobId=988 InitPrio=30962 usec=5641,job_submit_batch,info,scheduler,"job,submit,batch",37,[2025-09-03T03:50:43.0000 _slurm_rpc_submit_batch_job: JobId=988 InitPrio=30962 usec=5641 _slurm_rpc_,[2025-09-03T03:50:43.0000 _slurm_rpc_submit_batch_job: JobId=988 InitPrio=30962 usec=5641,job_submit_batch
,[2025-09-03T07:32:44.0000 _slurm_rpc_submit_batch_job: JobId=952 InitPrio=30423 usec=5707,job_submit_batch,info,scheduler,"job,submit,batch",38,5-09-03T07:32:44.0000 [2025-09-03T07:32:44.0000 _slurm_rpc_submit_batch_job: [2025-09-03T07:32:44.0000 _s,[2025-09-03T07:32:44.0000 _slurm_rpc_submit_batch_job: JobId=952 InitPrio=30423 usec=5707,job_submit_batch
,[2025-09-03T14:06:49.0000 sched: Allocate JobId=1001 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-2-dy-c7i4xlarge-2]dy-c7i4xlarge-2] [2025-09-03T14:06:06.0000 sched:,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,"[2025-09-03T18:30:16.0000 Gres Name=gpu Type=l4 Count=6 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T18:30:16.0000] [2025-09-03T18:30:16.0000 Gres4] [2025-09-03T18:30:16.0000 Gres]] [2025-09-03T18:,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,[2025-09-03T15:14:40.0000 _slurm_rpc_submit_batch_job: JobId=1017 InitPrio=35133 usec=5559,job_submit_batch,info,scheduler,"job,submit,batch",39,5-09-03T15:14:40.40.0000 _slurm_rpc_submit_batch_job: [2025-09-03T15:14:40.40.0000 _slurm_rpc_submit,[2025-09-03T15:14:40.0000 _slurm_rpc_submit_batch_job: JobId=1017 InitPrio=35133 usec=5559,job_submit_batch
,[2025-09-03T23:37:29.0000 Node cpu-8c-std-dy-c7i4xlarge-2 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T23:37:29.0000 Node cpu-8c-std-dy-c7i4xlarge-2 now responding now now]-c7i4xlarge-2 now responding now-c7i4x,<*> Node <*> now responding,node_now_responding
,[2025-09-03T07:07:34.0000 DISK: /var is 98% full on node cpu-8c-std,disk_usage_high,warning,storage,"disk,usage,high",24,] [2025-09T07:07:34.0000 DISK: /var is 98% full on node cpu-8c-std]]] [2025-09T07:07:34.0000 DISK:],<*> DISK: /var is <*> full on node cpu-8c-std,disk_usage_high
,[2025-09-03T05:11:49.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error,high,storage,"disk,scrubber,error",14,[2025-09-03T05:11:11.0000 DISK: scrubber error while cleaning temp files.] [2025-09-03T05:11:11.0000 DISK: scrubber error while cleaning temp files.,<*> DISK: scrubber error while cleaning temp files,disk_scrubber_error
,[2025-09-03T16:53:03.0000 _slurm_rpc_submit_batch_job: JobId=968 InitPrio=35750 usec=5093,job_submit_batch,info,scheduler,"job,submit,batch",40,[2025-09-03T16:53:03.0000 _slurm_rpc_submit_batch_job: [2025-09-03T16:53:03.0000 _slurm_rpc_submit_batch_job,[2025-09-03T16:53:03.0000 _slurm_rpc_submit_batch_job: JobId=968 InitPrio=35750 usec=5093,job_submit_batch
,[2025-09-03T04:53:28.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T04:53:28.0000 Running as primary controller.] [2025-09-03T04:53:28.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
,[2025-09-03T12:14:49.0000 AUTH: user root failed password authentication,auth_failure,high,security,"auth,security,login_failure",30,[2025-09-03T12:14:49.0000 AUTH: user root failed password authentication.,<*> AUTH: user root failed password authentication,auth_failure
,[2025-09-03T08:08:42.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T14:39:32.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,cpu=8c-std-dy-c7i4xlarge-4] [2025-09-03T14:39:32.0000]]--.00005-09-03T14:39:32.0000,<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
,[2025-09-03T07:55:21.0000 Node cpu-8c-std-dy-c7i4xlarge-2 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu 8c-std-dy-c7i4xlarge-2 now responding.]-c7i4xlarge-2 now responding-c7i4-c7i4x5-09-03T07:55:21.,<*> Node <*> now responding,node_now_responding
,"[2025-09-03T12:08:31.0000 Gres Name=gpu Type=l4 Count=3 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T12:08:31.0000 Gres] [2025-09-03T12:08:31.0000 Gres] [2025-09-03T12:08:31.0000 Gres] [2025-09-03T12:08:31,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,[2025-09-03T03:10:23.0000 POWER: Power save mode: 339 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T03:10:23.0000 POWER: Power save mode: 339 nodes] [2025-09-03T03:10:23.0000 POWER: Power save mode: 339 nodes] [2025-09-03T03:10:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T01:28:39.0000 _slurm_rpc_submit_batch_job: JobId=1018 InitPrio=34433 usec=5441,job_submit_batch,info,scheduler,"job,submit,batch",41,[2025-09-03T01:28:39.0000 _slurm_rpc_submit_batch_job: [2025-09-03T01:28:39.0000 _slurm_rpc_submit_batch,[2025-09-03T01:28:39.0000 _slurm_rpc_submit_batch_job: JobId=1018 InitPrio=34433 usec=5441,job_submit_batch
,[2025-09-03T01:12:49.0000 sched: Allocate JobId=1010 NodeList=cpu-8c-std-dy-c7i4xlarge-1 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-1] [2025-09-03T01:12:49.0000 sched:-dy-c7i4xlarge-1]] [2025-09-03T01,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T20:18:44.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T20:18:44.0000 Running as primary controller.] [2025-09-03T20:18:44.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
,[2025-09-03T14:03:34.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-3 now responding.]-c7i4xlarge-3] [2025-09T14:03:34.0000 Node cpu-8c-std-dy,<*> Node <*> now responding,node_now_responding
,[2025-09-03T00:35:17.0000 POWER: Power save mode: 333 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T00:35:17.0000 POWER: Power save mode: 333 nodes] [2025-09-03T00:35:17.0000 POWER: Power save mode: 333 nodes] [2025-09-03T00:35:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T09:39:35.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T09:39:35.0000.2025-09-03T09:39:35.0000 State of 0 reservations recovered recovered.0] [2025-09-03T09:39:35.0000 State of 0 reservations recovered recovered.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
,[2025-09-03T13:07:06.0000 Node cpu-8c-std-dy-c7i4xlarge-2 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-2 now responding.]-c7i4xlarge-2] [2025-09-03T13:07:06.0000 Node cpu-8c-std-d,<*> Node <*> now responding,node_now_responding
,[2025-09-03T11:43:27.0000 POWER: Power save mode: 333 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T11:43:27.0000 POWER: Power save mode: 333 nodes [2025-09-03T11:43:27.0000 POWER: Power save mode: 333 nodes [2025-09-03T11:43:27.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T13:39:36.0000 Node cpu-8c-std-dy-c7i4xlarge-2 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-2 now responding.]-c7i4xlarge-2 now responding.] [2025-09-03T13:39:36.0000-c7i4xlarge-2,<*> Node <*> now responding,node_now_responding
,[2025-09-03T13:57:49.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T13:57:49.0000 State of 0 reservations recovered recovered recovered recovered2025-09-03T13:57:49.0000 State of0000 State of 0 reservations recovered recovered0:57:49.00005-09-03,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
,[2025-09-03T15:09:13.0000 DISK: /var is 97% full on node cpu-8c-std,disk_usage_high,warning,storage,"disk,usage,high",24,] [2025-09-03T15:09:13.0000 DISK: /var is 97% full on node cpu-8c-std]]]].0000] [2025-09-03T15:09:13,<*> DISK: /var is <*> full on node cpu-8c-std,disk_usage_high
,[2025-09-03T21:59:53.0000 AUTH: user root failed password authentication,auth_failure,high,security,"auth,security,login_failure",30,[2025-09-03T21:59:53.0000 AUTH: user root failed password authentication.] [2025-09-03T21:59:53.0000 AUTH: user root failed password authentication.,<*> AUTH: user root failed password authentication,auth_failure
,[2025-09-03T23:35:38.0000 POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",19,] [2025-09-03T23:35:38.0000 POWER: power_save: waking nodes]] [2025-09-03T23:35:38.0000 POWER: power_save: waking nodes],<*> POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake
,[2025-09-03T03:55:19.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T03:55:19.0000 AUTH: authentication failure for user admin] [2025-09-03T03:55:19.0000 AUTH: authentication failure for user admin] [2025-09-03T03:55:19.0000,<*> AUTH: authentication failure for user admin,auth_failure
,[2025-09-03T00:41:18.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-3 now responding now now]-c7i4xlarge-3] [2025-09-03T00:41:18.0000 Node cpu-c7i4,<*> Node <*> now responding,node_now_responding
,[2025-09-03T14:28:20.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T14:28:20.0000 select/cons_tres: preparing for 19 partitions..]. [2025-09-03T14:28:20.0000 select/cons_tres: preparing for 19 partitions..,<*> select/cons_tres: preparing for 19 partitions,unknown
,[2025-09-03T04:02:49.0000 POWER: Power save mode: 339 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T04:02:49.0000 POWER: Power save mode: 339 nodes: [2025-09-03T04:02:49.0000 POWER: [2025-09-03T04:02:49.0000 POWER: Power save mode,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,"[2025-09-03T18:30:20.0000 Gres Name=gpu Type=l4 Count=5 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T18:30:30.0000 Gres] [2025-09-03T18:30:30.0000 Gres] [2025-09-03T18:30:30.0000 Gres] [2025-09-03T18:30:30,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,[2025-09-03T22:02:04.0000 _slurm_rpc_submit_batch_job: JobId=1015 InitPrio=32210 usec=5378,job_submit_batch,info,scheduler,"job,submit,batch",42,[2025-09-03T22:02:04.0000 _slurm_rpc_submit_batch_job: [2025-09-03T22:02:04.0000 _slurm_rpc_submit_batch_job,[2025-09-03T22:02:04.0000 _slurm_rpc_submit_batch_job: JobId=1015 InitPrio=32210 usec=5378,job_submit_batch
,"[2025-09-03T21:26:42.0000 Gres Name=gpu Type=l4 Count=3 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T21:26:42.0000 Gres] [2025-09-03T21:26:42.0000 Gres] [2025-09-03T21:26:42.0000 Gres] [2025-09-03T21:26:42,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,"[2025-09-03T14:02:57.0000 Gres Name=gpu Type=l4 Count=8 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T14:02:57.0000 Gres] [2025-09-03T14:02:57.0000 Gres] [2025-09-03T14:02:57.0000 Gres] [2025-09-03T14:02:57,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,[2025-09-03T17:24:32.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",8,] [2025-09-03T17:24:32.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding..] [2025-09-03T17:24:32.0000 Node-,<*> Node <*> now responding,node_now_responding
,"[2025-09-03T02:48:54.0000 Gres Name=gpu Type=l4 Count=7 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T02:48:54.0000] [2025-09-03T02:48:54.0000 Gres Name=gpu Type=l4 Count=74]]] [2025-09-03T02:48:54.,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,"[2025-09-03T21:27:08.0000 Gres Name=gpu Type=l4 Count=8 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T21:27:08.0000 Gres] [2025-09-03T21:27:08.0000 Gres] [2025-09-03T21:27:08.0000 Gres] [2025-09-03T21:27:08.0000 Gres,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,[2025-09-03T06:54:17.0000 POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",19,] [2025-09-03T06:54:17.0000 POWER: power_save: waking nodes]] [2025-09-03T06:54:17.0000 POWER: power_save: waking nodes],<*> POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake
,[2025-09-03T10:09:59.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09-03T10:09:59.0000 select/cons_tres: preparing for 19 partitions. [2025-09-03T10:09:59.0000 select/cons_tres: preparing for 19 partitions.,<*> select/cons_tres: preparing for 19 partitions,unknown
,[2025-09-03T13:13:02.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T13:13:02.0000 Running as primary controller.] [2025-09-03T13:13:02.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
,"[2025-09-03T12:51:29.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T12:51:29.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T12:51:29.0000] [","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
,[2025-09-03T15:47:22.0000 AUTH: user root failed password authentication,auth_failure,high,security,"auth,security,login_failure",30,[2025-09-03T15:47:22.0000 AUTH: user root failed password authentication.] [2025-09-03T15:47:22.0000 AUTH: user root failed password authentication.,<*> AUTH: user root failed password authentication,auth_failure
,"[2025-09-03T06:56:56.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T06:56:56.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T06:56:56.0000 [202","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
,[2025-09-03T22:18:31.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T22:18:31.0000 AUTH: authentication failure for user admin] [2025-09-03T22:18:31.0000 AUTH: authentication failure for user admin] [2025-09-03T22:18:31.0000,<*> AUTH: authentication failure for user admin,auth_failure
,[2025-09-03T10:21:01.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,cpu=8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197..]]-c7i4xlarge-4-x2025-09-03T10:,<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
,[2025-09-03T16:50:42.0000 POWER: Power save mode: 322 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T16:50:42.0000 POWER: Power save mode: 322 nodes. [2025-09-03T16:50:42.0000 POWER: Power save mode: 322 nodes.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,"[2025-09-03T12:55:55.0000 Gres Name=gpu Type=l4 Count=7 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T12:55:55.0000] [2025-09-03T12:55:55.0000 Gres Name=gpu Type=l4 Count=7 [2025-09-03T12:55:55.0000 Gres Name=gpu,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,[2025-09-03T15:29:00.0000 _slurm_rpc_submit_batch_job: JobId=1016 InitPrio=35895 usec=6740,job_submit_batch,info,scheduler,"job,submit,batch",43,5-09-03T15:29:00.0000 _slurm_rpc_submit_batch_job: [2025-09-03T15:29:00.0000 _slurm_rpc_submit_batch_,[2025-09-03T15:29:00.0000 _slurm_rpc_submit_batch_job: JobId=1016 InitPrio=35895 usec=6740,job_submit_batch
,[2025-09-03T20:06:46.0000 _slurm_rpc_submit_batch_job: JobId=958 InitPrio=31222 usec=6831,job_submit_batch,info,scheduler,"job,submit,batch",44,-] [2025-09-03T20:06:46.0000 _slurm_rpc_submit_batch_job:] [2025-09-03T20:06:46.0000 [2025-09-03T20:06,[2025-09-03T20:06:46.0000 _slurm_rpc_submit_batch_job: JobId=958 InitPrio=31222 usec=6831,job_submit_batch
,[2025-09-03T21:27:46.0000 POWER: Power save mode: 333 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T21:27:46.0000 POWER: Power save mode: 333 nodes [2025-09-03T21:27:46.0000 POWER: Power save mode: 333 nodes [2025-09-03T21:27:46.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T11:10:04.0000 sched: Allocate JobId=963 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T11:10:04.0000 sched: Allocate JobId=963-dy-c7i4xlarge-3]-dy-c7i4xlarge-3] [2025-09-03T11:10,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T13:02:23.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T13:02:23.0000 select/cons_tres: preparing for 19 partitions..]. [2025-09-03T13:02:23.0000 select/cons_tres: preparing for 19 partitions..,<*> select/cons_tres: preparing for 19 partitions,unknown
,[2025-09-03T01:07:41.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error,high,storage,"disk,scrubber,error",14,] [2025-09T01:07:41.0000 DISK: scrubber error while cleaning temp files..]..]. [2025-09T01:07:41.0000 DISK: scrubber error while cleaning temp files.,<*> DISK: scrubber error while cleaning temp files,disk_scrubber_error
,[2025-09-03T16:33:06.0000 POWER: Power save mode: 330 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T16:33:06.0000 POWER: Power save mode: 330 nodes [2025-09-03T16:33:06.0000 POWER: Power save mode: 330 nodes [2025-09-03T16:33:06.0000 POWER,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T11:42:35.0000 DISK: /var is 86% full on node cpu-8c-std,disk_usage_high,warning,storage,"disk,usage,high",24,] [2025-09-03T11:42:35.0000 DISK:]]] [2025-09-03T11:42:35.0000 DISK:: /var is 86% full on node cpu-8c,<*> DISK: /var is <*> full on node cpu-8c-std,disk_usage_high
,[2025-09-03T17:17:47.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,cpu=8c-std-dy-c7i4xlarge-4] [2025-09T17:17:47.0000]-c7i4xlarge-4---5-09T17:17:47.,<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
,[2025-09-03T02:04:33.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T15:06:12.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding,info,infra,"node,recovered,online",8,] [2025-09-03T15:06:12.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding.] [2025-09-03T15:06:12.0000-c7i4x,<*> Node <*> now responding,node_now_responding
,[2025-09-03T00:41:41.0000 _slurm_rpc_submit_batch_job: JobId=979 InitPrio=31554 usec=5855,job_submit_batch,info,scheduler,"job,submit,batch",45,[2025-09-03T00:41:41.0000 _slurm_rpc_submit_batch_job: JobId=979 InitPrio=31554 usec=5855 [2025-09-03T00:41:,[2025-09-03T00:41:41.0000 _slurm_rpc_submit_batch_job: JobId=979 InitPrio=31554 usec=5855,job_submit_batch
,[2025-09-03T00:22:34.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error,high,storage,"disk,scrubber,error",14,[2025-09-03T00:22:34.0000 DISK: scrubber error while cleaning temp files. [2025-09-03T00:22:34.0000 DISK: scrubber error while cleaning temp files.,<*> DISK: scrubber error while cleaning temp files,disk_scrubber_error
,[2025-09-03T15:02:42.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T15:02:42.0000 select/cons_tres: preparing for 19 partitions..]. [2025-09-03T15:02:42.0000 select/cons_tres: preparing for 19 partitions..,<*> select/cons_tres: preparing for 19 partitions,unknown
,[2025-09-03T00:58:51.0000 POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",19,] [2025-09-03T00:58:58.0000 POWER: power_save: waking nodes]]-c7i4xlarge-4] [2025-09-03T00:58:58.0000 POWER: power_s,<*> POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake
,[2025-09-03T21:04:51.0000 _slurm_rpc_submit_batch_job: JobId=999 InitPrio=35504 usec=6817,job_submit_batch,info,scheduler,"job,submit,batch",46,-] [2025-09-03T21:04:51.0000 _slurm_rpc_submit_batch_job:] [2025-09-03T21:04:51.0000 [2025-09-03T21:04,[2025-09-03T21:04:51.0000 _slurm_rpc_submit_batch_job: JobId=999 InitPrio=35504 usec=6817,job_submit_batch
,[2025-09-03T15:08:44.0000 sched: Allocate JobId=1018 NodeList=cpu-8c-std-dy-c7i4xlarge-5 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T15:08:44.0000 sched:-dy-c7i4xlarge-5]d-dy-c7i4xlarge-5] [2025-09-03T15:08:44.0000 sched,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T18:49:25.0000 Node cpu-8c-std-dy-c7i4xlarge-2 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-2 now responding now now]-c7i4xlarge-2 now responding now-c7i4xlarge-2-c7i4x-5-09T18,<*> Node <*> now responding,node_now_responding
,[2025-09-03T10:25:26.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T10:25:26.0000.2025-09-03T10:25:26.0000 State of 0 reservations recovered recovered.0000.-03T10:25:26.00005-09-03T10:25:,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
,[2025-09-03T11:31:43.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error,high,storage,"disk,scrubber,error",14,] [2025-09-03T11:31:43.0000 DISK: scrubber error while cleaning temp files..]] [2025-09-03T11:31:43.0000 DISK: scrubber error while cleaning temp files..,<*> DISK: scrubber error while cleaning temp files,disk_scrubber_error
,[2025-09-03T02:36:48.0000 _slurm_rpc_submit_batch_job: JobId=959 InitPrio=31071 usec=5827,job_submit_batch,info,scheduler,"job,submit,batch",47,[2025-09-03T02:36:48.0000 _slurm_rpc_submit_batch_job: [2025-09-03T02:36:48.0000 _slurm_rpc_submit_batch,[2025-09-03T02:36:48.0000 _slurm_rpc_submit_batch_job: JobId=959 InitPrio=31071 usec=5827,job_submit_batch
,[2025-09-03T01:08:52.0000 _slurm_rpc_submit_batch_job: JobId=1028 InitPrio=31783 usec=6993,job_submit_batch,info,scheduler,"job,submit,batch",48,[2025-09-03T01:08:52.0000 _slurm_rpc_submit_batch_job: [2025-09-03T01:08:52.0000 _slurm_rpc_submit_batch,[2025-09-03T01:08:52.0000 _slurm_rpc_submit_batch_job: JobId=1028 InitPrio=31783 usec=6993,job_submit_batch
,[2025-09-03T17:47:31.0000 sched: Allocate JobId=988 NodeList=cpu-8c-std-dy-c7i4xlarge-1 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T17:47:31.0000 sched:-dy-c7i4xlarge-1]d-dy-c7i4xlarge-1] [2025-09-03T17:47:31.0000 sched,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T10:14:24.0000 _slurm_rpc_submit_batch_job: JobId=1022 InitPrio=33714 usec=5207,job_submit_batch,info,scheduler,"job,submit,batch",49,[2025-09-03T10:14:24.0000 _slurm_rpc_submit_batch_job: [2025-09-03T10:14:24.0000 _slurm_rpc_submit_batch,[2025-09-03T10:14:24.0000 _slurm_rpc_submit_batch_job: JobId=1022 InitPrio=33714 usec=5207,job_submit_batch
,[2025-09-03T19:07:55.0000 POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",19,] [2025-09-03T19:07:07.0000 POWER: power_save: waking nodes]] [2025-09-03T19:07:07.0000 POWER:-c7i4xlarge-4] [2025-09-,<*> POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake
,[2025-09-03T05:00:32.0000 POWER: Power save mode: 339 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T05:00:32.0000 POWER: Power save mode: 339 nodes. [2025-09-03T05:00:32.0000 POWER: Power save mode: 339 nodes.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T18:02:19.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09-03T18:02:19.0000 select/cons_tres: preparing for 19 partitions..]] [2025-09-03T18:02:19.0000 select/cons_tres: preparing for 19 partitions,<*> select/cons_tres: preparing for 19 partitions,unknown
,[2025-09-03T14:42:05.0000 _slurm_rpc_submit_batch_job: JobId=984 InitPrio=35508 usec=6679,job_submit_batch,info,scheduler,"job,submit,batch",50,--] [2025-09-03T14:42:05.0000 [2025-09-03T14:42:05.0000 [2025-09-03T14:42:05.0000 _slurm_rpc_submit_b,[2025-09-03T14:42:05.0000 _slurm_rpc_submit_batch_job: JobId=984 InitPrio=35508 usec=6679,job_submit_batch
,[2025-09-03T22:51:19.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T22:51:19.0000 Running as primary controller.] [2025-09-03T22:51:19.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
,[2025-09-03T02:08:43.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T02:08:43.0000.2025-09-03T02:08:43.0000 State of0000] [2025-09-03T02:08:43.0000 State of 0 reservations recovered recovered.5-09-03T,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
,[2025-09-03T10:47:02.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold.....2025-09-03T10:47:02.0000 NETWORK: latency to storage cluster exceeded threshold..2025-09-03T10:47:02.00005-09-03T,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T23:04:11.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,[2025-09-03T23:04:11.0000 NETWORK: latency to storage cluster exceeded threshold.] [2025-09-03T23:04:11.0000 NETWORK: latency to storage cluster exceeded threshold.,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T20:34:53.0000 sched: Allocate JobId=962 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-3]-dy-c7i4xlarge-3]d-dy-c7i4xlarge-3] [2025-09-03T20:34:53.0000,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T01:47:24.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error,high,storage,"disk,scrubber,error",14,[2025-09-03T01:47:24.0000 DISK: scrubber error while cleaning temp files.] [2025-09-03T01:47:24.0000 DISK: scrubber error while cleaning temp files.,<*> DISK: scrubber error while cleaning temp files,disk_scrubber_error
,[2025-09-03T03:18:22.0000 sched: Allocate JobId=990 NodeList=cpu-8c-std-dy-c7i4xlarge-1 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-1-dy-c7i4xlarge-1]-dy-c7i4xlarge-1] [2025-09-03T03:18:22.0000 s,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T08:12:32.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold..].. latency to storage cluster exceeded threshold..2025-09-03T08:12:32.0000] [2025-09-03T08:12:32.00005-09-03T,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T19:25:09.0000 AUTH: failed password for ubuntu,auth_failure,high,security,"auth,security,login_failure",23,2025-09-03T19:25:09.0000 AUTH: failed password for ubuntu] [2025-09-03T19:25:09.0000 AUTH: failed password for ubuntu]] [2025-09-03T,<*> AUTH: failed password for ubuntu,auth_failure
,[2025-09-03T12:25:45.0000 POWER: Power save mode: 322 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T12:25:45.0000 POWER: Power save mode: 322 nodes [2025-09-03T12:25:45.0000 POWER: Power save mode: 322 nodes [2025-09-03T12:25:45.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T01:38:04.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,to storage cluster exceeded threshold.. latency to storage cluster exceeded threshold... latency to storage cluster exceeded threshold.2025-09T01:38:04.0000 NETWORK: latency to storage cluster exceeded threshold..2025-09T01:38,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T18:01:24.0000 POWER: Power save mode: 323 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T18:01:24.0000 POWER: Power save mode: 323 nodes: [2025-09-03T18:01:24.0000 POWER: [2025-09-03T18:01:24.0000 POWER: Power save mode,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T02:53:30.0000 POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete,info,infra,"power,save,complete",1,] [2025-09-03T02:53:30.0000 POWER: no more nodes resume for job]]5-09-03T02:53:30.0000 POWER: no more nodes resume for job JobId=986] [2025-09-,<*> POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete
,[2025-09-03T09:42:15.0000 POWER: Power save mode: 337 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T09:42:15.0000 POWER: Power save mode: 337 nodes. [2025-09-03T09:42:15.0000 POWER: Power save mode: 337 nodes.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T18:28:19.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T18:28:19.0000 State of 0 reservations recovered recovered recovered recovered2025-09-03T18:28:19.0000 State of 0 reservations recovered recovered0000 State of 0 reservations recovered recovered]-03T18:28,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
,[2025-09-03T18:54:27.0000 sched: Allocate JobId=952 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8-dy-c7i4xlarge-3]d-dy-c7i4xlarge-3] [2025-09-03T18:,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T22:23:18.0000 POWER: Power save mode: 329 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T22:23:18.0000 POWER: Power save mode: 329 nodes. [2025-09-03T22:23:18.0000 POWER: Power save mode: 329 nodes.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T03:02:09.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-3 now responding.]-c7i4xlarge-3] [2025-09T03:02:09.0000 Node cpu-8c-std-dy-,<*> Node <*> now responding,node_now_responding
,[2025-09-03T13:22:53.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,cpu=8c-std-dy-c7i4xlarge-4]].0000] [2025-09-03T13:22:53.0000--.0000] [2025-09-03T13:22:,<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
,[2025-09-03T21:48:18.0000 sched: Allocate JobId=1017 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-3-dy-c7i4xlarge-3]dy-c7i4xlarge-3] [2025-09-03T21:48:18.0000 sched,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T00:52:00.0000 POWER: Power save mode: 332 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T00:52:00.0000 POWER: Power save mode: 332 nodes: [2025-09-03T00:52:00.0000 POWER: [2025-09-03T00:52:00.0000 POWER: Power save mode: 332,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,"[2025-09-03T06:17:21.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T06:17:21.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T06:17:21.0000] [","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
,"[2025-09-03T00:06:48.0000 Gres Name=gpu Type=l4 Count=8 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09T00:06:48.0000] [2025-09-03T00:06:48.0000 Gres Name=gpu Type=l4 Count=8 Gres Name=gpu] [2025-09-03T00:06:48.0000 Gre,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,[2025-09-03T16:17:23.0000 POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",19,] [2025-09-03T16:17:23.0000 POWER: power_save: waking nodes]] [2025-09-03T16:17:23.0000 POWER: power_save: waking nodes],<*> POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake
,"[2025-09-03T07:46:34.0000 Gres Name=gpu Type=l4 Count=5 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T07:46:34.0000 Gres] [2025-09-03T07:46:34.0000 Gres] [2025-09-03T07:46:34.0000 Gres] [2025-09-03T07:46:34,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,[2025-09-03T22:32:58.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T06:40:49.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09-03T06:40:49.0000 select/cons_tres: preparing for 19 partitions. [2025-09-03T06:40:49.0000 select/cons_tres: preparing for 19 partitions.,<*> select/cons_tres: preparing for 19 partitions,unknown
,[2025-09-03T08:45:03.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T08:45:03.0000 select/cons_tres: preparing for 19 partitions..] [2025-09T08:45:03.0000 select/cons_tres: preparing for 19 partitions] [2025-09-03T,<*> select/cons_tres: preparing for 19 partitions,unknown
,[2025-09-03T11:52:42.0000 _slurm_rpc_submit_batch_job: JobId=965 InitPrio=33623 usec=6540,job_submit_batch,info,scheduler,"job,submit,batch",51,[2025-09-03T11:52:42.0000 _slurm_rpc_submit_batch_job: [2025-09-03T11:52:42.0000 _slurm_rpc_submit_batch,[2025-09-03T11:52:42.0000 _slurm_rpc_submit_batch_job: JobId=965 InitPrio=33623 usec=6540,job_submit_batch
,"[2025-09-03T14:46:29.0000 Gres Name=gpu Type=l4 Count=2 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T14:46:29.0000 Gres] [2025-09-03T14:46:29.0000 Gres] [2025-09-03T14:46:29.0000 Gres] [2025-09-03T14:46:29,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,[2025-09-03T11:29:15.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T11:29:15.0000.2025-09-03T11:29:15.00000000] [2025-09-03T11:29:15.0000 State of 0 reservations recovered recovered.]:29:15.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
,[2025-09-03T21:31:33.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T21:31:33.0000.2025-09-03T21:31:33.0000 State of 0 reservations recovered recovered.0000] [2025-09-03T21:31:33.0000 State of 0 reservations recovered recovered.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
,[2025-09-03T15:41:47.0000 _slurm_rpc_submit_batch_job: JobId=977 InitPrio=35779 usec=5532,job_submit_batch,info,scheduler,"job,submit,batch",52,5-09-03T15:41:40.00 _slurm_rpc_submit_batch_job: [2025-09-03T15:41:40.00 _slurm_rpc_submit_batch_job:,[2025-09-03T15:41:47.0000 _slurm_rpc_submit_batch_job: JobId=977 InitPrio=35779 usec=5532,job_submit_batch
,[2025-09-03T14:15:05.0000 POWER: Power save mode: 335 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T14:15:05.0000 POWER: Power save mode: 335 nodes [2025-09-03T14:15:05.0000 POWER: Power save mode: 335 nodes [2025-09-03T14:15:05.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,"[2025-09-03T18:13:48.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T18:13:13.0000] [2025-09-03T18:13:13.0000 [2025-09-03T18:13:13.0000 SchedulerParameters=preempt_youngest_first,bf_re","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
,[2025-09-03T03:00:54.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error,high,storage,"disk,scrubber,error",14,[2025-09-03T03:00:54.0000 DISK: scrubber error while cleaning temp files.] [2025-09-03T03:00:54.0000 DISK: scrubber error while cleaning temp files.,<*> DISK: scrubber error while cleaning temp files,disk_scrubber_error
,[2025-09-03T10:29:37.0000 _slurm_rpc_submit_batch_job: JobId=960 InitPrio=34740 usec=5010,job_submit_batch,info,scheduler,"job,submit,batch",53,5-09-03T10:29:37.0000 _slurm_rpc_submit_batch_job:] [2025-09-03T10:29:37.0000 _slurm_rpc_submit_job,[2025-09-03T10:29:37.0000 _slurm_rpc_submit_batch_job: JobId=960 InitPrio=34740 usec=5010,job_submit_batch
,[2025-09-03T05:21:26.0000 POWER: Power save mode: 337 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T05:21:26.0000 POWER: Power save mode: 337 nodes: [2025-09-03T05:21:26.0000 POWER: [2025-09-03T05:21:26.0000 POWER: Power save mode,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T01:23:28.0000 _slurm_rpc_submit_batch_job: JobId=1024 InitPrio=34747 usec=6494,job_submit_batch,info,scheduler,"job,submit,batch",54,[2025-09-03T01:23:28.0000 _slurm_rpc_submit_batch_job: [2025-09-03T01:23:28.0000 _slurm_rpc_submit_batch,[2025-09-03T01:23:28.0000 _slurm_rpc_submit_batch_job: JobId=1024 InitPrio=34747 usec=6494,job_submit_batch
,[2025-09-03T06:58:29.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T06:58:29.0000 Running as primary controller.] [2025-09-03T06:58:29.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
,[2025-09-03T00:43:48.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T00:43:48.0000 Running as primary controller.] [2025-09-03T00:43:48.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
,[2025-09-03T19:40:47.0000 sched: Allocate JobId=977 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,std-dy-c7i4xlarge-2-dy-c7i4xlarge-2]-dy-c7i4xlarge-2] [2025-09-03T19:40:03.0000 sched: Allocate Job,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T00:37:43.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T00:37:43.0000 select/cons_tres: preparing for 19 partitions..]] [2025-09-03T00:37:43.0000 select/cons_tres: preparing for 19 partitions..,<*> select/cons_tres: preparing for 19 partitions,unknown
,[2025-09-03T21:19:31.0000 POWER: Power save mode: 338 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T21:19:31.0000 POWER: Power save mode: 338 nodes. [2025-09-03T21:19:31.0000 POWER: Power save mode: 338 nodes.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T22:24:43.0000 POWER: Power save mode: 323 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T22:24:43.0000 POWER: Power save mode: 323 nodes: [2025-09-03T22:24:43.0000 POWER: [2025-09-03T22:24:43.0000 POWER: Power save mode,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,"[2025-09-03T11:29:34.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T11:29:34.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T11:29:34.0000 [202","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
,[2025-09-03T12:58:04.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-3 now responding.]-c7i4xlarge-3] [2025-09-03T12:58:04.0000 Node cpu-8c-std-d,<*> Node <*> now responding,node_now_responding
,[2025-09-03T10:06:01.0000 POWER: Power save mode: 331 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T10:06:01.0000 POWER: Power save mode: 331 nodes [2025-09-03T10:06:01.0000 POWER: Power save mode: 331 nodes [2025-09-03T10:06:01.0000 POWER,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,"[2025-09-03T05:04:53.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T05:04:53.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T05:04:53.0000] [","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
,[2025-09-03T18:10:52.0000 _slurm_rpc_submit_batch_job: JobId=961 InitPrio=30777 usec=5525,job_submit_batch,info,scheduler,"job,submit,batch",55,5-09-03T18:10:10.0000 _slurm_rpc_submit_batch_job: [2025-09-03T18:10:10.0000 _slurm_rpc_submit_batch_job,[2025-09-03T18:10:52.0000 _slurm_rpc_submit_batch_job: JobId=961 InitPrio=30777 usec=5525,job_submit_batch
,[2025-09-03T21:10:29.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T21:10:29.0000 AUTH: authentication failure for user admin] [2025-09-03T21:10:29.0000 AUTH: authentication failure for user admin] [2025-09-03T21:10:29.0000,<*> AUTH: authentication failure for user admin,auth_failure
,[2025-09-03T23:42:06.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,[2025-09-03T23:42:06.0000 State of 0 reservations recovered. [2025-09-03T23:42:06.0000 State of 0 reservations recovered.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
,[2025-09-03T23:46:31.0000 POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete,info,infra,"power,save,complete",1,] [2025-09-03T23:46:31.0000 POWER: no more nodes resume for job] [2025-09-03T23:46:31.0000 POWER: no more nodes resume for job] POWER: no more nodes:,<*> POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete
,[2025-09-03T23:15:08.0000 _slurm_rpc_submit_batch_job: JobId=972 InitPrio=32933 usec=6989,job_submit_batch,info,scheduler,"job,submit,batch",56,[2025-09-03T23:15:08.0000 _slurm_rpc_submit_batch_job: JobId=972 InitPrio=32933 usec=6989] [2025-09-03T23:15:,[2025-09-03T23:15:08.0000 _slurm_rpc_submit_batch_job: JobId=972 InitPrio=32933 usec=6989,job_submit_batch
,[2025-09-03T10:58:32.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error,high,storage,"disk,scrubber,error",14,[2025-09-03T10:58:32.0000 DISK: scrubber error while cleaning temp files.] [2025-09-03T10:58:32.0000 DISK: scrubber error while cleaning temp files.,<*> DISK: scrubber error while cleaning temp files,disk_scrubber_error
,[2025-09-03T05:19:35.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,cpu=8c-std-dy-c7i4xlarge-4.]]-c7i4xlarge-4]-.00005-09-03T05:19:35.0000 POWER: Node:,<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
,[2025-09-03T05:25:16.0000 sched: Allocate JobId=972 NodeList=cpu-8c-std-dy-c7i4xlarge-1 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-1-dy-c7i4xlarge-1]d-dy-c7i4xlarge-1] [2025-09-03T05:25:16.0000,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T02:29:18.0000 POWER: Power save mode: 335 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T02:29:18.0000 POWER: Power save mode: 335 nodes. [2025-09-03T02:29:18.0000 POWER: Power save mode: 335 nodes.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T16:28:51.0000 POWER: Power save mode: 325 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,] [2025-09-03T16:28:28.0000 POWER: Power save mode: 325 nodes]] [2025-09-03T16:28:28.0000 POWER: Power save mode: 325 nodes] [2025-09-03,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,"[2025-09-03T03:27:06.0000 Gres Name=gpu Type=l4 Count=6 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09T03:27:06.0000 Gres] [2025-09-03T03:27:06.0000 Gres] [2025-09-03T03:27:06.0000 Gres] [2025-09-03T03:27:06.0000 Gres] [,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,[2025-09-03T08:10:00.0000 POWER: Power save mode: 324 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T08:10:00.0000 POWER: Power save mode: 324 nodes [2025-09-03T08:10:00.0000 POWER: Power save mode: 324 nodes [2025-09-03T08:10:00.0000 POWER,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T12:16:05.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T12:16:05.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding now now]-c7i4xlarge-1 now responding now-c7i4x,<*> Node <*> now responding,node_now_responding
,[2025-09-03T04:17:39.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T04:17:39.0000.2025-09-03T04:17:39.00000000] [2025-09-03T04:17:39.0000 [2025-09-03T04:17:39.0000 State of,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
,[2025-09-03T22:00:44.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T22:00:44.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding. [2025-09-03T22:00:44.0000 Node cpu-8c-std-,<*> Node <*> now responding,node_now_responding
,[2025-09-03T03:43:04.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T22:03:29.0000 sched: Allocate JobId=1013 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-3] [2025-09-03T22:03:29.0000 sched: Allocate JobId=1013-dy-c7i4xlarge-3],<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T20:08:18.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-1 now responding.]-c7i4xlarge-1 now responding-c7i4xlarge-1]-c7i4x5-09-03T20:,<*> Node <*> now responding,node_now_responding
,"[2025-09-03T06:33:13.0000 Gres Name=gpu Type=l4 Count=2 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T06:33:13.0000 Gres] [2025-09-03T06:33:13.0000 Gres] [2025-09-03T06:33:13.0000 Gres] [2025-09-03T06:33:13,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,[2025-09-03T16:49:02.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T16:49:02.0000 AUTH: authentication failure for user admin] [2025-09-03T16:49:02.0000 AUTH: authentication failure for user admin] [2025-09-03T16:49:02.0000 AUTH:,<*> AUTH: authentication failure for user admin,auth_failure
,[2025-09-03T15:19:45.0000 AUTH: user root failed password authentication,auth_failure,high,security,"auth,security,login_failure",30,[2025-09-03T15:19:45.0000 AUTH: user root failed password authentication.] [2025-09-03T15:19:45.0000 AUTH: user root failed password authentication.,<*> AUTH: user root failed password authentication,auth_failure
,[2025-09-03T22:35:50.0000 Node cpu-8c-std-dy-c7i4xlarge-4 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-4 now responding..c7i4xlarge-4 now responding.c7i4xlarge-4 now responding.-c7i4xlarge-4-c7i4,<*> Node <*> now responding,node_now_responding
,[2025-09-03T00:16:31.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T00:16:31.0000 select/cons_tres: preparing for 19 partitions..2025-09T00:16:31.0000 select/cons_tres: preparing for 19 partitions.] [2025-09T00,<*> select/cons_tres: preparing for 19 partitions,unknown
,[2025-09-03T04:19:35.0000 AUTH: failed password for ubuntu,auth_failure,high,security,"auth,security,login_failure",23,] [2025-09-03T04:19:35.0000 AUTH: failed password for ubuntu.]. [2025-09-03T04:19:35.0000 AUTH: failed password for ubuntu..,<*> AUTH: failed password for ubuntu,auth_failure
,[2025-09-03T17:21:20.0000 POWER: Power save mode: 335 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T17:21:20.0000 POWER: Power save mode: 335 nodes [2025-09-03T17:21:20.0000 POWER: Power save mode: 335 nodes [2025-09-03T17:21:20.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T16:42:53.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold..].] [2025-09-03T16:42:53.0000 [2025-09-03T16:42:53.0000] [2025-09-03T16:42:53.0000.],<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T05:34:28.0000 sched: Allocate JobId=954 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T05:34:28.0000 sched:-dy-c7i4xlarge-2]d-dy-c7i4xlarge-2] [2025-09-03T05:34:28.0000 sched,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T05:25:20.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T05:25:20.0000 select/cons_tres: preparing for 19 partitions..]. [2025-09-03T05:25:20.0000 select/cons_tres: preparing for 19 partitions..,<*> select/cons_tres: preparing for 19 partitions,unknown
,[2025-09-03T20:18:25.0000 _slurm_rpc_submit_batch_job: JobId=975 InitPrio=32293 usec=5314,job_submit_batch,info,scheduler,"job,submit,batch",57,5-09-03T20:18:25.0000 [2025-09-03T20:18:25.0000 _slurm_rpc_submit_batch_job: [2025-09-03T20:18:25.0000 _s,[2025-09-03T20:18:25.0000 _slurm_rpc_submit_batch_job: JobId=975 InitPrio=32293 usec=5314,job_submit_batch
,[2025-09-03T01:53:48.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T01:53:48.0000 AUTH: authentication failure for user admin] [2025-09-03T01:53:48.0000 AUTH: authentication failure for user admin] [2025-09-03T01:53:48.0000,<*> AUTH: authentication failure for user admin,auth_failure
,[2025-09-03T02:58:35.0000 POWER: Power save mode: 323 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T02:58:35.0000 POWER: Power save mode: 323 nodes: [2025-09-03T02:58:35.0000 POWER: [2025-09-03T02:58:35.0000 POWER: Power save mode,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T17:50:44.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold...].] [2025-09-03T17:50:44.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T17:50:44.0000 NETWORK,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T06:18:38.0000 POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",19,] [2025-09-03T06:18:38.0000 POWER: power_save: waking nodes]] [2025-09-03T06:18:38.0000 POWER: power_save: waking nodes],<*> POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake
,[2025-09-03T01:21:25.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T01:21:25.0000 Running as primary controller.] [2025-09-03T01:21:25.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
,[2025-09-03T05:20:06.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error,high,storage,"disk,scrubber,error",14,[2025-09-03T05:20:06.0000 DISK: scrubber error while cleaning temp files.] [2025-09-03T05:20:06.0000 DISK: scrubber error while cleaning temp files.,<*> DISK: scrubber error while cleaning temp files,disk_scrubber_error
,[2025-09-03T14:39:49.0000 _slurm_rpc_submit_batch_job: JobId=954 InitPrio=34048 usec=6726,job_submit_batch,info,scheduler,"job,submit,batch",58,] [2025-09-03T14:39:39.0000 _slurm_rpc_submit_batch_job: [2025-09-03T14:39:39.0000 _slurm_rpc_submit_,[2025-09-03T14:39:49.0000 _slurm_rpc_submit_batch_job: JobId=954 InitPrio=34048 usec=6726,job_submit_batch
,[2025-09-03T11:01:49.0000 POWER: Power save mode: 323 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T11:01:49.0000 POWER: Power save mode: 323 nodes: [2025-09-03T11:01:49.0000 POWER: [2025-09-03T11:01:49.0000 POWER: Power save mode,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T04:25:43.0000 POWER: Power save mode: 320 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T04:25:43.0000 POWER: Power save mode: 320 nodes. [2025-09-03T04:25:43.0000 POWER: [2025-09-03T04:25:43.0000 POWER: Power save mode,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,"[2025-09-03T20:47:56.0000 Gres Name=gpu Type=l4 Count=2 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T20:47:56.0000] [2025-09-03T20:47:56.0000 Gres4]]] [2025-09-03T20:47:56.0000 Gres=gpu] [2025-0,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,[2025-09-03T19:22:47.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T19:22:47.0000.2025-09-03T19:22:47.0000 State of 0 reservations recovered recovered.0000] [2025-09-03T19:22:47.0000 State of 0 reservations recovered recovered.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
,[2025-09-03T10:51:33.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,cpu=8c-std-dy-c7i4xlarge-4]]]-c7i4xlarge-4]-.00005-09-03T10:51:33.00005-09-03T10,<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
,[2025-09-03T09:51:25.0000 Node cpu-8c-std-dy-c7i4xlarge-4 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T09:51:25.0000 Node cpu-8c-std-dy-c7i4xlarge-4 now responding. [2025-09-03T09:51:25.0000 Node cpu-8c-st,<*> Node <*> now responding,node_now_responding
,[2025-09-03T03:35:48.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T03:35:48.0000 select/cons_tres: preparing for 19 partitions..]. [2025-09T03:35:48.0000 select/cons_tres: preparing for 19 partitions..,<*> select/cons_tres: preparing for 19 partitions,unknown
,[2025-09-03T15:24:01.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,cpu=8c-std-dy-c7i4xlarge-4.]]-c7i4xlarge-4]-i42025-09-03T15:24:01.0000]5:005-09-,<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
,[2025-09-03T11:04:36.0000 _slurm_rpc_submit_batch_job: JobId=997 InitPrio=33665 usec=5225,job_submit_batch,info,scheduler,"job,submit,batch",59,5-09-03T11:04:36.0000 [2025-09-03T11:04:36.0000 _slurm_rpc_submit_batch_job: [2025-09-03T11:04:36.0000 _s,[2025-09-03T11:04:36.0000 _slurm_rpc_submit_batch_job: JobId=997 InitPrio=33665 usec=5225,job_submit_batch
,[2025-09-03T13:50:13.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,.]..].] [2025-09T13:50:13.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T13:50:13.0000 NETWORK:].] [202,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T09:57:45.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold...].] [2025-09-03T09:57:45.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T09:57:45.0000 NETWORK,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T20:27:35.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,...].] [2025-09-03T20:27:35.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T20:27:35.0000 NETWORK: latency to storage cluster exceeded,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T09:22:20.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T09:22:20.0000.2025-09-03T09:22:20.0000 State of 0 reservations recovered recovered.0000] [2025-09-03T09:22:20.0000 State of 0 reservations recovered recovered.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
,"[2025-09-03T20:58:12.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T20:58:12.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T20:58:12.0000 [2025-09","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
,[2025-09-03T18:19:30.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,[2025-09-03T18:19:30.0000 State of 0 reservations recovered recovered. [2025-09-03T18:19:30.0000 State of 0 reservations recovered.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
,[2025-09-03T06:37:04.0000 POWER: Power save mode: 340 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T06:37:04.0000 POWER: Power save mode: 340 nodes [2025-09-03T06:37:04.0000 POWER: Power save mode: 340 nodes [2025-09-03T06:37:04.0000 POWER,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T23:22:29.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T23:22:29.0000 Running as primary controller.] [2025-09-03T23:22:29.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
,[2025-09-03T05:12:23.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T05:12:23.0000 State of 0 reservations recovered recovered recovered recovered2025-09-03T05:12:23.0000 State of 0 reservations recovered recovered recovered0000]-03T05:12:23.00005-0,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
,[2025-09-03T13:48:22.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T13:48:22.0000 AUTH: authentication failure for user admin] [2025-09-03T13:48:22.0000 AUTH: authentication failure for user admin] [2025-09-03T13:48:22.0000,<*> AUTH: authentication failure for user admin,auth_failure
,[2025-09-03T05:37:02.0000 sched: Allocate JobId=1025 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T05:37:02.0000 sched:-dy-c7i4xlarge-2]dy-c7i4xlarge-2] [2025-09-03T05:37:02.0000 sched: Allocate,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T05:45:09.0000 sched: Allocate JobId=979 NodeList=cpu-8c-std-dy-c7i4xlarge-4 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T05:45:09.0000 sched: Allocate JobId=979-dy-c7i4xlarge-4]c7i4xlarge-4] [2025-09-03T05:45:09.0000,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T09:17:02.0000 _slurm_rpc_submit_batch_job: JobId=1014 InitPrio=31247 usec=6682,job_submit_batch,info,scheduler,"job,submit,batch",60,[2025-09-03T09:17:02.0000 _slurm_rpc_submit_batch_job: [2025-09-03T09:17:02.0000 _slurm_rpc_submit_batch_job,[2025-09-03T09:17:02.0000 _slurm_rpc_submit_batch_job: JobId=1014 InitPrio=31247 usec=6682,job_submit_batch
,"[2025-09-03T12:08:19.0000 Gres Name=gpu Type=l4 Count=6 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T12:08:19.0000 Gres] [2025-09-03T12:08:19.0000 Gres] [2025-09-03T12:08:19.0000 Gres] [2025-09-03T12:08:19,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,"[2025-09-03T08:03:33.0000 Gres Name=gpu Type=l4 Count=7 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09T08:03:33.0000] [2025-09-03T08:03:33.0000 Gres Name=gpu Type=l4 Count=74]2025-09T08:03:33.0000 Gres]202,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,[2025-09-03T00:29:51.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,node cpu=8c-std-dy-c7i4xlarge-4]]] POWER: Node cpu-8c-std-dy-c7i4xlarge-4--.0000,<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
,[2025-09-03T10:58:42.0000 sched: Allocate JobId=1022 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T10:58:42.0000-dy-c7i4xlarge-3]dy-c7i4xlarge-3] [2025-09-03T10:58:42.0000 sched: Allocate JobI,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T05:59:34.0000 POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete,info,infra,"power,save,complete",1,] [2025-09-03T05:59:34.0000 POWER: no more nodes resume for job] [2025-09-03T05:59:34.0000 POWER: no more nodes resume for job]:59:34.0000 POWER,<*> POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete
,"[2025-09-03T06:22:38.0000 Gres Name=gpu Type=l4 Count=8 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T06:22:38.0000 Gres] [2025-09-03T06:22:38.0000 Gres] [2025-09-03T06:22:38.0000 Gres] [2025-09-03T06:22:38,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,[2025-09-03T11:56:30.0000 sched: Allocate JobId=998 NodeList=cpu-8c-std-dy-c7i4xlarge-5 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-5-dy-c7i4xlarge-5]dy-c7i4xlarge-5] [2025-09-03T11:56:30.0000 sched,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,"[2025-09-03T20:02:06.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T20:02:06.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T20:02:06.0000] [2025-0","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
,[2025-09-03T01:23:06.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,[2025-09-03T01:23:06.0000 NETWORK: latency to storage cluster exceeded threshold. [2025-09-03T01:23:06.0000 NETWORK: latency to storage cluster exceeded threshold.,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T10:40:25.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T10:40:25.0000 select/cons_tres: preparing for 19 partitions.. [2025-09-03T10:40:25.0000 select/cons_tres: preparing for 19 partitions]..,<*> select/cons_tres: preparing for 19 partitions,unknown
,[2025-09-03T04:55:53.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold..].] [2025-09-03T04:55:53.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T04:55:53.0000 to storage cluster,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T05:08:16.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold..].] [2025-09-03T05:08:16.0000 [2025-09-03T05:08:16.0000 NETWORK: latency to storage cluster exceeded threshold..].0000,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T16:54:41.0000 DISK: /var is 93% full on node cpu-8c-std,disk_usage_high,warning,storage,"disk,usage,high",24,] [2025-09-03T16:54:41.0000 DISK:] [2025-09-03T16:54:41.0000 DISK:]] [2025-09-03T16:54:41.0000 DISK:,<*> DISK: /var is <*> full on node cpu-8c-std,disk_usage_high
,[2025-09-03T00:34:37.0000 AUTH: user root failed password authentication,auth_failure,high,security,"auth,security,login_failure",30,[2025-09-03T00:34:37.0000 AUTH: user root failed password authentication.,<*> AUTH: user root failed password authentication,auth_failure
,[2025-09-03T16:18:04.0000 AUTH: user root failed password authentication,auth_failure,high,security,"auth,security,login_failure",30,2025-09-03T16:18:04.0000 AUTH: user root failed password authentication..: user root failed password authentication]] [2025-09-03T16:18:04.0000 AUTH: user root failed password authentication..2025-0,<*> AUTH: user root failed password authentication,auth_failure
,[2025-09-03T01:31:19.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T01:31:19.0000 AUTH: authentication failure for user admin] [2025-09-03T01:31:19.0000 AUTH: authentication failure for user admin] [2025-09-03T01:31:19.0000,<*> AUTH: authentication failure for user admin,auth_failure
,[2025-09-03T06:56:09.0000 POWER: Power save mode: 339 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T06:56:09.0000 POWER: Power save mode: 339 nodes [2025-09-03T06:56:09.0000 POWER: Power save mode: 339 nodes] [2025-09-03T06:56:09.0000,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T04:57:16.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T04:57:16.0000 select/cons_tres: preparing for 19 partitions..]. [2025-09-03T04:57:16.0000 select/cons_tres: preparing for 19 partitions..,<*> select/cons_tres: preparing for 19 partitions,unknown
,[2025-09-03T09:55:30.0000 sched: Allocate JobId=971 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,std-dy-c7i4xlarge-3-dy-c7i4xlarge-3]d-dy-c7i4xlarge-3-std-dy-c7i4xlarge-3]cpu,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T08:12:04.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T08:12:04.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding.]-c7i4xlarge-5] [2025-09-03T08:12:,<*> Node <*> now responding,node_now_responding
,"[2025-09-03T14:27:13.0000 Gres Name=gpu Type=l4 Count=2 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T14:27:13.0000 Gres] [2025-09-03T14:27:13.0000 Gres] [2025-09-03T14:27:13.0000 Gres] [2025-09-03T14:27:13,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,[2025-09-03T13:33:35.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T13:33:35.0000.2025-09-03T13:33:35.0000 State of 0 reservations recovered recovered.0000] [2025-09-03T13:33:35.0000 State of 0 reservations recovered recovered.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
,[2025-09-03T03:12:32.0000 sched: Allocate JobId=989 NodeList=cpu-8c-std-dy-c7i4xlarge-4 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-4-dy-c7i4xlarge-4]-dy-c7i4xlarge-4] [2025-09-03T03:12:32.0000 s,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T05:02:10.0000 DISK: /var is 89% full on node cpu-8c-std,disk_usage_high,warning,storage,"disk,usage,high",24,] [2025-09-03T05:02:10.0000 DISK: /var is 89% full on node cpu-8c-std]]] [2025-09-03T05:02:10.0000 DISK:],<*> DISK: /var is <*> full on node cpu-8c-std,disk_usage_high
,[2025-09-03T19:21:53.0000 Node cpu-8c-std-dy-c7i4xlarge-2 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T19:21:53.0000 Node cpu-8c-std-dy-c7i4xlarge-2 now responding.]-c7i4xlarge-2 now responding-c7i4xlarge-2,<*> Node <*> now responding,node_now_responding
,[2025-09-03T08:18:14.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T08:18:14.0000.2025-09-03T08:18:14.0000 State of 0 reservations recovered recovered.0000] [2025-09-03T08:18:14.0000 State of 0 reservations recovered recovered.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
,[2025-09-03T19:02:54.0000 sched: Allocate JobId=1028 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T19:02:54.0000 sched:-dy-c7i4xlarge-3]d-dy-c7i4xlarge-3] [2025-09-03T19:02:54.0000 sched,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,"[2025-09-03T13:57:28.0000 Gres Name=gpu Type=l4 Count=5 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T13:57:28.0000 Gres] [2025-09-03T13:57:28.0000 Gres] [2025-09-03T13:57:28.0000 Gres] [2025-09-03T13:57:28,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,[2025-09-03T03:38:25.0000 sched: Allocate JobId=974 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-2-dy-c7i4xlarge-2]-dy-c7i4xlarge-2] [2025-09-03T03:38:25.0000 s,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T09:53:15.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T09:53:15.0000 AUTH: authentication failure for user admin] [2025-09-03T09:53:15.0000 AUTH: authentication failure for user admin] [2025-09-03T09:53:15.0000,<*> AUTH: authentication failure for user admin,auth_failure
,[2025-09-03T18:14:17.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-1 now responding now now]-c7i4xlarge-1 now responding now-c7i4xlarge-1]-c7i4xlarge-1-5-0,<*> Node <*> now responding,node_now_responding
,[2025-09-03T21:22:08.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,cpu=8c-std-dy-c7i4xlarge-4.]]-c7i4xlarge-4]-i4xlarge-42025-09-03T21:22:08.0000] [5-0,<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
,[2025-09-03T06:40:51.0000 _slurm_rpc_submit_batch_job: JobId=982 InitPrio=30833 usec=6290,job_submit_batch,info,scheduler,"job,submit,batch",61,[2025-09-03T06:40:40.0000 _slurm_rpc_submit_batch_job: JobId=982 InitPrio=30833 usec=6290 [2025-09-03T06:40:,[2025-09-03T06:40:51.0000 _slurm_rpc_submit_batch_job: JobId=982 InitPrio=30833 usec=6290,job_submit_batch
,"[2025-09-03T15:29:03.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T15:29:03.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T15:29:03.0000 SchedulerPara","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
,[2025-09-03T17:13:55.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T17:13:55.0000 State of.2025-09-03T17:13:55.0000 State of 0 reservations recovered recovered.0000] [2025-09-03T17:13:55.0000 State of 0 reservations recovered,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
,[2025-09-03T01:52:53.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error,high,storage,"disk,scrubber,error",14,] [2025-09T01:52:53.0000 DISK: scrubber error while cleaning temp files..]] [2025-09-03T01:52:53.0000 DISK: scrubber error while cleaning temp files..],<*> DISK: scrubber error while cleaning temp files,disk_scrubber_error
,[2025-09-03T18:45:37.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T18:45:37.0000 AUTH: authentication failure for user admin] [2025-09-03T18:45:37.0000 AUTH: authentication failure for user admin] [2025-09-03T18:45:37.0000,<*> AUTH: authentication failure for user admin,auth_failure
,[2025-09-03T14:55:01.0000 _slurm_rpc_submit_batch_job: JobId=963 InitPrio=31283 usec=6299,job_submit_batch,info,scheduler,"job,submit,batch",62,[2025-09-03T14:55:01.0000 _slurm_rpc_submit_batch_job: JobId=963 InitPrio=31283 usec=6299 [2025-09-03T14:55:01.,[2025-09-03T14:55:01.0000 _slurm_rpc_submit_batch_job: JobId=963 InitPrio=31283 usec=6299,job_submit_batch
,"[2025-09-03T18:06:45.0000 Gres Name=gpu Type=l4 Count=5 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T18:06:06.0000 Gres] [2025-09-03T18:06:05.0000 Gres] [2025-09-03T18:06:06.0000 Gres] [2025-09-03T18:06:05.0000,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
,[2025-09-03T00:51:55.0000 POWER: Power save mode: 325 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T00:51:55.0000 POWER: Power save mode: 325 nodes: [2025-09-03T00:51:55.0000 POWER: [2025-09-03T00:51:55.0000 POWER: Power save mode,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T17:41:32.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T17:41:32.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding now now]-c7i4xlarge-3]-c7i4x-,<*> Node <*> now responding,node_now_responding
,[2025-09-03T08:52:35.0000 sched: Allocate JobId=1016 NodeList=cpu-8c-std-dy-c7i4xlarge-5 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T08:52:35.0000 sched:-dy-c7i4xlarge-5]d-dy-c7i4xlarge-5] [2025-09-03T08:52:35.0000 sched,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T04:24:19.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T04:24:19.0000 State of 0 reservations recovered recovered recovered recovered2025-09-03T04:24:19.0000 State of0000] [2025-09-03T04:24:19.0000 State of 0 reservations recovered,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
,"[2025-09-03T16:00:45.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T16:00:00.0000] [2025-09-03T16:00:00.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320,bf_","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
,[2025-09-03T20:05:42.0000 sched: Allocate JobId=966 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,[2025-09-03T20:05:42.0000 sched: Allocate JobId=966-dy-c7i4xlarge-3]d-dy-c7i4xlarge-3] [2025-09-03T,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T21:35:04.0000 DISK: /var is 86% full on node cpu-8c-std,disk_usage_high,warning,storage,"disk,usage,high",24,/var is 86% full on node cpu-8c-std] [2025-09-03T21:35:04.0000 DISK:] /var is 86% full on node cpu-8c-std],<*> DISK: /var is <*> full on node cpu-8c-std,disk_usage_high
,[2025-09-03T17:51:05.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold..].] [2025-09-03T17:51:05.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T17:51:05.0000 NETWORK:,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T21:06:52.0000 sched: Allocate JobId=965 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T21:06:52.0000-dy-c7i4xlarge-2]dy-c7i4xlarge-2] [2025-09-03T21:06:52.0000 sched: Allocate JobI,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T23:56:12.0000 POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",19,] [2025-09-03T23:56:12.0000 POWER: power_save: waking nodes]] [2025-09-03T23:56:12.0000 POWER: power_save: waking nodes]-,<*> POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake
,"[2025-09-03T15:42:01.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T15:42:01.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T15:42:01.0000 SchedulerPara","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
,[2025-09-03T17:06:53.0000 POWER: Power save mode: 329 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T17:06:53.0000 POWER: Power save mode: 329 nodes. [2025-09-03T17:06:53.0000 POWER: Power save mode: 329 nodes.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
,[2025-09-03T10:10:21.0000 sched: Allocate JobId=999 NodeList=cpu-8c-std-dy-c7i4xlarge-1 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,] [2025-09-03T10:10:21.0000 sched: Allocate JobId=999-dy-c7i4xlarge-1]-dy-c7i4xlarge-1] [2025-09-03T10,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
,[2025-09-03T19:15:02.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold.....2025-09-03T19:15:02.0000] [2025-09-03T19:15:02.0000 NETWORK: latency to storage cluster exceeded threshold..2025-09-,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
,[2025-09-03T15:23:44.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T15:23:44.0000 AUTH: authentication failure for user admin] [2025-09-03T15:23:44.0000 AUTH: authentication failure for user admin] [2025-09-03T15:23:44.0000,<*> AUTH: authentication failure for user admin,auth_failure
,[2025-09-03T13:28:31.0000 _slurm_rpc_submit_batch_job: JobId=1025 InitPrio=30573 usec=6786,job_submit_batch,info,scheduler,"job,submit,batch",63,] [2025-09-03T13:28:31.0000 [2025-09-03T13:28:31.0000 _slurm_rpc_submit_batch_job: [2025-09-03T13:28:31.0000,[2025-09-03T13:28:31.0000 _slurm_rpc_submit_batch_job: JobId=1025 InitPrio=30573 usec=6786,job_submit_batch
,[2025-09-03T05:06:11.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T05:06:11.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding now now]-c7i4xlarge-5] [2025-09-03T05:06,<*> Node <*> now responding,node_now_responding
,[2025-09-03T23:04:11.0000 POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",19,] [2025-09-03T23:04:11.0000 POWER: power_save: waking nodes]] POWER: power_save: waking nodes] POWER: power_save: waking nodes-,<*> POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake
