batch_id,processed_at,source_file,job_mode,timestamp,raw,final_label,severity,owner_team,tags,cluster_id,t5_template,cluster_template,auto_label
325b7e2d-ba24-47c5-8b47-88ec3804f608,2025-12-10T23:49:35.865082,test.log,both,2025-09-03T07:52:15.220,[2025-09-03T07:52:15.220] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,gpu_inventory_update,info,infra,"gpu,l4,inventory",1,Gres_FILE Gpu=l4=l4 [2025-09-03T07:52:15.220] Gres Name=gpu Type=l4 Count=1 Gres Gres Gres2025-09-03T07,[2025-09-03T07:52:15.220] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,gpu_inventory_update
325b7e2d-ba24-47c5-8b47-88ec3804f608,2025-12-10T23:49:35.865082,test.log,both,2025-09-03T07:52:15.221,[2025-09-03T07:52:15.221] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,gpu_inventory_update,info,infra,"gpu,l4,inventory",1,Gres_FILE Gres=l4 [2025-09-03T07:52:15.221] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE Gres Name=gpu Type=l4 Count,<*> Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,gpu_inventory_update
325b7e2d-ba24-47c5-8b47-88ec3804f608,2025-12-10T23:49:35.865082,test.log,both,2025-09-03T07:52:15.222,[2025-09-03T07:52:15.222] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,gpu_inventory_update,info,infra,"gpu,l4,inventory",1,Gres=gpu Gres [2025-09-03T07:52:15.222] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE] Gres Name=gpu Type=l4 Count=1,<*> Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,gpu_inventory_update
325b7e2d-ba24-47c5-8b47-88ec3804f608,2025-12-10T23:49:35.865082,test.log,both,2025-09-03T07:52:15.223,[2025-09-03T07:52:15.223] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,gpu_inventory_update,info,infra,"gpu,l4,inventory",1,Gres_FILE gpu=l4 [2025-09-03T07:52:15.223] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE] Gres Name=gpu Type=l4,<*> Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,gpu_inventory_update
325b7e2d-ba24-47c5-8b47-88ec3804f608,2025-12-10T23:49:35.865082,test.log,both,2025-09-03T07:52:15.224,[2025-09-03T07:52:15.224] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,gpu_inventory_update,info,infra,"gpu,l4,inventory",1,Gres=gpu Gres [2025-09-03T07:52:15.224] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE] Gres Name=gpu Type=l4 Count=1,<*> Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,gpu_inventory_update
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T07:52:15.236,"[2025-09-03T07:52:15.236] Gres Name=gpu Type=l4 Count=8 Flags=HAS_FILE,HAS_TYPE,ENV_NVML,ENV_RSMI,ENV_ONEAPI,ENV_OPENCL,ENV_DEFAULT",gpu_inventory_update,info,infra,"gpu,l4,inventory",1,[2025-09-03T07:52:15.236] Gres [2025-09-03T07:52:15.236] Gres [2025-09-03T07:52:15.236] Gres [2025-09-03T07:52:,<*> Gres Name=gpu Type=l4 <*> <*>,gpu_inventory_update
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T07:52:15.241,"[2025-09-03T07:52:15.241] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,HAS_TYPE,ENV_NVML,ENV_RSMI,ENV_ONEAPI,ENV_OPENCL,ENV_DEFAULT",gpu_inventory_update,info,infra,"gpu,l4,inventory",1,[2025-09-03T07:52:15.241] Gres [2025-09-03T07:52:15.241] Gres [2025-09-03T07:52:15.241] Gres [2025-09-03T07:52:,<*> Gres Name=gpu Type=l4 <*> <*>,gpu_inventory_update
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T07:52:15.242,"[2025-09-03T07:52:15.242] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,HAS_TYPE,ENV_NVML,ENV_RSMI,ENV_ONEAPI,ENV_OPENCL,ENV_DEFAULT",gpu_inventory_update,info,infra,"gpu,l4,inventory",1,[2025-09-03T07:52:15.242] Gres [2025-09-03T07:52:15.242] Gres [2025-09-03T07:52:15.242] Gres [2025-09-03T07,<*> Gres Name=gpu Type=l4 <*> <*>,gpu_inventory_update
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T07:52:15.242,"[2025-09-03T07:52:15.242] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,HAS_TYPE,ENV_NVML,ENV_RSMI,ENV_ONEAPI,ENV_OPENCL,ENV_DEFAULT",gpu_inventory_update,info,infra,"gpu,l4,inventory",1,[2025-09-03T07:52:15.242] Gres [2025-09-03T07:52:15.242] Gres [2025-09-03T07:52:15.242] Gres [2025-09-03T07,<*> Gres Name=gpu Type=l4 <*> <*>,gpu_inventory_update
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T07:52:15.243,"[2025-09-03T07:52:15.243] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,HAS_TYPE,ENV_NVML,ENV_RSMI,ENV_ONEAPI,ENV_OPENCL,ENV_DEFAULT",gpu_inventory_update,info,infra,"gpu,l4,inventory",1,[2025-09-03T07:52:15.243] Gres [2025-09-03T07:52:15.243] Gres [2025-09-03T07:52:15.243] Gres [2025-09-03T07:52:,<*> Gres Name=gpu Type=l4 <*> <*>,gpu_inventory_update
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T07:52:15.243,"[2025-09-03T07:52:15.243] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,HAS_TYPE,ENV_NVML,ENV_RSMI,ENV_ONEAPI,ENV_OPENCL,ENV_DEFAULT",gpu_inventory_update,info,infra,"gpu,l4,inventory",1,[2025-09-03T07:52:15.243] Gres [2025-09-03T07:52:15.243] Gres [2025-09-03T07:52:15.243] Gres [2025-09-03T07:52:,<*> Gres Name=gpu Type=l4 <*> <*>,gpu_inventory_update
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T07:52:15.244,[2025-09-03T07:52:15.244] select/cons_tres: part_data_create_array: select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,2,[2025-09-03T07:52:15.244] select/cons_tres: preparing for 19 partitions [2025-09-03T07:52:15.244] select/cons_tres: part_data_create_array: select/cons_,[2025-09-03T07:52:15.244] select/cons_tres: part_data_create_array: select/cons_tres: preparing for 19 partitions,unknown
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T07:52:15.244,[2025-09-03T07:52:15.244] Recovered state of 0 reservations,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",3,[2025-09-03T07:52:15.244] Recovered state of 0 reservations. [2025-09-03T07:52:15.244] [2025-09-03T07:52:15.244] Recovered state of 0 reservations. [,[2025-09-03T07:52:15.244] Recovered state of 0 reservations,cluster_state_recovered_no_reservations
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T07:52:15.244,[2025-09-03T07:52:15.244] State of 0 triggers recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",4,[2025-09-03T07:52:15.244].] [2025-09-03T07:52:15.244]] [2025-09-03T07:52:15.244] [2025-09-03T07:52:,[2025-09-03T07:52:15.244] State of 0 triggers recovered,cluster_state_recovered_no_reservations
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T07:52:15.244,[2025-09-03T07:52:15.244] read_slurm_conf: backup_controller not specified,controller_backup_not_configured,warning,infra,"controller,backup,not_configured",5,[2025-09-03T07:52:15.244] read_slurm_conf: backup_controller not specified [2025-09-03T07:52:15.244] read_slurm_conf: backup_controller not specified [2025-0,[2025-09-03T07:52:15.244] read_slurm_conf: backup_controller not specified,controller_backup_not_configured
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T07:52:15.244,[2025-09-03T07:52:15.244] select/cons_tres: select_p_reconfigure: select/cons_tres: reconfigure,unknown,info,unknown,,6,[2025-09-03T07:52:15.244] select/cons_tres: select_p_reconfigure: select/cons_tres: reconfigure [2025-09-03T07:52:15.244] select/cons_tres: recon,[2025-09-03T07:52:15.244] select/cons_tres: select_p_reconfigure: select/cons_tres: reconfigure,unknown
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T07:52:15.244,[2025-09-03T07:52:15.244] select/cons_tres: part_data_create_array: select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,2,[2025-09-03T07:52:15.244] select/cons_tres: preparing for 19 partitions [2025-09-03T07:52:15.244] select/cons_tres: part_data_create_array: select/cons_,[2025-09-03T07:52:15.244] select/cons_tres: part_data_create_array: select/cons_tres: preparing for 19 partitions,unknown
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T07:52:15.245,[2025-09-03T07:52:15.245] Running as primary controller,controller_primary,info,infra,"controller,primary",7,[2025-09-03T07:52:15.245] Running as primary controller. [2025-09-03T07:52:15.245] Running as primary controller.,[2025-09-03T07:52:15.245] Running as primary controller,controller_primary
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T07:53:16.000,"[2025-09-03T07:53:16.000] SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320,bf_interval=60,bf_continue,bf_busy_nodes,bf_job_part_count_reserve=5,bf_min_age_reserve=60",scheduler_parameters,info,scheduler,"scheduler,config",8,[2025-09-03T07:53:16.000] [2025-09-03T07:53:16.000] [2025-09-03T07:53:16.000] [2025-09-03T07:53:16.000] [202,"[2025-09-03T07:53:16.000] SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320,bf_interval=60,bf_continue,bf_busy_nodes,bf_job_part_count_reserve=5,bf_min_age_reserve=60",scheduler_parameters
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T07:54:51.273,[2025-09-03T07:54:51.273] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T07:54:51.273] POWER: Power save mode: 334 nodes [2025-09-03T07:54:51.273] POWER: Power save mode: 334 nodes [2025-09-03T07:54:,[2025-09-03T07:54:51.273] POWER: Power save mode: 334 nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T08:05:11.347,[2025-09-03T08:05:11.347] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T08:05:11.347] POWER: Power save mode: 334 nodes [2025-09-03T08:05:11.347] POWER: Power save mode: 334 nodes [2025-09-03T08:05:,<*> POWER: Power save mode: 334 nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T08:15:31.423,[2025-09-03T08:15:31.423] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T08:15:31.423] POWER: Power save mode: 334 nodes [2025-09-03T08:15:31.423] POWER: Power save mode: 334 nodes [2025-09-03T08:15:,<*> POWER: Power save mode: 334 nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T08:25:51.498,[2025-09-03T08:25:51.498] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T08:25:51.498] POWER: Power save mode: 334 nodes: [2025-09-03T08:25:51.498] POWER:. [2025-09-03T08:25:51.4,<*> POWER: Power save mode: 334 nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T08:36:11.572,[2025-09-03T08:36:11.572] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T08:36:11.572] POWER: Power save mode: 334 nodes [2025-09-03T08:36:11.572] POWER: Power save mode: 334 nodes [2025-09-03T08:36:,<*> POWER: Power save mode: 334 nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T08:46:31.642,[2025-09-03T08:46:31.642] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T08:46:31.642] POWER: Power save mode: 334 nodes [2025-09-03T08:46:31.642] POWER: Power save mode: 334 nodes [2025-09-03T08:46:,<*> POWER: Power save mode: 334 nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T08:56:51.715,[2025-09-03T08:56:51.715] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T08:56:51.715] POWER: Power save mode: 334 nodes [2025-09-03T08:56:51.715] POWER: Power save mode: 334 nodes [2025-09-03T08:56:,<*> POWER: Power save mode: 334 nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:07:11.790,[2025-09-03T09:07:11.790] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T09:07:11.790] POWER: Power save mode: 334 nodes [2025-09-03T09:07:11.790] POWER: Power save mode: 334 nodes [2025-09-03T09:07:,<*> POWER: Power save mode: 334 nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:17:31.865,[2025-09-03T09:17:31.865] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T09:17:31.865] POWER: Power save mode: 334 nodes [2025-09-03T09:17:31.865] POWER: Power save mode: 334 nodes [2025-09-03T09:17:,<*> POWER: Power save mode: 334 nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:27:51.938,[2025-09-03T09:27:51.938] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T09:27:51.938] POWER: Power save mode: 334 nodes [2025-09-03T09:27:51.938] POWER: Power save mode: 334 nodes [2025-09-03T09:27:,<*> POWER: Power save mode: 334 nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:38:11.010,[2025-09-03T09:38:11.010] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T09:38:11.010] POWER: Power save mode: 334 nodes [2025-09-03T09:38:11.010] POWER: Power save mode: 334 nodes [2025-09-03T09:38:,<*> POWER: Power save mode: 334 nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:38:11.413,[2025-09-03T09:38:11.413] _slurm_rpc_submit_batch_job: JobId=958 InitPrio=35869 usec=6139,job_submit_batch,info,scheduler,"job,submit,batch",10,[2025-09-03T09:38:11.413] _slurm_rpc_submit_batch_job: JobId=958 InitPrio=35869 usec=6139 [2025-09-03T09:38,[2025-09-03T09:38:11.413] _slurm_rpc_submit_batch_job: JobId=958 InitPrio=35869 usec=6139,job_submit_batch
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:38:12.000,[2025-09-03T09:38:12.000] sched: Allocate JobId=958 NodeList=cpu-8c-std-dy-c7i4xlarge-1 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T09:38:12.000] sched: [2025-09-03T09:38:12.000] sched: [2025-09-03T09:38:12.000] sched: [2025-09-03T09,[2025-09-03T09:38:12.000] sched: Allocate JobId=958 NodeList=cpu-8c-std-dy-c7i4xlarge-1 #CPUs=8 Partition=cpu-8c-std,job_allocate
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:38:42.013,[2025-09-03T09:38:42.013] POWER: no more nodes to resume for job JobId=958,cluster_power_save_complete,info,infra,"power,save,complete",12,[2025-09-03T09:38:42.013] POWER: no more nodes to resume for job JobId=958 JobId=958 JobId=958 JobId=958 JobId=958 JobId=95,[2025-09-03T09:38:42.013] POWER: no more nodes to resume for job JobId=958,cluster_power_save_complete
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:38:42.013,[2025-09-03T09:38:42.013] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-1,cluster_power_save_wake,info,infra,"power,save,wakeup",13,cpu_save: waking nodes cpu-std-dy-c7i4xlarge-1_save: waking nodes-dy-c7i4xlarge-1--- POWER,[2025-09-03T09:38:42.013] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-1,cluster_power_save_wake
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:42:37.720,[2025-09-03T09:42:37.720] Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-1 now responding. [2025-09-03T09:42:37.720]. [2025-09-03T09:42:37.720]--c7i4xlarge-1 now responding. [2025-09-,[2025-09-03T09:42:37.720] Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:42:37.720,"[2025-09-03T09:42:37.720] POWER: Node cpu-8c-std-dy-c7i4xlarge-1/cpu-8c-std-dy-c7i4xlarge-1/192.168.13.70 powered up with instance_id=i-0c30e29de39a401f6, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,-1/192.168.13.70-1 and node_name=c7i.4xlarge-1. [2025-09-03T09:42:37.720] POWER: Node0] [2025-09-03T09:42:37.720,"[2025-09-03T09:42:37.720] POWER: Node cpu-8c-std-dy-c7i4xlarge-1/cpu-8c-std-dy-c7i4xlarge-1/192.168.13.70 powered up with instance_id=i-0c30e29de39a401f6, instance_type=c7i.4xlarge",cluster_power_node_online
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:42:45.001,[2025-09-03T09:42:45.001] job_time_limit: Configuration for JobId=958 complete,job_time_limit_configured,info,scheduler,"job,time_limit,config",16,[2025-09-03T09:42:01.001] job_time_limit: Configuration for JobId=958 complete [2025-09-03T09:42:02.001] job_time_limit: Configuration for JobId=958 complete [2025-0,[2025-09-03T09:42:45.001] job_time_limit: Configuration for JobId=958 complete,job_time_limit_configured
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:42:45.001,[2025-09-03T09:42:45.001] Resetting JobId=958 start time for node power up,job_reset_start_time,info,scheduler,"job,reset_start_time,node_power_up",17,[2025-09-03T09:42:45.001]. [2025-09-03T09:42:45.001] [2025-09-03T09:42:45.001] Resetting JobId=958 start time for node,[2025-09-03T09:42:45.001] Resetting JobId=958 start time for node power up,job_reset_start_time
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:48:31.084,[2025-09-03T09:48:31.084] POWER: Power save mode: 333 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T09:48:31.084] POWER: Power save mode: 333 nodes [2025-09-03T09:48:31.084] POWER: Power save mode: 333 nodes [2025-09-03T09:48:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:49:06.084,[2025-09-03T09:49:06.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T09:49:06.084] POWER: Power save mode: 334 nodes [2025-09-03T09:49:06.084] POWER: Power save mode: 334 nodes [2025-09-03T09:49:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:49:41.084,[2025-09-03T09:49:41.084] _slurm_rpc_submit_batch_job: JobId=961 InitPrio=30001 usec=5001,job_submit_batch,info,scheduler,"job,submit,batch",18,[2025-09-03T09:49:41.084] _slurm_rpc_submit_batch_job: [2025-09-03T09:49:41.084] _slurm_rpc_submit_,[2025-09-03T09:49:41.084] _slurm_rpc_submit_batch_job: JobId=961 InitPrio=30001 usec=5001,job_submit_batch
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:50:16.084,[2025-09-03T09:50:16.084] sched: Allocate JobId=962 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T09:50:16.084] sched: [2025-09-03T09:50:16.084] sched: [2025-09-03T09:50:16.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:50:51.084,[2025-09-03T09:50:51.084] POWER: no more nodes to resume for job JobId=963,cluster_power_save_complete,info,infra,"power,save,complete",12,POWER: no more nodes to resume for job [2025-09-03T09:50:51.084] POWER: no more nodes to resume for job JobId=963 [2025-09-03T09:50:51.084] POWER:,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:51:26.084,[2025-09-03T09:51:26.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-3,cluster_power_save_wake,info,infra,"power,save,wakeup",13,_save: waking nodes_save: waking nodes_save: waking nodes-c7i4xlarge-3-c7i4--2025-09-03T09:51:26.084,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:52:01.084,[2025-09-03T09:52:01.084] Node cpu-8c-std-dy-c7i4xlarge-4 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-4 now responding. [2025-09-03T09:52:01.084]. [2025-09-03T09:52:01.084]--c7i4xlarge-4 now responding. [2025-09-,<*> Node <*> now responding,node_now_responding
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:52:36.084,"[2025-09-03T09:52:36.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-5/cpu-8c-std-dy-c7i4xlarge-5/192.168.13.71 powered up with instance_id=i-0c30e29de39a40001, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"_id=i-0c30e29de39a40001, instance_type=c7i.4xlarge-5.-5.00-5/cpu-8c-std-dy-c7i4xlarge-5/192.168",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:53:11.084,[2025-09-03T09:53:11.084] AUTH: user user1 failed password authentication from 10.1.1.101,auth_failure,high,security,"auth,security,login_failure",19,[2025-09-03T09:53:11.084] AUTH: user user1 failed password authentication from 10.1.1.101. [2025-09-03T09:53:11.084] AUTH: user user1 failed password authentication from 10.1.1.101.,[2025-09-03T09:53:11.084] AUTH: user user1 failed password authentication from 10.1.1.101,auth_failure
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:53:46.084,[2025-09-03T09:53:46.084] DISK: /var is 89% full on node cpu-16c-std-2,disk_usage_high,warning,storage,"disk,usage,high",20,[2025-09-03T09:53:46.084] DISK: [2025-09-03T09:53:46.084] [2025-09-03T09:53:46.084] [2025-09-03T09:53:46.0,[2025-09-03T09:53:46.084] DISK: /var is 89% full on node cpu-16c-std-2,disk_usage_high
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:54:21.084,[2025-09-03T09:54:21.084] NETWORK: latency to storage cluster increased to 81ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T09:54:21.084] [2025-09-03T09:54:21.084] [2025-09-03T09:54:21.084] [2025-09-03T09:54:21.084] [202,[2025-09-03T09:54:21.084] NETWORK: latency to storage cluster increased to 81ms (no SLA breach),network_latency_storage_cluster
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:54:56.084,[2025-09-03T09:54:56.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T09:54:56.084] POWER: Power save mode: 334 nodes [2025-09-03T09:54:56.084] POWER: Power save mode: 334 nodes [2025-09-03T09:54:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:55:31.084,[2025-09-03T09:55:31.084] POWER: Power save mode: 335 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T09:55:31.084] POWER: Power save mode: 335 nodes [2025-09-03T09:55:31.084] POWER: Power save mode: 335 nodes [2025-09-03T09:55:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:56:06.084,[2025-09-03T09:56:06.084] _slurm_rpc_submit_batch_job: JobId=971 InitPrio=30011 usec=5011,job_submit_batch,info,scheduler,"job,submit,batch",22,[2025-09-03T09:56:06.084] _slurm_rpc_submit_batch_job: JobId=971 InitPrio=30011 usec=5011 [2025-09-03T09:56:,[2025-09-03T09:56:06.084] _slurm_rpc_submit_batch_job: JobId=971 InitPrio=30011 usec=5011,job_submit_batch
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:56:41.084,[2025-09-03T09:56:41.084] sched: Allocate JobId=972 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T09:56:41.084] sched: [2025-09-03T09:56:41.084] sched: [2025-09-03T09:56:41.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:57:16.084,[2025-09-03T09:57:16.084] POWER: no more nodes to resume for job JobId=973,cluster_power_save_complete,info,infra,"power,save,complete",12,[2025-09-03T09:57:16.084] POWER: no more nodes to resume for job [2025-09-03T09:57:16.084] POWER: no more nodes to resume for job.:0.:,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:57:51.084,[2025-09-03T09:57:51.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",13,_save: waking nodes_save: waking nodes_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4-c7i4xlarge-4--,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:58:26.084,[2025-09-03T09:58:26.084] Node cpu-8c-std-dy-c7i4xlarge-5 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-5 [2025-09-03T09:58:26.084].-c7i4xlarge-5-c7i4xlarge-5 [2025-09-03T09:58:26.084] [2025-09,<*> Node <*> now responding,node_now_responding
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:59:01.084,"[2025-09-03T09:59:01.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-1/cpu-8c-std-dy-c7i4xlarge-1/192.168.13.72 powered up with instance_id=i-0c30e29de39a40002, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"-1/cpu-8c-std-dy-c7i4xlarge-1/192.168.13.72, instance_type=c7i.4xlarge-1.]0-1/cpu-8c-std-dy-",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T09:59:36.084,[2025-09-03T09:59:36.084] AUTH: user user2 failed password authentication from 10.1.2.102,auth_failure,high,security,"auth,security,login_failure",19,[2025-09-03T09:59:36.084] AUTH: user user2 failed password authentication from 10.1.2.102. [2025-09-03T09:59:36.084] AUTH: user user2 failed password authentication from 10.1.2.102.,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:00:11.084,[2025-09-03T10:00:11.084] DISK: /var is 90% full on node cpu-16c-std-3,disk_usage_high,warning,storage,"disk,usage,high",20,: [2025-09-03T10:00:11.084] DISK: [2025-09-03T10:00:11.084] DISK:-std-3-node-node.]:,<*> DISK: /var is <*> full on node <*>,disk_usage_high
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:00:46.084,[2025-09-03T10:00:46.084] NETWORK: latency to storage cluster increased to 82ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T10:00:46.084] [2025-09-03T10:00:46.084] [2025-09-03T10:00:46.084] [2025-09-03T10:00:46.084] [2025-09-03,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:01:21.084,[2025-09-03T10:01:21.084] POWER: Power save mode: 333 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:01:21.084] POWER: Power save mode: 333 nodes: [2025-09-03T10:01:21.084] POWER: [2025-09-03T10:01:21.084] POWER:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:01:56.084,[2025-09-03T10:01:56.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:01:56.084] POWER: Power save mode: 334 nodes [2025-09-03T10:01:56.084] POWER: Power save mode: 334 nodes [2025-09-03T10:01:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:02:31.084,[2025-09-03T10:02:31.084] _slurm_rpc_submit_batch_job: JobId=981 InitPrio=30021 usec=5021,job_submit_batch,info,scheduler,"job,submit,batch",23,[2025-09-03T10:02:31.084] _slurm_rpc_submit_batch_job: JobId=981 InitPrio=30021 usec=5021 [2025-09-03T10:02:,[2025-09-03T10:02:31.084] _slurm_rpc_submit_batch_job: JobId=981 InitPrio=30021 usec=5021,job_submit_batch
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:03:06.084,[2025-09-03T10:03:06.084] sched: Allocate JobId=982 NodeList=cpu-8c-std-dy-c7i4xlarge-4 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T10:03:06.084] sched: [2025-09-03T10:03:06.084] [2025-09-03T10:03:06.084] sched: [2025-09-03T10:,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:03:41.084,[2025-09-03T10:03:41.084] POWER: no more nodes to resume for job JobId=983,cluster_power_save_complete,info,infra,"power,save,complete",12,[2025-09-03T10:03:41.084] POWER: no more nodes to resume for job [2025-09-03T10:03:41.084] POWER: no more nodes to resume for job:983:41.084,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:04:16.084,[2025-09-03T10:04:16.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-5,cluster_power_save_wake,info,infra,"power,save,wakeup",13,_save: waking nodes-save: waking nodes_save: waking nodes cpu-8c-std-dy-c7i4xlarge-5-c7i4x-c7i4,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:04:51.084,[2025-09-03T10:04:51.084] Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-1.-c7i4xlarge-1-c7i4xlarge-1 [2025-09-03T10:04:51.084] [2025-09-03T10:04:51.084] [2025-09,<*> Node <*> now responding,node_now_responding
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:05:26.084,"[2025-09-03T10:05:26.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-2/cpu-8c-std-dy-c7i4xlarge-2/192.168.13.73 powered up with instance_id=i-0c30e29de39a40003, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"-2/192.168.13.73-2, instance_name=c7i.4xlarge-2, instance_type=c7i.4xlarge-2, instance_id=i-0c30e29de39a40003, instance_type=c7i",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:06:01.084,[2025-09-03T10:06:01.084] AUTH: user user3 failed password authentication from 10.1.3.103,auth_failure,high,security,"auth,security,login_failure",19,[2025-09-03T10:06:01.084] AUTH: user user3 failed password authentication from 10.1.3.103. [2025-09-03T10:06:01.084] AUTH: user user3 failed password authentication from 10.1.3.103.,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:06:36.084,[2025-09-03T10:06:36.084] DISK: /var is 91% full on node cpu-16c-std-4,disk_usage_high,warning,storage,"disk,usage,high",20,:36.084] [2025-09-03T10:06:36.084] [2025-09-03T10:06:36.084] [2025-09-03T10:06:36.084] [2025-09-03T10:06:3,<*> DISK: /var is <*> full on node <*>,disk_usage_high
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:07:11.084,[2025-09-03T10:07:11.084] NETWORK: latency to storage cluster increased to 83ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T10:07:11.084] NETWORK: latency to storage cluster increased to 83ms (no SLA breach). [2025-09-03T10:07:11.084] NETWORK: latency to storage cluster increased,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:07:46.084,[2025-09-03T10:07:46.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:07:46.084] POWER: Power save mode: 334 nodes [2025-09-03T10:07:46.084] POWER: Power save mode: 334 nodes [2025-09-03T10:07:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:08:21.084,[2025-09-03T10:08:21.084] POWER: Power save mode: 335 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:08:21.084] POWER: Power save mode: 335 nodes: [2025-09-03T10:08:21.084] POWER: [2025-09-03T10:08:21.084] POWER:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:08:56.084,[2025-09-03T10:08:56.084] _slurm_rpc_submit_batch_job: JobId=991 InitPrio=30031 usec=5031,job_submit_batch,info,scheduler,"job,submit,batch",24,[2025-09-03T10:08:56.084] _slurm_rpc_submit_batch_job: JobId=991 InitPrio=30031 usec=5031 [2025-09-03T10:08:,[2025-09-03T10:08:56.084] _slurm_rpc_submit_batch_job: JobId=991 InitPrio=30031 usec=5031,job_submit_batch
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:09:31.084,[2025-09-03T10:09:31.084] sched: Allocate JobId=992 NodeList=cpu-8c-std-dy-c7i4xlarge-5 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T10:09:31.084] sched: [2025-09-03T10:09:31.084] sched: [2025-09-03T10:09:31.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:10:06.084,[2025-09-03T10:10:06.084] POWER: no more nodes to resume for job JobId=993,cluster_power_save_complete,info,infra,"power,save,complete",12,[2025-09-03T10:10:06.084] POWER: no more nodes to resume for job [2025-09-03T10:10:06.084] POWER: no more nodes to resume for job. POWER: no more no,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:10:41.084,[2025-09-03T10:10:41.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-1,cluster_power_save_wake,info,infra,"power,save,wakeup",13,cpu-8c-std-dy-c7i4xlarge-1 cpu-8c-std-dy-c7i4xlarge-1_save: waking nodes-c7i4xlarge-1-,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:11:16.084,[2025-09-03T10:11:16.084] Node cpu-8c-std-dy-c7i4xlarge-2 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-2 [2025-09-03T10:11:16.084]. [2025-09-03T10:11:16.084]-c7i4xlarge-2-c7i4xlarge-2 now responding-,<*> Node <*> now responding,node_now_responding
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:11:51.084,"[2025-09-03T10:11:51.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-3/cpu-8c-std-dy-c7i4xlarge-3/192.168.13.74 powered up with instance_id=i-0c30e29de39a40004, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"-c7i4xlarge-3, instance_id=i-0c30e29de39a40004, instance_type=c7i.4xlarge-3.-3, instance_type=c7i.4xlarge-3.00.0:5",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:12:26.084,[2025-09-03T10:12:26.084] AUTH: user user4 failed password authentication from 10.1.4.104,auth_failure,high,security,"auth,security,login_failure",19,[2025-09-03T10:12:26.084] AUTH: user user4 failed password authentication from 10.1.4.104.,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:13:01.084,[2025-09-03T10:13:01.084] DISK: /var is 92% full on node cpu-16c-std-1,disk_usage_high,warning,storage,"disk,usage,high",20,[2025-09-03T10:13:01.084] DISK: [2025-09-03T10:13:01.084] DISK: [2025-09-03T10:13:01.084] DISK: [2025-09-,<*> DISK: /var is <*> full on node <*>,disk_usage_high
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:13:36.084,[2025-09-03T10:13:36.084] NETWORK: latency to storage cluster increased to 84ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T10:13:36.084] NETWORK: latency to storage cluster increased to 84ms (no SLA breach) [2025-09-03T10:13:36.084] NETWORK: latency to storage cluster increased to,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:14:11.084,[2025-09-03T10:14:11.084] POWER: Power save mode: 333 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:14:11.084] POWER: Power save mode: 333 nodes [2025-09-03T10:14:11.084] POWER: Power save mode: 333 nodes [2025-09-03T10:14:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:14:46.084,[2025-09-03T10:14:46.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:14:46.084] POWER: Power save mode: 334 nodes [2025-09-03T10:14:46.084] POWER: Power save mode: 334 nodes [2025-09-03T10:14:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:15:21.084,[2025-09-03T10:15:21.084] _slurm_rpc_submit_batch_job: JobId=1001 InitPrio=30041 usec=5041,job_submit_batch,info,scheduler,"job,submit,batch",25,[2025-09-03T10:15:21.084] _slurm_rpc_submit_batch_job: JobId=1001 InitPrio=30041 usec=5041 [2025-09-03T10:15:,[2025-09-03T10:15:21.084] _slurm_rpc_submit_batch_job: JobId=1001 InitPrio=30041 usec=5041,job_submit_batch
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:15:56.084,[2025-09-03T10:15:56.084] sched: Allocate JobId=1002 NodeList=cpu-8c-std-dy-c7i4xlarge-1 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T10:15:56.084] sched: [2025-09-03T10:15:56.084] sched: [2025-09-03T10:15:56.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:16:31.084,[2025-09-03T10:16:31.084] POWER: no more nodes to resume for job JobId=1003,cluster_power_save_complete,info,infra,"power,save,complete",12,nodes to resume for job [2025-09-03T10:16:31.084] POWER: no more nodes to resume for job JobId=1003 [2025-09-03T10:16:31.084] POWER: no more nodes to,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:17:06.084,[2025-09-03T10:17:06.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-2,cluster_power_save_wake,info,infra,"power,save,wakeup",13,_save: waking nodes_save: waking nodes_save: waking nodes cpu-8c-std-dy-c7i4xlarge-2-c7i4xlarge-2-,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:17:41.084,[2025-09-03T10:17:41.084] Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-3 [2025-09-03T10:17:41.084]. [2025-09-03T10:17:41.084]-c7i4xlarge-3-c7i4xlarge-3 [2025-09,<*> Node <*> now responding,node_now_responding
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:18:16.084,"[2025-09-03T10:18:16.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-4/cpu-8c-std-dy-c7i4xlarge-4/192.168.13.75 powered up with instance_id=i-0c30e29de39a40005, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"_id=i-0c30e29de39a40005, instance_type=c7i.4xlarge-4.-4.00.-4/cpu-8c-std-dy-c7i4xlarge-4/192.1",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:18:51.084,[2025-09-03T10:18:51.084] AUTH: user user5 failed password authentication from 10.1.5.105,auth_failure,high,security,"auth,security,login_failure",19,failed password authentication from 10.1.5.105. [2025-09-03T10:18:51.084] AUTH: user user5 failed password authentication from 10.1.5.105. [2025-09-03T10:18:51.084] [2025-09-03T,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:19:26.084,[2025-09-03T10:19:26.084] DISK: /var is 93% full on node cpu-16c-std-2,disk_usage_high,warning,storage,"disk,usage,high",20,[2025-09-03T10:19:26.084] DISK: /var is 93% full on node cpu-16c-std-2. [2025-09-03T10:19:26.084] [2025-09-03T10,<*> DISK: /var is <*> full on node <*>,disk_usage_high
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:20:01.084,[2025-09-03T10:20:01.084] NETWORK: latency to storage cluster increased to 85ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T10:20:01.084] NETWORK: latency to storage cluster increased to 85ms (no SLA breach) [2025-09-03T10:20:01.084] NETWORK: latency to storage cluster increased to,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:20:36.084,[2025-09-03T10:20:36.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:20:36.084] POWER: Power save mode: 334 nodes [2025-09-03T10:20:36.084] POWER: Power save mode: 334 nodes [2025-09-03T10:20:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:21:11.084,[2025-09-03T10:21:11.084] POWER: Power save mode: 335 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:21:11.084] POWER: Power save mode: 335 nodes [2025-09-03T10:21:11.084] POWER: Power save mode: 335 nodes [2025-09-03T10:21:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:21:46.084,[2025-09-03T10:21:46.084] _slurm_rpc_submit_batch_job: JobId=1011 InitPrio=30051 usec=5051,job_submit_batch,info,scheduler,"job,submit,batch",26,[2025-09-03T10:21:46.084] _slurm_rpc_submit_batch_job: JobId=1011 InitPrio=30051 usec=5051 [2025-09-03T10:21:,[2025-09-03T10:21:46.084] _slurm_rpc_submit_batch_job: JobId=1011 InitPrio=30051 usec=5051,job_submit_batch
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:22:21.084,[2025-09-03T10:22:21.084] sched: Allocate JobId=1012 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T10:22:21.084] sched: [2025-09-03T10:22:21.084] sched: [2025-09-03T10:22:21.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:22:56.084,[2025-09-03T10:22:56.084] POWER: no more nodes to resume for job JobId=1013,cluster_power_save_complete,info,infra,"power,save,complete",12,nodes to resume for job [2025-09-03T10:22:56.084] POWER: no more nodes to resume for job [2025-09-03T10:22:56.084] POWER: no more nodes to resume for job:10,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:23:31.084,[2025-09-03T10:23:31.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-3,cluster_power_save_wake,info,infra,"power,save,wakeup",13,: waking nodes cpu-std-dy-c7i4xlarge-3_save: waking nodes-c7i4xlarge-3-c7i4--2025-09-03T10:,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:24:06.084,[2025-09-03T10:24:06.084] Node cpu-8c-std-dy-c7i4xlarge-4 now responding,node_now_responding,info,infra,"node,recovered,online",14,node cpu-8c-std-dy-c7i4xlarge-4 now responding..-c7i4xlarge-4-c7i4xlarge-4 [2025-09-03T10:24:06.084] [202,<*> Node <*> now responding,node_now_responding
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:24:41.084,"[2025-09-03T10:24:41.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-5/cpu-8c-std-dy-c7i4xlarge-5/192.168.13.76 powered up with instance_id=i-0c30e29de39a40006, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"_id=i-0c30e29de39a40006, instance_type=c7i.4xlarge-5.-5.41.084] [2025-09-03T10:24:41.084] [2025-09-03T10:",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:25:16.084,[2025-09-03T10:25:16.084] AUTH: user user6 failed password authentication from 10.1.6.106,auth_failure,high,security,"auth,security,login_failure",19,[2025-09-03T10:25:16.084] AUTH: user user6 failed password authentication from 10.1.6.106.,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:25:51.084,[2025-09-03T10:25:51.084] DISK: /var is 94% full on node cpu-16c-std-3,disk_usage_high,warning,storage,"disk,usage,high",20,[2025-09-03T10:25:51.084] DISK: [2025-09-03T10:25:51.084] [2025-09-03T10:25:51.084] [2025-09-03T10:25:51.0,<*> DISK: /var is <*> full on node <*>,disk_usage_high
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:26:26.084,[2025-09-03T10:26:26.084] NETWORK: latency to storage cluster increased to 86ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T10:26:26.084] NETWORK: latency to storage cluster increased to 86ms (no SLA breach) [2025-09-03T10:26:26.084] NETWORK: latency to storage cluster increased to,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:27:01.084,[2025-09-03T10:27:01.084] POWER: Power save mode: 333 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:27:01.084] POWER: Power save mode: 333 nodes [2025-09-03T10:27:01.084] POWER: Power save mode: 333 nodes [2025-09-03T10:27:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:27:36.084,[2025-09-03T10:27:36.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:27:36.084] POWER: Power save mode: 334 nodes [2025-09-03T10:27:36.084] POWER: Power save mode: 334 nodes [2025-09-03T10:27:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:28:11.084,[2025-09-03T10:28:11.084] _slurm_rpc_submit_batch_job: JobId=1021 InitPrio=30061 usec=5061,job_submit_batch,info,scheduler,"job,submit,batch",27,[2025-09-03T10:28:11.084] _slurm_rpc_submit_batch_job: JobId=1021 InitPrio=30061 usec=5061 [2025-09-03T10:28:,[2025-09-03T10:28:11.084] _slurm_rpc_submit_batch_job: JobId=1021 InitPrio=30061 usec=5061,job_submit_batch
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:28:46.084,[2025-09-03T10:28:46.084] sched: Allocate JobId=1022 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T10:28:46.084] sched: [2025-09-03T10:28:46.084] sched: [2025-09-03T10:28:46.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:29:21.084,[2025-09-03T10:29:21.084] POWER: no more nodes to resume for job JobId=1023,cluster_power_save_complete,info,infra,"power,save,complete",12,nodes to resume for job [2025-09-03T10:29:21.084] POWER: no more nodes to resume for job [2025-09-03T10:29:21.084] POWER: no more nodes to resume for job JobId=,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:29:56.084,[2025-09-03T10:29:56.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",13,: waking nodes cpu-std-dy-c7i4xlarge-4_save: waking nodes-c7i4xlarge-4 cpu-8c-std-dy-c7i4xlarge,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:30:31.084,[2025-09-03T10:30:31.084] Node cpu-8c-std-dy-c7i4xlarge-5 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-5 [2025-09-03T10:30:31.084].-c7i4xlarge-5-c7i4xlarge-5 [2025-09-03T10:30:31.084] [2025-09,<*> Node <*> now responding,node_now_responding
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:31:06.084,"[2025-09-03T10:31:06.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-1/cpu-8c-std-dy-c7i4xlarge-1/192.168.13.77 powered up with instance_id=i-0c30e29de39a40007, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"-1/cpu-8c-std-dy-c7i4xlarge-1/192.168.13.77, instance_type=c7i.4xlarge-1.000:00+0c30e29de39a40007, instance_",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:31:41.084,[2025-09-03T10:31:41.084] AUTH: user user0 failed password authentication from 10.1.0.107,auth_failure,high,security,"auth,security,login_failure",19,failed password authentication from 10.1.0.107. [2025-09-03T10:31:41.084] [2025-09-03T10:31:41.084] [2025-09-03T10:31:41.084] [2025-09-03T10,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:32:16.084,[2025-09-03T10:32:16.084] DISK: /var is 95% full on node cpu-16c-std-4,disk_usage_high,warning,storage,"disk,usage,high",20,[2025-09-03T10:32:16.084] DISK: [2025-09-03T10:32:16.084] [2025-09-03T10:32:16.084] [2025-09-03T10:32:16.0,<*> DISK: /var is <*> full on node <*>,disk_usage_high
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:32:51.084,[2025-09-03T10:32:51.084] NETWORK: latency to storage cluster increased to 87ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T10:32:51.084] [2025-09-03T10:32:51.084] [2025-09-03T10:32:51.084] [2025-09-03T10:32:51.084] [202,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:33:26.084,[2025-09-03T10:33:26.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:33:26.084] POWER: Power save mode: 334 nodes [2025-09-03T10:33:26.084] POWER: Power save mode: 334 nodes [2025-09-03T10:33:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:34:01.084,[2025-09-03T10:34:01.084] POWER: Power save mode: 335 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:34:01.084] POWER: Power save mode: 335 nodes [2025-09-03T10:34:01.084] POWER: Power save mode: 335 nodes [2025-09-03T10:34:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:34:36.084,[2025-09-03T10:34:36.084] _slurm_rpc_submit_batch_job: JobId=1031 InitPrio=30071 usec=5071,job_submit_batch,info,scheduler,"job,submit,batch",28,[2025-09-03T10:34:36.084] _slurm_rpc_submit_batch_job: JobId=1031 InitPrio=30071 usec=5071 [2025-09-03T10:34:,[2025-09-03T10:34:36.084] _slurm_rpc_submit_batch_job: JobId=1031 InitPrio=30071 usec=5071,job_submit_batch
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:35:11.084,[2025-09-03T10:35:11.084] sched: Allocate JobId=1032 NodeList=cpu-8c-std-dy-c7i4xlarge-4 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T10:35:11.084] sched: [2025-09-03T10:35:11.084] sched: [2025-09-03T10:35:11.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:35:46.084,[2025-09-03T10:35:46.084] POWER: no more nodes to resume for job JobId=1033,cluster_power_save_complete,info,infra,"power,save,complete",12,[2025-09-03T10:35:46.084] POWER: no more nodes to resume for job JobId=1033 [2025-09-03T10:35:46.084] POWER: no more nodes to resume for job [2025-0,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:36:21.084,[2025-09-03T10:36:21.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-5,cluster_power_save_wake,info,infra,"power,save,wakeup",13,_save: waking nodes-save: waking nodes_save: waking nodes cpu-8c-std-dy-c7i4xlarge-5-c7i4x-c7i4,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:36:56.084,[2025-09-03T10:36:56.084] Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-1 now responding.. [2025-09-03T10:36:56.084]-c7i4xlarge-1-c7i4xlarge-1 now responding. [2025-09-03T10:36:56.0,<*> Node <*> now responding,node_now_responding
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:37:31.084,"[2025-09-03T10:37:31.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-2/cpu-8c-std-dy-c7i4xlarge-2/192.168.13.78 powered up with instance_id=i-0c30e29de39a40008, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"-c7i4xlarge-2/192.168.13.78-2, instance_type=c7i.4xlarge-2, instance_id=i-0c30e29de39a40008, instance_type=c7i.4xlarge-2.",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:38:06.084,[2025-09-03T10:38:06.084] AUTH: user user1 failed password authentication from 10.1.1.108,auth_failure,high,security,"auth,security,login_failure",19,[2025-09-03T10:38:06.084] AUTH: user user1 failed password authentication from 10.1.1.108 from 10.1.1.108. [2025-09-03T10:38:06.084] AUTH: user user1 failed password authentication from 10.,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:38:41.084,[2025-09-03T10:38:41.084] DISK: /var is 96% full on node cpu-16c-std-1,disk_usage_high,warning,storage,"disk,usage,high",20,[2025-09-03T10:38:41.084] [2025-09-03T10:38:41.084] [2025-09-03T10:38:41.084] [2025-09-03T10:38:41.084] [202,<*> DISK: /var is <*> full on node <*>,disk_usage_high
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:39:16.084,[2025-09-03T10:39:16.084] NETWORK: latency to storage cluster increased to 88ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T10:39:16.084] NETWORK: latency to storage cluster increased to 88ms (no SLA breach). [2025-09-03T10:39:16.084] NETWORK: latency to storage cluster increased,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:39:51.084,[2025-09-03T10:39:51.084] POWER: Power save mode: 333 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:39:51.084] POWER: Power save mode: 333 nodes [2025-09-03T10:39:51.084] POWER: Power save mode: 333 nodes [2025-09-03T10:39:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:40:26.084,[2025-09-03T10:40:26.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:40:26.084] POWER: Power save mode: 334 nodes [2025-09-03T10:40:26.084] POWER: Power save mode: 334 nodes [2025-09-03T10:40:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:41:01.084,[2025-09-03T10:41:01.084] _slurm_rpc_submit_batch_job: JobId=1041 InitPrio=30081 usec=5081,job_submit_batch,info,scheduler,"job,submit,batch",29,[2025-09-03T10:41:01.084] _slurm_rpc_submit_batch_job: JobId=1041 InitPrio=30081 usec=5081 [2025-09-03T10:41:,[2025-09-03T10:41:01.084] _slurm_rpc_submit_batch_job: JobId=1041 InitPrio=30081 usec=5081,job_submit_batch
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:41:36.084,[2025-09-03T10:41:36.084] sched: Allocate JobId=1042 NodeList=cpu-8c-std-dy-c7i4xlarge-5 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T10:41:36.084] sched: [2025-09-03T10:41:36.084] sched: [2025-09-03T10:41:36.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:42:11.084,[2025-09-03T10:42:11.084] POWER: no more nodes to resume for job JobId=1043,cluster_power_save_complete,info,infra,"power,save,complete",12,[2025-09-03T10:42:11.084] POWER: no more nodes to resume for job JobId=1043 JobId=1043 JobId=1043 JobId=1043:10 POWER: no more nodes,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:42:46.084,[2025-09-03T10:42:46.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-1,cluster_power_save_wake,info,infra,"power,save,wakeup",13,cpu_save: waking nodes cpu-8c-std-dy-c7i4xlarge-1_save: waking nodes_save: waking nodes-c7i4xlarge-1,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:43:21.084,[2025-09-03T10:43:21.084] Node cpu-8c-std-dy-c7i4xlarge-2 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-2 [2025-09-03T10:43:21.084]. [2025-09-03T10:43:21.084]-c7i4xlarge-2-c7i4xlarge-2 now responding. [,<*> Node <*> now responding,node_now_responding
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:43:56.084,"[2025-09-03T10:43:56.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-3/cpu-8c-std-dy-c7i4xlarge-3/192.168.13.79 powered up with instance_id=i-0c30e29de39a40009, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"-3/192.168.13.79-3, instance_type=c7i.4xlarge-3, instance_id=i-0c30e29de39a40009, instance_type=c7i.4xlarge-3.0.0.",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:44:31.084,[2025-09-03T10:44:31.084] AUTH: user user2 failed password authentication from 10.1.2.109,auth_failure,high,security,"auth,security,login_failure",19,[2025-09-03T10:44:31.084] AUTH: user user2 failed password authentication from 10.1.2.109 from 10.1.2.109. [2025-09-03T10:44:31.084] AUTH: user user2 failed password authentication from 10.,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:45:06.084,[2025-09-03T10:45:06.084] DISK: /var is 97% full on node cpu-16c-std-2,disk_usage_high,warning,storage,"disk,usage,high",20,DISK: [2025-09-03T10:45:06.084] DISK: [2025-09-03T10:45:06.084] [2025-09-03T10:45:06.084] [2025-09-03T10:,<*> DISK: /var is <*> full on node <*>,disk_usage_high
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:45:41.084,[2025-09-03T10:45:41.084] NETWORK: latency to storage cluster increased to 89ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T10:45:41.084] NETWORK: latency to storage cluster increased to 89ms (no SLA breach) [2025-09-03T10:45:41.084] NETWORK: latency to storage cluster increased,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:46:16.084,[2025-09-03T10:46:16.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:46:16.084] POWER: Power save mode: 334 nodes [2025-09-03T10:46:16.084] POWER: Power save mode: 334 nodes [2025-09-03T10:46:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:46:51.084,[2025-09-03T10:46:51.084] POWER: Power save mode: 335 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:46:51.084] POWER: Power save mode: 335 nodes [2025-09-03T10:46:51.084] POWER: Power save mode: 335 nodes [2025-09-03T10:46:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:47:26.084,[2025-09-03T10:47:26.084] _slurm_rpc_submit_batch_job: JobId=1051 InitPrio=30091 usec=5091,job_submit_batch,info,scheduler,"job,submit,batch",30,[2025-09-03T10:47:26.084] _slurm_rpc_submit_batch_job: JobId=1051 InitPrio=30091 usec=5091 [2025-09-03T10:47:,[2025-09-03T10:47:26.084] _slurm_rpc_submit_batch_job: JobId=1051 InitPrio=30091 usec=5091,job_submit_batch
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:48:01.084,[2025-09-03T10:48:01.084] sched: Allocate JobId=1052 NodeList=cpu-8c-std-dy-c7i4xlarge-1 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T10:48:01.084] sched: [2025-09-03T10:48:01.084] sched: [2025-09-03T10:48:01.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:48:36.084,[2025-09-03T10:48:36.084] POWER: no more nodes to resume for job JobId=1053,cluster_power_save_complete,info,infra,"power,save,complete",12,[2025-09-03T10:48:36.084] POWER: no more nodes to resume for job [2025-09-03T10:48:36.084] POWER: no more nodes to resume for job10:36.084],<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:49:11.084,[2025-09-03T10:49:11.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-2,cluster_power_save_wake,info,infra,"power,save,wakeup",13,_save: waking nodes_save: waking nodes_save: waking nodes-c7i4xlarge-2-c7i4xlarge-2 cpu-8c-std-dy-c,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:49:46.084,[2025-09-03T10:49:46.084] Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-3 [2025-09-03T10:49:46.084].-c7i4xlarge-3-c7i4xlarge-3 [2025-09-03T10:49:46.084] [2025-09,<*> Node <*> now responding,node_now_responding
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:50:21.084,"[2025-09-03T10:50:21.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-4/cpu-8c-std-dy-c7i4xlarge-4/192.168.13.80 powered up with instance_id=i-0c30e29de39a40010, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"-c7i4xlarge-4, instance_type=c7i.4xlarge-4.-4, instance_type=c7i.4xlarge-4. [2025-09-03T10:50:21.084] POWER: Node00",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:50:56.084,[2025-09-03T10:50:56.084] AUTH: user user3 failed password authentication from 10.1.3.110,auth_failure,high,security,"auth,security,login_failure",19,[2025-09-03T10:50:56.084] AUTH: user user3 failed password authentication from 10.1.3.110 from 10.1.3.110. [2025-09-03T10:50:56.084] AUTH: user user3 failed password authentication from 10.,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:51:31.084,[2025-09-03T10:51:31.084] DISK: /var is 98% full on node cpu-16c-std-3,disk_usage_high,warning,storage,"disk,usage,high",20,[2025-09-03T10:51:31.084] [2025-09-03T10:51:31.084] [2025-09-03T10:51:31.084] [2025-09-03T10:51:31.084] [202,<*> DISK: /var is <*> full on node <*>,disk_usage_high
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:52:06.084,[2025-09-03T10:52:06.084] NETWORK: latency to storage cluster increased to 90ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T10:52:06.084] NETWORK: latency to storage cluster increased to 90ms (no SLA breach) [2025-09-03T10:52:06.084] NETWORK: latency to storage cluster increased to,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:52:41.084,[2025-09-03T10:52:41.084] POWER: Power save mode: 333 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:52:41.084] POWER: Power save mode: 333 nodes [2025-09-03T10:52:41.084] POWER: Power save mode: 333 nodes [2025-09-03T10:52:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:53:16.084,[2025-09-03T10:53:16.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:53:16.084] POWER: Power save mode: 334 nodes [2025-09-03T10:53:16.084] POWER: Power save mode: 334 nodes [2025-09-03T10:53:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:53:51.084,[2025-09-03T10:53:51.084] _slurm_rpc_submit_batch_job: JobId=1061 InitPrio=30101 usec=5101,job_submit_batch,info,scheduler,"job,submit,batch",31,[2025-09-03T10:53:51.084] _slurm_rpc_submit_batch_job: [2025-09-03T10:53:51.084] _slurm_rpc_submit_,[2025-09-03T10:53:51.084] _slurm_rpc_submit_batch_job: JobId=1061 InitPrio=30101 usec=5101,job_submit_batch
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:54:26.084,[2025-09-03T10:54:26.084] sched: Allocate JobId=1062 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T10:54:26.084] sched: [2025-09-03T10:54:26.084] sched: [2025-09-03T10:54:26.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:55:01.084,[2025-09-03T10:55:01.084] POWER: no more nodes to resume for job JobId=1063,cluster_power_save_complete,info,infra,"power,save,complete",12,[2025-09-03T10:55:01.084] POWER: no more nodes to resume for job [2025-09-03T10:55:01.084] POWER: no more nodes to resume for job: POWER: no more no,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:55:36.084,[2025-09-03T10:55:36.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-3,cluster_power_save_wake,info,infra,"power,save,wakeup",13,_save: waking nodes_save: waking nodes_save: waking nodes-c7i4xlarge-3-c7i4-c7i4xlarge-3-c7i4 POWER,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:56:11.084,[2025-09-03T10:56:11.084] Node cpu-8c-std-dy-c7i4xlarge-4 now responding,node_now_responding,info,infra,"node,recovered,online",14,node cpu-8c-std-dy-c7i4xlarge-4 now responding..-c7i4xlarge-4-c7i4xlarge-4 [2025-09-03T10:56:11.084] [202,<*> Node <*> now responding,node_now_responding
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:56:46.084,"[2025-09-03T10:56:46.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-5/cpu-8c-std-dy-c7i4xlarge-5/192.168.13.81 powered up with instance_id=i-0c30e29de39a40011, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"_id=i-0c30e29de39a40011, instance_type=c7i.4xlarge-5.-5, instance_type=c7i.4xlarge-5.46.084] [2025-09-03T10:56:",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:57:21.084,[2025-09-03T10:57:21.084] AUTH: user user4 failed password authentication from 10.1.4.111,auth_failure,high,security,"auth,security,login_failure",19,[2025-09-03T10:57:21.084] AUTH: user user4 failed password authentication from 10.1.4.111. [2025-09-03T10:57:21.084] AUTH: user user4 failed password authentication from 10.1.4.111.,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:57:56.084,[2025-09-03T10:57:56.084] DISK: /var is 99% full on node cpu-16c-std-4,disk_usage_high,warning,storage,"disk,usage,high",20,[2025-09-03T10:57:56.084] DISK: [2025-09-03T10:57:56.084] [2025-09-03T10:57:56.084] [2025-09-03T10:57:56.0,<*> DISK: /var is <*> full on node <*>,disk_usage_high
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:58:31.084,[2025-09-03T10:58:31.084] NETWORK: latency to storage cluster increased to 91ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T10:58:31.084] NETWORK: latency to storage cluster increased to 91ms (no SLA breach). [2025-09-03T10:58:31.084] NETWORK: latency to storage cluster increased,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:59:06.084,[2025-09-03T10:59:06.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:59:06.084] POWER: Power save mode: 334 nodes [2025-09-03T10:59:06.084] POWER: Power save mode: 334 nodes [2025-09-03T10:59:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T10:59:41.084,[2025-09-03T10:59:41.084] POWER: Power save mode: 335 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:59:41.084] POWER: Power save mode: 335 nodes [2025-09-03T10:59:41.084] POWER: Power save mode: 335 nodes [2025-09-03T10:59:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T11:00:16.084,[2025-09-03T11:00:16.084] _slurm_rpc_submit_batch_job: JobId=1071 InitPrio=30111 usec=5111,job_submit_batch,info,scheduler,"job,submit,batch",32,[2025-09-03T11:00:16.084] _slurm_rpc_submit_batch_job: JobId=1071 InitPrio=30111 usec=5111 [2025-09-03T11:00:16.0,[2025-09-03T11:00:16.084] _slurm_rpc_submit_batch_job: JobId=1071 InitPrio=30111 usec=5111,job_submit_batch
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T11:00:51.084,[2025-09-03T11:00:51.084] sched: Allocate JobId=1072 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T11:00:51.084] sched: [2025-09-03T11:00:51.084] sched: [2025-09-03T11:00:51.084] sched: [2025-09-03T1,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T11:01:26.084,[2025-09-03T11:01:26.084] POWER: no more nodes to resume for job JobId=1073,cluster_power_save_complete,info,infra,"power,save,complete",12,[2025-09-03T11:01:26.084] [2025-09-03T11:01:26.084] [2025-09-03T11:01:26.084] [2025-09-03T11:01:26.084] [202,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T11:02:01.084,[2025-09-03T11:02:01.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",13,power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4_save: waking nodes cpu-std-dy-c7i4xlarge-4-,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T11:02:36.084,[2025-09-03T11:02:36.084] Node cpu-8c-std-dy-c7i4xlarge-5 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-5 [2025-09-03T11:02:36.084].-c7i4xlarge-5-c7i4xlarge-5 [2025-09-03T11:02:36.084] [2025-09,<*> Node <*> now responding,node_now_responding
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T11:03:11.084,"[2025-09-03T11:03:11.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-1/cpu-8c-std-dy-c7i4xlarge-1/192.168.13.82 powered up with instance_id=i-0c30e29de39a40012, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"-std-dy-c7i4xlarge-1/192.168.13.82-1, instance_type=c7i.4xlarge-1.0] [2025-09-03T11:03:11.084] [2025-09-03",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T11:03:46.084,[2025-09-03T11:03:46.084] AUTH: user user5 failed password authentication from 10.1.5.112,auth_failure,high,security,"auth,security,login_failure",19,[2025-09-03T11:03:46.084] AUTH: user user5 failed password authentication from 10.1.5.112 user user5 failed password authentication from 10.1.5.112 user user5 failed password authentication from 10.1.5.112 user user5 failed password authentication from,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T11:04:21.084,[2025-09-03T11:04:21.084] DISK: /var is 88% full on node cpu-16c-std-1,disk_usage_high,warning,storage,"disk,usage,high",20,[2025-09-03T11:04:21.084] DISK: /var is 88% full on node cpu-16c-std-1. [2025-09-03T11:04:21.084] DISK: /var is,<*> DISK: /var is <*> full on node <*>,disk_usage_high
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T11:04:56.084,[2025-09-03T11:04:56.084] NETWORK: latency to storage cluster increased to 92ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T11:04:56.084] NETWORK: latency to storage cluster increased to 92ms (no SLA breach). [2025-09-03T11:04:56.084] NETWORK: latency to storage cluster increased,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T11:05:31.084,[2025-09-03T11:05:31.084] POWER: Power save mode: 333 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T11:05:31.084] POWER: Power save mode: 333 nodes: [2025-09-03T11:05:31.084] POWER: [2025-09-03T11:05:31.084] POWER:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T11:06:06.084,[2025-09-03T11:06:06.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T11:06:06.084] POWER: Power save mode: 334 nodes [2025-09-03T11:06:06.084] POWER: Power save mode: 334 nodes [2025-09-03T11:06:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T11:06:41.084,[2025-09-03T11:06:41.084] _slurm_rpc_submit_batch_job: JobId=1081 InitPrio=30121 usec=5121,job_submit_batch,info,scheduler,"job,submit,batch",33,[2025-09-03T11:06:41.084] _slurm_rpc_submit_batch_job: [2025-09-03T11:06:41.084] _slurm_rpc_submit_,[2025-09-03T11:06:41.084] _slurm_rpc_submit_batch_job: JobId=1081 InitPrio=30121 usec=5121,job_submit_batch
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T11:07:16.084,[2025-09-03T11:07:16.084] sched: Allocate JobId=1082 NodeList=cpu-8c-std-dy-c7i4xlarge-4 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T11:07:16.084] sched: [2025-09-03T11:07:16.084] sched: [2025-09-03T11:07:16.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T11:07:51.084,[2025-09-03T11:07:51.084] POWER: no more nodes to resume for job JobId=1083,cluster_power_save_complete,info,infra,"power,save,complete",12,POWER: no more nodes to resume for job [2025-09-03T11:07:51.084] POWER: no more nodes to resume for job JobId=1083 [2025-09-03T11:07:51.084] POWER:,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T11:08:26.084,[2025-09-03T11:08:26.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-5,cluster_power_save_wake,info,infra,"power,save,wakeup",13,_save: waking nodes_save: waking nodes_save: waking nodes cpu-8c-std-dy-c7i4xlarge-5-c7i4x- cpu,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T11:09:01.084,[2025-09-03T11:09:01.084] Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",14,cpu 8c-std-dy-c7i4xlarge-1 now responding. [2025-09-03T11:09:01.084]..-c7i4xlarge-1 now responding. [2025-09-03T11,<*> Node <*> now responding,node_now_responding
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T11:09:36.084,"[2025-09-03T11:09:36.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-2/cpu-8c-std-dy-c7i4xlarge-2/192.168.13.83 powered up with instance_id=i-0c30e29de39a40013, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"-2/192.168.13.83-2, instance_type=c7i.4xlarge-2.00-2/cpu-8c-std-dy-c7i4xlarge-2/192.168.13.83-2.168",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T11:10:11.084,[2025-09-03T11:10:11.084] AUTH: user user6 failed password authentication from 10.1.6.113,auth_failure,high,security,"auth,security,login_failure",19,[2025-09-03T11:10:11.084] AUTH: user user6 failed password authentication from 10.1.6.113. [2025-09-03T11:10:11.084] AUTH: user user6 failed password authentication from 10.1.6.113.,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T11:10:46.084,[2025-09-03T11:10:46.084] DISK: /var is 89% full on node cpu-16c-std-2,disk_usage_high,warning,storage,"disk,usage,high",20,[2025-09-03T11:10:46.084] DISK: /var is 89% full on node cpu-16c-std-2. [2025-09-03T11:10:46.084] DISK: /var is,<*> DISK: /var is <*> full on node <*>,disk_usage_high
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T11:11:21.084,[2025-09-03T11:11:21.084] NETWORK: latency to storage cluster increased to 93ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T11:11:21.084] NETWORK: latency to storage cluster increased to 93ms (no SLA breach) [2025-09-03T11:11:21.084] NETWORK: latency to storage cluster increased,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T11:11:56.084,[2025-09-03T11:11:56.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T11:11:56.084] POWER: Power save mode: 334 nodes [2025-09-03T11:11:56.084] POWER: Power save mode: 334 nodes [2025-09-03T11:11:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T11:12:31.084,[2025-09-03T11:12:31.084] POWER: Power save mode: 335 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T11:12:31.084] POWER: Power save mode: 335 nodes [2025-09-03T11:12:31.084] POWER: Power save mode: 335 nodes [2025-09-03T11:12:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T11:13:06.084,[2025-09-03T11:13:06.084] _slurm_rpc_submit_batch_job: JobId=1091 InitPrio=30131 usec=5131,job_submit_batch,info,scheduler,"job,submit,batch",34,[2025-09-03T11:13:06.084] _slurm_rpc_submit_batch_job: JobId=1091 InitPrio=30131 usec=5131 [2025-09-03T11:13:,[2025-09-03T11:13:06.084] _slurm_rpc_submit_batch_job: JobId=1091 InitPrio=30131 usec=5131,job_submit_batch
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T11:13:41.084,[2025-09-03T11:13:41.084] sched: Allocate JobId=1092 NodeList=cpu-8c-std-dy-c7i4xlarge-5 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T11:13:41.084] sched: [2025-09-03T11:13:41.084] sched: [2025-09-03T11:13:41.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T11:14:16.084,[2025-09-03T11:14:16.084] POWER: no more nodes to resume for job JobId=1093,cluster_power_save_complete,info,infra,"power,save,complete",12,POWER: no more nodes to resume for job [2025-09-03T11:14:16.084] POWER: no more nodes to resume for job JobId=1093 [2025-09-03T11:14:16.084] POWER:,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T11:14:51.084,[2025-09-03T11:14:51.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-1,cluster_power_save_wake,info,infra,"power,save,wakeup",13,cpu-8c-std-dy-c7i4xlarge-1 cpu-8c-std-dy-c7i4xlarge-1_save: waking nodes-- cpu--,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
2469179c-5e4b-4825-b07c-87aede019ac2,2025-12-10T23:55:52.920906,test2.txt,both,2025-09-03T11:15:21.084,[2025-09-03T11:15:21.084] Node cpu-8c-std-dy-c7i4xlarge-2 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-2 now responding. [2025-09-03T11:15:21.084]. [2025-09-03T11:15:21.084]-c7i4xlarge-2 now responding.-c7i4x,<*> Node <*> now responding,node_now_responding
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T10:58:31.084,[2025-09-03T10:58:31.084] NETWORK: latency to storage cluster increased to 91ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T10:58:31.084] NETWORK: latency to storage cluster increased to 91ms (no SLA breach). [2025-09-03T10:58:31.084] NETWORK: latency to storage cluster increased,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T10:59:06.084,[2025-09-03T10:59:06.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:59:06.084] POWER: Power save mode: 334 nodes [2025-09-03T10:59:06.084] POWER: Power save mode: 334 nodes [2025-09-03T10:59:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T10:59:41.084,[2025-09-03T10:59:41.084] POWER: Power save mode: 335 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T10:59:41.084] POWER: Power save mode: 335 nodes [2025-09-03T10:59:41.084] POWER: Power save mode: 335 nodes [2025-09-03T10:59:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T11:00:16.084,[2025-09-03T11:00:16.084] _slurm_rpc_submit_batch_job: JobId=1071 InitPrio=30111 usec=5111,job_submit_batch,info,scheduler,"job,submit,batch",32,[2025-09-03T11:00:16.084] _slurm_rpc_submit_batch_job: JobId=1071 InitPrio=30111 usec=5111 [2025-09-03T11:00:16.0,[2025-09-03T11:00:16.084] _slurm_rpc_submit_batch_job: JobId=1071 InitPrio=30111 usec=5111,job_submit_batch
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T11:00:51.084,[2025-09-03T11:00:51.084] sched: Allocate JobId=1072 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T11:00:51.084] sched: [2025-09-03T11:00:51.084] sched: [2025-09-03T11:00:51.084] sched: [2025-09-03T1,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T11:01:26.084,[2025-09-03T11:01:26.084] POWER: no more nodes to resume for job JobId=1073,cluster_power_save_complete,info,infra,"power,save,complete",12,[2025-09-03T11:01:26.084] [2025-09-03T11:01:26.084] [2025-09-03T11:01:26.084] [2025-09-03T11:01:26.084] [202,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T11:02:01.084,[2025-09-03T11:02:01.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",13,power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4_save: waking nodes cpu-std-dy-c7i4xlarge-4-,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T11:02:36.084,[2025-09-03T11:02:36.084] Node cpu-8c-std-dy-c7i4xlarge-5 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-5 [2025-09-03T11:02:36.084].-c7i4xlarge-5-c7i4xlarge-5 [2025-09-03T11:02:36.084] [2025-09,<*> Node <*> now responding,node_now_responding
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T11:03:11.084,"[2025-09-03T11:03:11.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-1/cpu-8c-std-dy-c7i4xlarge-1/192.168.13.82 powered up with instance_id=i-0c30e29de39a40012, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"-std-dy-c7i4xlarge-1/192.168.13.82-1, instance_type=c7i.4xlarge-1.0] [2025-09-03T11:03:11.084] [2025-09-03",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T11:03:46.084,[2025-09-03T11:03:46.084] AUTH: user user5 failed password authentication from 10.1.5.112,auth_failure,high,security,"auth,security,login_failure",19,[2025-09-03T11:03:46.084] AUTH: user user5 failed password authentication from 10.1.5.112 user user5 failed password authentication from 10.1.5.112 user user5 failed password authentication from 10.1.5.112 user user5 failed password authentication from,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T11:04:21.084,[2025-09-03T11:04:21.084] DISK: /var is 88% full on node cpu-16c-std-1,disk_usage_high,warning,storage,"disk,usage,high",20,[2025-09-03T11:04:21.084] DISK: /var is 88% full on node cpu-16c-std-1. [2025-09-03T11:04:21.084] DISK: /var is,<*> DISK: /var is <*> full on node <*>,disk_usage_high
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T11:04:56.084,[2025-09-03T11:04:56.084] NETWORK: latency to storage cluster increased to 92ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T11:04:56.084] NETWORK: latency to storage cluster increased to 92ms (no SLA breach). [2025-09-03T11:04:56.084] NETWORK: latency to storage cluster increased,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T11:05:31.084,[2025-09-03T11:05:31.084] POWER: Power save mode: 333 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T11:05:31.084] POWER: Power save mode: 333 nodes: [2025-09-03T11:05:31.084] POWER: [2025-09-03T11:05:31.084] POWER:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T11:06:06.084,[2025-09-03T11:06:06.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T11:06:06.084] POWER: Power save mode: 334 nodes [2025-09-03T11:06:06.084] POWER: Power save mode: 334 nodes [2025-09-03T11:06:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T11:06:41.084,[2025-09-03T11:06:41.084] _slurm_rpc_submit_batch_job: JobId=1081 InitPrio=30121 usec=5121,job_submit_batch,info,scheduler,"job,submit,batch",33,[2025-09-03T11:06:41.084] _slurm_rpc_submit_batch_job: [2025-09-03T11:06:41.084] _slurm_rpc_submit_,[2025-09-03T11:06:41.084] _slurm_rpc_submit_batch_job: JobId=1081 InitPrio=30121 usec=5121,job_submit_batch
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T11:07:16.084,[2025-09-03T11:07:16.084] sched: Allocate JobId=1082 NodeList=cpu-8c-std-dy-c7i4xlarge-4 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T11:07:16.084] sched: [2025-09-03T11:07:16.084] sched: [2025-09-03T11:07:16.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T11:07:51.084,[2025-09-03T11:07:51.084] POWER: no more nodes to resume for job JobId=1083,cluster_power_save_complete,info,infra,"power,save,complete",12,POWER: no more nodes to resume for job [2025-09-03T11:07:51.084] POWER: no more nodes to resume for job JobId=1083 [2025-09-03T11:07:51.084] POWER:,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T11:08:26.084,[2025-09-03T11:08:26.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-5,cluster_power_save_wake,info,infra,"power,save,wakeup",13,_save: waking nodes_save: waking nodes_save: waking nodes cpu-8c-std-dy-c7i4xlarge-5-c7i4x- cpu,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T11:09:01.084,[2025-09-03T11:09:01.084] Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",14,cpu 8c-std-dy-c7i4xlarge-1 now responding. [2025-09-03T11:09:01.084]..-c7i4xlarge-1 now responding. [2025-09-03T11,<*> Node <*> now responding,node_now_responding
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T11:09:36.084,"[2025-09-03T11:09:36.084] POWER: Node cpu-8c-std-dy-c7i4xlarge-2/cpu-8c-std-dy-c7i4xlarge-2/192.168.13.83 powered up with instance_id=i-0c30e29de39a40013, instance_type=c7i.4xlarge",cluster_power_node_online,info,infra,"power,node,online",15,"-2/192.168.13.83-2, instance_type=c7i.4xlarge-2.00-2/cpu-8c-std-dy-c7i4xlarge-2/192.168.13.83-2.168",<*> POWER: Node <*> powered up with <*> instance_type=c7i.4xlarge,cluster_power_node_online
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T11:10:11.084,[2025-09-03T11:10:11.084] AUTH: user user6 failed password authentication from 10.1.6.113,auth_failure,high,security,"auth,security,login_failure",19,[2025-09-03T11:10:11.084] AUTH: user user6 failed password authentication from 10.1.6.113. [2025-09-03T11:10:11.084] AUTH: user user6 failed password authentication from 10.1.6.113.,<*> AUTH: user <*> failed password authentication from <*>,auth_failure
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T11:10:46.084,[2025-09-03T11:10:46.084] DISK: /var is 89% full on node cpu-16c-std-2,disk_usage_high,warning,storage,"disk,usage,high",20,[2025-09-03T11:10:46.084] DISK: /var is 89% full on node cpu-16c-std-2. [2025-09-03T11:10:46.084] DISK: /var is,<*> DISK: /var is <*> full on node <*>,disk_usage_high
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T11:11:21.084,[2025-09-03T11:11:21.084] NETWORK: latency to storage cluster increased to 93ms (no SLA breach),network_latency_storage_cluster,warning,network,"network,latency,storage",21,[2025-09-03T11:11:21.084] NETWORK: latency to storage cluster increased to 93ms (no SLA breach) [2025-09-03T11:11:21.084] NETWORK: latency to storage cluster increased,<*> NETWORK: latency to storage cluster increased to <*> (no SLA breach),network_latency_storage_cluster
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T11:11:56.084,[2025-09-03T11:11:56.084] POWER: Power save mode: 334 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T11:11:56.084] POWER: Power save mode: 334 nodes [2025-09-03T11:11:56.084] POWER: Power save mode: 334 nodes [2025-09-03T11:11:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T11:12:31.084,[2025-09-03T11:12:31.084] POWER: Power save mode: 335 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",9,[2025-09-03T11:12:31.084] POWER: Power save mode: 335 nodes [2025-09-03T11:12:31.084] POWER: Power save mode: 335 nodes [2025-09-03T11:12:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T11:13:06.084,[2025-09-03T11:13:06.084] _slurm_rpc_submit_batch_job: JobId=1091 InitPrio=30131 usec=5131,job_submit_batch,info,scheduler,"job,submit,batch",34,[2025-09-03T11:13:06.084] _slurm_rpc_submit_batch_job: JobId=1091 InitPrio=30131 usec=5131 [2025-09-03T11:13:,[2025-09-03T11:13:06.084] _slurm_rpc_submit_batch_job: JobId=1091 InitPrio=30131 usec=5131,job_submit_batch
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T11:13:41.084,[2025-09-03T11:13:41.084] sched: Allocate JobId=1092 NodeList=cpu-8c-std-dy-c7i4xlarge-5 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",11,[2025-09-03T11:13:41.084] sched: [2025-09-03T11:13:41.084] sched: [2025-09-03T11:13:41.084] sched: [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T11:14:16.084,[2025-09-03T11:14:16.084] POWER: no more nodes to resume for job JobId=1093,cluster_power_save_complete,info,infra,"power,save,complete",12,POWER: no more nodes to resume for job [2025-09-03T11:14:16.084] POWER: no more nodes to resume for job JobId=1093 [2025-09-03T11:14:16.084] POWER:,<*> POWER: no more nodes to resume for job <*>,cluster_power_save_complete
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T11:14:51.084,[2025-09-03T11:14:51.084] POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-1,cluster_power_save_wake,info,infra,"power,save,wakeup",13,cpu-8c-std-dy-c7i4xlarge-1 cpu-8c-std-dy-c7i4xlarge-1_save: waking nodes-- cpu--,<*> POWER: power_save: waking nodes <*>,cluster_power_save_wake
0b2986fb-516f-44f1-a5bc-aa144530d240,2025-12-10T23:56:43.773489,test3.txt,both,2025-09-03T11:15:21.084,[2025-09-03T11:15:21.084] Node cpu-8c-std-dy-c7i4xlarge-2 now responding,node_now_responding,info,infra,"node,recovered,online",14,-c7i4xlarge-2 now responding. [2025-09-03T11:15:21.084]. [2025-09-03T11:15:21.084]-c7i4xlarge-2 now responding.-c7i4x,<*> Node <*> now responding,node_now_responding
a379325d-47d1-4df0-9e4d-422241eca245,2025-12-12T10:15:32.688074,test.log,both,2025-09-03T07:52:15.220,[2025-09-03T07:52:15.220] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,gpu_inventory_update,info,infra,"gpu,l4,inventory",1,Gres_FILE Gpu=l4=l4 [2025-09-03T07:52:15.220] Gres Name=gpu Type=l4 Count=1 Gres Gres Gres2025-09-03T07,[2025-09-03T07:52:15.220] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,gpu_inventory_update
a379325d-47d1-4df0-9e4d-422241eca245,2025-12-12T10:15:32.688074,test.log,both,2025-09-03T07:52:15.221,[2025-09-03T07:52:15.221] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,gpu_inventory_update,info,infra,"gpu,l4,inventory",1,Gres_FILE Gres=l4 [2025-09-03T07:52:15.221] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE Gres Name=gpu Type=l4 Count,<*> Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,gpu_inventory_update
a379325d-47d1-4df0-9e4d-422241eca245,2025-12-12T10:15:32.688074,test.log,both,2025-09-03T07:52:15.222,[2025-09-03T07:52:15.222] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,gpu_inventory_update,info,infra,"gpu,l4,inventory",1,Gres=gpu Gres [2025-09-03T07:52:15.222] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE] Gres Name=gpu Type=l4 Count=1,<*> Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,gpu_inventory_update
a379325d-47d1-4df0-9e4d-422241eca245,2025-12-12T10:15:32.688074,test.log,both,2025-09-03T07:52:15.223,[2025-09-03T07:52:15.223] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,gpu_inventory_update,info,infra,"gpu,l4,inventory",1,Gres_FILE gpu=l4 [2025-09-03T07:52:15.223] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE] Gres Name=gpu Type=l4,<*> Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,gpu_inventory_update
a379325d-47d1-4df0-9e4d-422241eca245,2025-12-12T10:15:32.688074,test.log,both,2025-09-03T07:52:15.224,[2025-09-03T07:52:15.224] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,gpu_inventory_update,info,infra,"gpu,l4,inventory",1,Gres=gpu Gres [2025-09-03T07:52:15.224] Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE] Gres Name=gpu Type=l4 Count=1,<*> Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T02:09:33.0000 POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete,info,infra,"power,save,complete",1,] [2025-09-03T02:09:33.0000 POWER: no more nodes resume for job] [2025-09-03T02:09:33.0000 POWER: no more nodes resume for job] POWER: no more nodes:,[2025-09-03T02:09:33.0000 POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T15:09:31.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold...].] [2025-09-03T15:09:31.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T15:09:31.0000 NETWORK,[2025-09-03T15:09:31.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T15:00:57.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T15:00:57.0000] [2025-09-03T15:00:57.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-0","[2025-09-03T15:00:57.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T11:46:13.0000 _slurm_rpc_submit_batch_job: JobId=973 InitPrio=30459 usec=6689,job_submit_batch,info,scheduler,"job,submit,batch",4,[2025-09-03T11:46:13.0000 _slurm_rpc_submit_batch_job: [2025-09-03T11:46:13.0000 _slurm_rpc_submit_batch,[2025-09-03T11:46:13.0000 _slurm_rpc_submit_batch_job: JobId=973 InitPrio=30459 usec=6689,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T19:18:59.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T19:18:59.0000.2025-09-03T19:18:59.0000 State of 0 reservations recovered recovered.0000] [2025-09-03T19:18:59.0000 State of 0 reservations recovered recovered.,[2025-09-03T19:18:59.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T03:47:41.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09T03:47:41.0000] [2025-09-03T03:47:41.0000,bf_resolution=60,bf_window=4320]5-09-03T03:47:41.0000] [202","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T21:31:32.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T21:31:32.0000 Running as primary controller.] [2025-09-03T21:31:32.0000 Running as primary controller.,[2025-09-03T21:31:32.0000 Running as primary controller,controller_primary
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T02:13:11.0000 POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete,info,infra,"power,save,complete",1,] [2025-09-03T02:13:11.0000 POWER: no more nodes resume for job] [2025-09-03T02:13:11.0000 POWER: no more nodes resume for job] POWER: no more nodes resume for job,<*> POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T19:57:47.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T19:57:47.0000 AUTH: authentication failure for user admin] [2025-09-03T19:57:47.0000 AUTH: authentication failure for user admin] [2025-09-03T19:57:47.0000,[2025-09-03T19:57:47.0000 AUTH: authentication failure for user admin,auth_failure
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T08:13:32.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T08:13:32.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding now now]-c7i4xlarge-3]-c7i4x-,[2025-09-03T08:13:32.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T17:27:32.0000 _slurm_rpc_submit_batch_job: JobId=1008 InitPrio=35944 usec=6535,job_submit_batch,info,scheduler,"job,submit,batch",9,5-09-03T17:27:32.0000 _slurm_rpc_submit_batch_job: [2025-09-03T17:27:32.0000 _slurm_rpc_submit_batch_,[2025-09-03T17:27:32.0000 _slurm_rpc_submit_batch_job: JobId=1008 InitPrio=35944 usec=6535,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T17:38:06.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-3 now responding.]-c7i4xlarge-3] [2025-09-03T17:38:06.0000 Node cpu-8c-std-d,<*> Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T21:13:50.0000 _slurm_rpc_submit_batch_job: JobId=991 InitPrio=32548 usec=5610,job_submit_batch,info,scheduler,"job,submit,batch",10,[2025-09-03T21:13:13.0000 _slurm_rpc_submit_batch_job: JobId=991 InitPrio=32548 usec=5610 _slurm_rpc_,[2025-09-03T21:13:50.0000 _slurm_rpc_submit_batch_job: JobId=991 InitPrio=32548 usec=5610,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T22:27:40.0000 POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete,info,infra,"power,save,complete",1,] [2025-09-03T22:27:40.0000 POWER: no more nodes resume for job] [2025-09-03T22:27:40.0000 POWER: no more nodes resume for job]:40.0000 POWER: no,<*> POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T13:20:13.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T13:20:13.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding. [2025-09-03T13:20:13.0000 Node cpu-8c-st,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T01:27:18.0000 _slurm_rpc_submit_batch_job: JobId=976 InitPrio=35529 usec=5381,job_submit_batch,info,scheduler,"job,submit,batch",11,[2025-09-03T01:27:18.0000 _slurm_rpc_submit_batch_job: [2025-09-03T01:27:18.0000 _slurm_rpc_submit_batch,[2025-09-03T01:27:18.0000 _slurm_rpc_submit_batch_job: JobId=976 InitPrio=35529 usec=5381,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T13:15:14.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T13:15:14.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding now now]-c7i4xlarge-5] [2025-09-03T13:,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T16:53:25.0000 Gres Name=gpu Type=l4 Count=7 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T16:53:25.0000 Gres] [2025-09-03T16:53:25.0000 Gres] [2025-09-03T16:53:25.0000 Gres] [2025-09-03T16:53:25,"[2025-09-03T16:53:25.0000 Gres Name=gpu Type=l4 Count=7 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T13:18:20.0000 sched: Allocate JobId=1011 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T13:18:20.0000 sched:-dy-c7i4xlarge-2]d-dy-c7i4xlarge-2] [2025-09-03T13:18:20.0000 sched,[2025-09-03T13:18:20.0000 sched: Allocate JobId=1011 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T21:30:46.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error,high,storage,"disk,scrubber,error",14,] [2025-09T21:30:46.0000 DISK: scrubber error while cleaning temp files..]..]. [2025-09T21:30:46.0000 DISK: scrubber error while cleaning temp files.05,[2025-09-03T21:30:46.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T16:18:49.0000 Gres Name=gpu Type=l4 Count=7 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T16:18:49.0000] [2025-09-03T16:18:49.0000 Gres [2025-09-03T16:18:49.0000 Gres [2025-09-03T16:18:49.0000 Gres,"<*> Gres Name=gpu Type=l4 Count=7 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T00:43:49.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error,high,storage,"disk,scrubber,error",14,] [2025-09-03T00:43:40.00 DISK: scrubber error while cleaning temp files..]] [2025-09-03T00:43:40.00 DISK: scrubber error while cleaning temp files..] [202,<*> DISK: scrubber error while cleaning temp files,disk_scrubber_error
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T23:08:09.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold.. latency to storage cluster exceeded threshold..] [2025-09-03T23:08:09.0000 NETWORK: latency to storage cluster exceeded threshold.. Storage cluster exceeded threshold. Storage cluster exceeded threshold.:,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T22:39:32.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T22:39:32.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T22:39:32.0000] [","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T18:34:05.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T18:34:05.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T18:34:05.0000 [202","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T19:46:44.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T19:46:44.0000.2025-09-03T19:46:44.0000 State of 0 reservations recovered recovered.0000] [2025-09-03T19:46:44.0000 State of 0 reservations recovered recovered.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T15:10:17.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,_id=i-1985197] [2025-09-03T15:10:10.0000] [2025-09-03T15:10:10.0000]-c7i4xlarge-4-c7i4xlarge-4-c7i,[2025-09-03T15:10:17.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T14:25:09.0000 sched: Allocate JobId=960 NodeList=cpu-8c-std-dy-c7i4xlarge-5 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T14:25:09.0000 sched:-dy-c7i4xlarge-5]dy-c7i4xlarge-5] [2025-09-03T14:25:09.0000 sched: Allocate,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T16:34:22.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T16:34:22.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T16:34:22.0000 [202","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T14:54:38.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error,high,storage,"disk,scrubber,error",14,[2025-09-03T14:54:38.0000 DISK: scrubber error while cleaning temp files.] [2025-09-03T14:54:38.0000 DISK: scrubber error while cleaning temp files.,<*> DISK: scrubber error while cleaning temp files,disk_scrubber_error
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T15:11:34.0000 sched: Allocate JobId=964 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T15:11:34.0000 sched:-dy-c7i4xlarge-3]d-dy-c7i4xlarge-3] [2025-09-03T15:11:34.0000 sched,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T01:55:12.0000 Node cpu-8c-std-dy-c7i4xlarge-2 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-2 now responding.]-c7i4xlarge-2 now responding-c7i4-c75-09-03T01:55:12.00005-09,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T06:25:23.0000 Gres Name=gpu Type=l4 Count=4 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T06:25:23.0000 Gres] [2025-09-03T06:25:23.0000 Gres] [2025-09-03T06:25:23.0000 Gres] [2025-09-03T06:25:23,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T02:30:47.0000 _slurm_rpc_submit_batch_job: JobId=989 InitPrio=32046 usec=6827,job_submit_batch,info,scheduler,"job,submit,batch",16,[2025-09-03T02:30:47.0000 _slurm_rpc_submit_batch_job: [2025-09-03T02:30:47.0000 _slurm_rpc_submit_batch_job,[2025-09-03T02:30:47.0000 _slurm_rpc_submit_batch_job: JobId=989 InitPrio=32046 usec=6827,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T23:57:16.0000 _slurm_rpc_submit_batch_job: JobId=956 InitPrio=35326 usec=6136,job_submit_batch,info,scheduler,"job,submit,batch",17,5-09-03T23:57:16.0000 [2025-09-03T23:57:16.0000 _slurm_rpc_submit_batch_job:]] [2025-09-03T23:57:16.,[2025-09-03T23:57:16.0000 _slurm_rpc_submit_batch_job: JobId=956 InitPrio=35326 usec=6136,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T04:16:20.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,cpu=8c-std-dy-c7i4xlarge-4] [2025-09-03T04:16:20.0000]-c7i4xlarge-4]-c7i4xlarge-4]-,<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T18:45:04.0000 POWER: Power save mode: 338 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,] [2025-09-03T18:45:04.0000 POWER: Power save mode: 338 nodes]2025-09-03T18:45:04.0000 POWER:: Power save mode: 338 nodes] [2025-09-03T18,[2025-09-03T18:45:04.0000 POWER: Power save mode: 338 nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T05:34:01.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-3 now responding.]c7i4xlarge-3]-c7i4xlarge-3]-c7i4xlarge-3]-5-09T05,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T05:54:09.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T05:54:09.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T05:54:09.0000] [2025-0","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T09:50:03.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold....] [2025-09-03T09:50:03.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T09:50:03.0000 NETWORK: latency,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T10:31:17.0000 sched: Allocate JobId=1027 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,] [2025-09-03T10:31:17.0000 sched: Allocate JobId=1027-dy-c7i4xlarge-3]-dy-c7i4xlarge-3] [2025-09-03T,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T17:58:56.0000 POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",19,] [2025-09-03T17:58:56.0000 POWER: power_save: waking nodes]] [2025-09-03T17:58:56.0000 POWER: power_save: waking nodes],[2025-09-03T17:58:56.0000 POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T01:48:13.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T01:48:13.0000 Running as primary controller.] [2025-09-03T01:48:13.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T13:51:07.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold..].] [2025-09-03T13:51:07.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T13:51:07.0000 NETWORK: latency,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T13:11:43.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09-03T13:11:43.0000 select/cons_tres: preparing for 19 partitions. [2025-09-03T13:11:43.0000 select/cons_tres: preparing for 19 partitions.,[2025-09-03T13:11:43.0000 select/cons_tres: preparing for 19 partitions,unknown
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T16:34:16.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T14:27:03.0000 sched: Allocate JobId=1021 NodeList=cpu-8c-std-dy-c7i4xlarge-5 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T14:27:03.0000 sched: Allocate JobId=1021-dy-c7i4xlarge-5]-dy-c7i4xlarge-5] [2025-09-03T14:27,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T03:43:14.0000 POWER: Power save mode: 330 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T03:43:14.0000 POWER: Power save mode: 330 nodes. [2025-09-03T03:43:14.0000 POWER: Power save mode: 330 nodes.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T03:01:27.0000 _slurm_rpc_submit_batch_job: JobId=1010 InitPrio=33871 usec=5060,job_submit_batch,info,scheduler,"job,submit,batch",21,[2025-09-03T03:01:27.0000 _slurm_rpc_submit_batch_job: [2025-09-03T03:01:27.0000 _slurm_rpc_submit_batch,[2025-09-03T03:01:27.0000 _slurm_rpc_submit_batch_job: JobId=1010 InitPrio=33871 usec=5060,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T16:49:05.0000 POWER: Power save mode: 338 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T16:49:05.0000 POWER: Power save mode: 338 nodes. [2025-09-03T16:49:05.0000 POWER: Power save mode: 338 nodes.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T00:51:52.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,2025-09-03T00:51:52.0000] [2025-09-03T00:51:52.0000 [2025-09-03T00:51:52.0000 [2025-09-03T00:51:52.0000] [2025-09-,"<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T06:20:19.0000 _slurm_rpc_submit_batch_job: JobId=955 InitPrio=33633 usec=5519,job_submit_batch,info,scheduler,"job,submit,batch",22,[2025-09-03T06:20:19.0000 _slurm_rpc_submit_batch_job: [2025-09-03T06:20:19.0000 _slurm_rpc_submit_batch,[2025-09-03T06:20:19.0000 _slurm_rpc_submit_batch_job: JobId=955 InitPrio=33633 usec=5519,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T09:20:54.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T09:20:54.0000 Running as primary controller.] [2025-09-03T09:20:54.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T19:12:05.0000 sched: Allocate JobId=1002 NodeList=cpu-8c-std-dy-c7i4xlarge-4 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,[2025-09-03T19:12:05.0000 sched: Allocate JobId=1002-dy-c7i4xlarge-4]-dy-c7i4xlarge-4] [2025-09-03T19,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T17:05:57.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T17:05:57.0000 Running as primary controller.] [2025-09-03T17:05:57.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T01:16:28.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,[2025-09-03T01:16:28.0000 State of 0 reservations recovered recovered. [2025-09-03T01:16:28.0000 State of 0 reservations recovered.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T22:10:50.0000 sched: Allocate JobId=985 NodeList=cpu-8c-std-dy-c7i4xlarge-1 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-1-dy-c7i4xlarge-1]-dy-c7i4xlarge-1] [2025-09-03T22:10:10.0000 s,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T00:41:12.0000 AUTH: failed password for ubuntu,auth_failure,high,security,"auth,security,login_failure",23,2025-09-03T00:41:12.0000 AUTH: failed password for ubuntu] [2025-09-03T00:41:12.0000 AUTH: failed password for ubuntu]] [2025-09-03T,[2025-09-03T00:41:12.0000 AUTH: failed password for ubuntu,auth_failure
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T09:23:42.0000 POWER: Power save mode: 335 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T09:23:42.0000 POWER: Power save mode: 335 nodes: [2025-09-03T09:23:42.0000 POWER: [2025-09-03T09:23:42.0000 POWER: Power save mode,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T12:45:32.0000 sched: Allocate JobId=959 NodeList=cpu-8c-std-dy-c7i4xlarge-5 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T12:45:32.0000 sched:-dy-c7i4xlarge-5]dy-c7i4xlarge-5] [2025-09-03T12:45:32.0000 sched:,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T02:06:39.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,_id=i-1985197] [2025-09-03T02:06:39.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4]]-c7i4xlarge-4],<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T21:05:55.0000 POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",19,] [2025-09-03T21:05:05.0000 POWER: power_save: waking nodes]]-c7i4xlarge-4] [2025-09-03T21:05:05.0000 POWER: power_s,<*> POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T23:35:28.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T20:48:23.0000 POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete,info,infra,"power,save,complete",1,] [2025-09-03T20:48:23.0000 POWER: no more nodes resume for job] [2025-09-03T20:48:23.0000 POWER: no more nodes resume for job] POWER: no more nodes:,<*> POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T22:23:17.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T22:23:17.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding now now]-c7i4xlarge-5] [2025-09-03T22:,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T23:07:33.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-1 now responding.]-c7i4xlarge-1 now responding-c7i4-c7i45-09-03T23:07:33.0000,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T00:05:10.0000 sched: Allocate JobId=978 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T00:05:10.0000 sched:-dy-c7i4xlarge-2] [2025-09-03T00:05:10.0000 sched: Allocate JobId=978] [2025-09-,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T09:49:25.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-5 now responding now now [2025-09-03T09:49:25.0000 Node cpu-c7i4xlarge-5 now responding now-c7i,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T01:21:32.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T01:21:32.0000 select/cons_tres: preparing for 19 partitions..]. [2025-09-03T01:21:32.0000 select/cons_tres: preparing for 19 partitions..,<*> select/cons_tres: preparing for 19 partitions,unknown
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T13:19:03.0000 POWER: Power save mode: 331 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T13:19:03.0000 POWER: Power save mode: 331 nodes [2025-09-03T13:19:03.0000 POWER: Power save mode: 331 nodes [2025-09-03T13:19:03.0000 POWER,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T22:12:36.0000 POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete,info,infra,"power,save,complete",1,] [2025-09-03T22:12:36.0000 POWER: no more nodes resume for job] [2025-09-03T22:12:36.0000 POWER: no more nodes resume for job]:36.0000 POWER: no,<*> POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T22:44:48.0000 DISK: /var is 97% full on node cpu-8c-std,disk_usage_high,warning,storage,"disk,usage,high",24,[2025-09-03T22:44:40.00 DISK: /var is 97% full on node cpu-8c-std]]] [2025-09-03T22:44:40.00 DISK:],[2025-09-03T22:44:48.0000 DISK: /var is 97% full on node cpu-8c-std,disk_usage_high
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T12:25:16.0000 POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete,info,infra,"power,save,complete",1,] [2025-09-03T12:25:16.0000 POWER: no more nodes resume for job] [2025-09-03T12:25:16.0000 POWER: no more nodes resume for job POWER: no more nodes resume for job,<*> POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T12:16:33.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T08:42:11.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T08:42:11.0000 Running as primary controller.] [2025-09-03T08:42:11.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T10:06:09.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,[2025-09-03T10:06:09.0000 State of 0 reservations recovered recovered. [2025-09-03T10:06:09.0000 State of 0 reservations recovered.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T11:34:43.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T11:34:43.0000 AUTH: authentication failure for user admin] [2025-09-03T11:34:43.0000 AUTH: authentication failure for user admin] [2025-09-03T11:34:43.0000,<*> AUTH: authentication failure for user admin,auth_failure
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T13:05:52.0000 _slurm_rpc_submit_batch_job: JobId=987 InitPrio=33692 usec=5263,job_submit_batch,info,scheduler,"job,submit,batch",25,[2025-09-03T13:05:52.0000 _slurm_rpc_submit_batch_job: [2025-09-03T13:05:52.0000 _slurm_rpc_submit_batch,[2025-09-03T13:05:52.0000 _slurm_rpc_submit_batch_job: JobId=987 InitPrio=33692 usec=5263,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T23:07:39.0000 DISK: /var is 96% full on node cpu-8c-std,disk_usage_high,warning,storage,"disk,usage,high",24,[2025-09-03T23:07:39.0000 DISK: /var is 96% full on node cpu-8c-std] [2025-09-03T23:07:39.0000 DISK: /var is 96% full,<*> DISK: /var is <*> full on node cpu-8c-std,disk_usage_high
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T19:51:56.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T19:51:56.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding now now]-c7i4xlarge-5] [2025-09-03T19:,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T08:29:54.0000 Gres Name=gpu Type=l4 Count=7 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T08:29:54.0000 Gres] [2025-09-03T08:29:54.0000 Gres] [2025-09-03T08:29:54.0000 Gres] [2025-09-03T08:29:54,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T22:06:07.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,[2025-09-03T22:06:07.0000 NETWORK: latency to storage cluster exceeded threshold.] [2025-09-03T22:06:07.0000 NETWORK: latency to storage cluster exceeded threshold.,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T21:09:01.0000 POWER: Power save mode: 336 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T21:09:01.0000 POWER: Power save mode: 336 nodes [2025-09-03T21:09:01.0000 POWER: Power save mode: 336 nodes [2025-09-03T21:09:01.0000 POWER,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T10:43:10.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,cpu=8c-std-dy-c7i4xlarge-4] [2025-09-03T10:43:10.0000]]-c7i4xlarge-4-x2025-09-03T10:,<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T22:03:30.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,2025-09T22:03:30.0000].]..2025-09T22:03:30.0000 Running as primary controller] [2025-09-03T22:03:30.0000 [2025-09-03T22:03:30.0000,<*> Running as primary controller,controller_primary
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T07:46:53.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T07:46:53.0000.2025-09-03T07:46:53.0000 State of 0 reservations recovered recovered.0000] [2025-09-03T07:46:53.0000 State of 0 reservations recovered recovered.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T03:14:10.0000 POWER: Power save mode: 328 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T03:14:10.0000 POWER: Power save mode: 328 nodes [2025-09-03T03:14:10.0000 POWER: Power save mode: 328 nodes [2025-09-03T03:14:10.0000 POWER,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T07:34:34.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,..].] [2025-09T07:34:34.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T07:34:34.0000 NETWORK: latency to storage cluster exceeded threshold.,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T15:40:27.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-1 now responding now now]-c7i4xlarge-1 now responding now-c7i4xlarge-1]-c7i4-5-09T15,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T01:31:10.0000 _slurm_rpc_submit_batch_job: JobId=971 InitPrio=35714 usec=5350,job_submit_batch,info,scheduler,"job,submit,batch",26,[2025-09-03T01:31:10.0000 _slurm_rpc_submit_batch_job: [2025-09-03T01:31:10.0000 _slurm_rpc_submit_batch_job,[2025-09-03T01:31:10.0000 _slurm_rpc_submit_batch_job: JobId=971 InitPrio=35714 usec=5350,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T11:05:09.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold.. latency to storage cluster exceeded threshold..] [2025-09-03T11:05:09.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T11:05:09.,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T09:30:15.0000 Gres Name=gpu Type=l4 Count=7 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09T09:30:15.0000] [2025-09-03T09:30:15.0000 Gres Name=gpu Type=l4 Count=74]2025-09-03T09:30:15.0000]] [2025-09-,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T07:57:15.0000 _slurm_rpc_submit_batch_job: JobId=983 InitPrio=33373 usec=5191,job_submit_batch,info,scheduler,"job,submit,batch",27,5-09-03T07:57:15.0000 [2025-09-03T07:57:15.0000 _slurm_rpc_submit_batch_job: [2025-09-03T07:57:15.0000 _s,[2025-09-03T07:57:15.0000 _slurm_rpc_submit_batch_job: JobId=983 InitPrio=33373 usec=5191,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T08:24:28.0000 _slurm_rpc_submit_batch_job: JobId=1019 InitPrio=34218 usec=5937,job_submit_batch,info,scheduler,"job,submit,batch",28,5-09-03T08:24:28.0000 _slurm_rpc_submit_batch_job: [2025-09-03T08:24:28.0000 _slurm_rpc_submit_batch_,[2025-09-03T08:24:28.0000 _slurm_rpc_submit_batch_job: JobId=1019 InitPrio=34218 usec=5937,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T21:51:34.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T21:51:34.0000 select/cons_tres: preparing for 19 partitions..]] [2025-09-03T21:51:34.0000 select/cons_tres: preparing for 19 partitions.,<*> select/cons_tres: preparing for 19 partitions,unknown
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T18:47:37.0000 Node cpu-8c-std-dy-c7i4xlarge-4 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-4 now responding now now [2025-09-03T18:47:37.0000 Node cpu-c7i4xlarge-4 now responding now-c7i,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T09:48:04.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,2025-09-03T09:48:04.0000 select/cons_tres: preparing for 19 partitions.s...s..s.2025-09-03T09:48:04.00005-09-03T09:,<*> select/cons_tres: preparing for 19 partitions,unknown
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T03:36:01.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-5 now responding.]-c7i4xlarge-5] [2025-09T03:36:01.0000 Node cpu-8c-std-dy-,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T05:37:09.0000 Gres Name=gpu Type=l4 Count=8 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T05:37:09.0000 Gres] [2025-09-03T05:37:09.0000 Gres] [2025-09-03T05:37:09.0000 Gres] [2025-09-03T05:37:09.0000 Gres,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T21:55:32.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T21:55:32.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T21:55:32.0000] [","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T16:45:36.0000 POWER: Power save mode: 339 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T16:45:36.0000 POWER: Power save mode: 339 nodes. [2025-09-03T16:45:36.0000 POWER: Power save mode: 339 nodes.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T21:06:33.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T21:06:33.0000 AUTH: authentication failure for user admin] [2025-09-03T21:06:33.0000 AUTH: authentication failure for user admin] [2025-09-03T21:06:33.0000,<*> AUTH: authentication failure for user admin,auth_failure
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T00:22:55.0000 Gres Name=gpu Type=l4 Count=4 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09T00:22:55.0000] [2025-09-03T00:22:55.0000 Gres4] [2025-09-03T00:22:55.0000 Gres55]0] [2025-09-03T00:,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T11:20:48.0000 _slurm_rpc_submit_batch_job: JobId=1003 InitPrio=32729 usec=6885,job_submit_batch,info,scheduler,"job,submit,batch",29,[2025-09-03T11:20:48.48.0000 _slurm_rpc_submit_batch_job: [2025-09-03T11:20:48.48.0000 _slurm_rpc_sub,[2025-09-03T11:20:48.0000 _slurm_rpc_submit_batch_job: JobId=1003 InitPrio=32729 usec=6885,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T10:38:31.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T10:38:31.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding. [2025-09-03T10:38:31.0000 Node cpu-8c-st,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T09:13:08.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold.2025-09-03T09:13:08.0000.. latency to storage cluster exceeded threshold..2025-09-03T09:13:08.0000]5-09-03T09:13:08.,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T10:31:39.0000 Gres Name=gpu Type=l4 Count=5 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T10:31:3.0000 Gres] [2025-09-03T10:31:3.0000 Gres] [2025-09-03T10:31:3.0000 Gres] [2025-09-03T10:31:03.0000 Gres,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T13:00:54.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,cpu=8c-std-dy-c7i4xlarge-4] [2025-09-03T13:00:54.0000]-c7i4xlarge-4---5-09-03T13:,<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T00:32:42.0000 POWER: Power save mode: 338 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T00:32:42.0000 POWER: Power save mode: 338 nodes. [2025-09-03T00:32:42.0000 POWER: Power save mode: 338 nodes.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T14:49:00.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,cpu=8c-std-dy-c7i4xlarge-4] [2025-09T14:49:00.0000]-c7i4xlarge-4]---5-09-03T14:49,<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T06:07:47.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold..] [2025-09-03T06:07:47.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T06:07:47.0000 NETWORK: latency to,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T15:01:58.0000 sched: Allocate JobId=961 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T15:01:58.0000 sched:-dy-c7i4xlarge-3]d-dy-c7i4xlarge-3] [2025-09-03T15:01:58.0000 sched,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T22:35:25.0000 POWER: Power save mode: 331 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T22:35:25.0000 POWER: Power save mode: 331 nodes: [2025-09-03T22:35:25.0000 POWER: [2025-09-03T22:35:25.0000 POWER: Power save mode,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T05:42:34.0000 sched: Allocate JobId=953 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T05:42:34.0000 sched:-dy-c7i4xlarge-2]d-dy-c7i4xlarge-2] [2025-09-03T05:42:34.0000 sched,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T05:33:32.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T05:33:32.0000 Running as primary controller.] [2025-09-03T05:33:32.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T01:27:01.0000 AUTH: user root failed password authentication,auth_failure,high,security,"auth,security,login_failure",30,[2025-09-03T01:27:01.0000 AUTH: user root failed password authentication.] [2025-09-03T01:27:01.0000 AUTH: user root failed password authentication.,[2025-09-03T01:27:01.0000 AUTH: user root failed password authentication,auth_failure
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T18:34:02.0000 _slurm_rpc_submit_batch_job: JobId=953 InitPrio=30192 usec=5880,job_submit_batch,info,scheduler,"job,submit,batch",31,[2025-09-03T18:34:02.0000 _slurm_rpc_submit_batch_job: JobId=953 InitPrio=30192 usec=5880 [2025-09-03T18:34:02.0000,[2025-09-03T18:34:02.0000 _slurm_rpc_submit_batch_job: JobId=953 InitPrio=30192 usec=5880,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T01:31:37.0000 DISK: /var is 93% full on node cpu-8c-std,disk_usage_high,warning,storage,"disk,usage,high",24,] [2025-09-03T01:31:37.0000 DISK: /var is 93% full on node cpu-8c-std]]] [2025-09-03T01:31:37.0000 DISK:,<*> DISK: /var is <*> full on node cpu-8c-std,disk_usage_high
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T20:43:55.0000 POWER: Power save mode: 330 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T20:43:55.0000 POWER: Power save mode: 330 nodes [2025-09-03T20:43:55.0000 POWER: Power save mode: 330 nodes [2025-09-03T20:43:55.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T03:06:32.0000 Gres Name=gpu Type=l4 Count=7 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09T03:06:32.0000 Gres] [2025-09-03T03:06:32.0000 Gres] [2025-09-03T03:06:32.0000 Gres] [2025-09-03T03:06:32.0000,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T22:07:01.0000 Gres Name=gpu Type=l4 Count=1 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T22:07:01.0000 Gres] [2025-09-03T22:07:01.0000 Gres] [2025-09-03T22:07:01.0000 Gres] [2025-09-03T22:07:01.0000 Gres,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T22:54:07.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold.. latency to storage cluster exceeded threshold...] [2025-09-03T22:54:07.0000 NETWORK: latency to storage cluster exceeded threshold..2025-09-03T22:54,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T01:29:03.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,[2025-09-03T01:29:03.0000 State of 0 reservations recovered recovered. [2025-09-03T01:29:03.0000 State of 0 reservations recovered.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T15:07:05.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T15:07:05.0000 select/cons_tres: preparing for 19 partitions..]. [2025-09-03T15:07:05.0000 select/cons_tres: preparing for 19 partitions..,<*> select/cons_tres: preparing for 19 partitions,unknown
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T02:49:47.0000 _slurm_rpc_submit_batch_job: JobId=1021 InitPrio=34758 usec=6066,job_submit_batch,info,scheduler,"job,submit,batch",32,[2025-09-03T02:49:47.0000 _slurm_rpc_submit_batch_job: [2025-09-03T02:49:47.0000 _slurm_rpc_submit_batch,[2025-09-03T02:49:47.0000 _slurm_rpc_submit_batch_job: JobId=1021 InitPrio=34758 usec=6066,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T19:56:11.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T19:56:11.0000 Running as primary controller. [2025-09-03T19:56:11.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T16:53:46.0000 _slurm_rpc_submit_batch_job: JobId=978 InitPrio=32923 usec=5007,job_submit_batch,info,scheduler,"job,submit,batch",33,[2025-09-03T16:53:46.0000 _slurm_rpc_submit_batch_job: [2025-09-03T16:53:46.0000 _slurm_rpc_submit_batch,[2025-09-03T16:53:46.0000 _slurm_rpc_submit_batch_job: JobId=978 InitPrio=32923 usec=5007,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T07:01:14.0000 sched: Allocate JobId=1000 NodeList=cpu-8c-std-dy-c7i4xlarge-4 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T07:01:14.0000 sched:-dy-c7i4xlarge-4]d-dy-c7i4xlarge-4] [2025-09-03T07:01:14.0000 sched,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T08:44:29.0000 DISK: /var is 97% full on node cpu-8c-std,disk_usage_high,warning,storage,"disk,usage,high",24,] [2025-09-03T08:44:29.0000 DISK:] [2025-09-03T08:44:29.0000 DISK:]] [2025-09-03T08:44:29.0000 DISK:,<*> DISK: /var is <*> full on node cpu-8c-std,disk_usage_high
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T14:06:26.0000 _slurm_rpc_submit_batch_job: JobId=986 InitPrio=31967 usec=5035,job_submit_batch,info,scheduler,"job,submit,batch",34,[2025-09-03T14:06:26.0000 _slurm_rpc_submit_batch_job: [2025-09-03T14:06:26.0000 _slurm_rpc_submit_batch,[2025-09-03T14:06:26.0000 _slurm_rpc_submit_batch_job: JobId=986 InitPrio=31967 usec=5035,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T11:41:58.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold..].] [2025-09-03T11:41:58.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T11:41:58.0000.0000,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T03:41:25.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T03:41:25.0000 AUTH: authentication failure for user admin] [2025-09-03T03:41:25.0000 AUTH: authentication failure for user admin] [2025-09-03T03:41:25.0000,<*> AUTH: authentication failure for user admin,auth_failure
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T04:26:45.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error,high,storage,"disk,scrubber,error",14,2025-09-03T04:26:06.0000 DISK: scrubber error while cleaning temp files..]..5-09-03T04:26:0000.5-09-03T04:26:00005-09-03T,<*> DISK: scrubber error while cleaning temp files,disk_scrubber_error
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T09:40:24.0000 sched: Allocate JobId=1008 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-2-dy-c7i4xlarge-2]d-dy-c7i4xlarge-2] [2025-09-03T09:40:24.0000,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T16:54:48.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T16:54:48.0000 select/cons_tres: preparing for 19 partitions..]. [2025-09-03T16:54:48.0000 select/cons_tres: preparing for 19 partitions..,<*> select/cons_tres: preparing for 19 partitions,unknown
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T05:07:59.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T05:07:59.0000 Running as primary controller.] [2025-09-03T05:07:59.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T10:27:52.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T10:27:52.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding. [2025-09-03T10:27:52.0000 Node cpu-8c-st,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T21:59:39.0000 sched: Allocate JobId=991 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-3-dy-c7i4xlarge-3]dy-c7i4xlarge-3] [2025-09-03T21:59:59.0000 sched,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T13:58:22.0000 DISK: /var is 86% full on node cpu-8c-std,disk_usage_high,warning,storage,"disk,usage,high",24,] [2025-09-03T13:58:22.0000 DISK:]]] [2025-09-03T13:58:22.0000 DISK: /var is 86% full on node cpu-8c-std,<*> DISK: /var is <*> full on node cpu-8c-std,disk_usage_high
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T22:59:27.0000 POWER: Power save mode: 326 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T22:59:27.0000 POWER: Power save mode: 326 nodes: [2025-09-03T22:59:27.0000 POWER: [2025-09-03T22:59:27.0000 POWER: Power save mode,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T13:58:39.0000 POWER: Power save mode: 322 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T13:58:39.0000 POWER: Power save mode: 322 nodes. [2025-09-03T13:58:39.0000 POWER: Power save mode: 322 nodes.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T22:51:10.0000 sched: Allocate JobId=967 NodeList=cpu-8c-std-dy-c7i4xlarge-4 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T22:51:10.0000 sched: Allocate JobId=967-dy-c7i4xlarge-4]dy-c7i4xlarge-4] [2025-09-03T22:51:,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T09:45:43.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T09:45:43.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding now now]-c7i4xlarge-3]-c7i4x-,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T02:29:10.0000 POWER: Power save mode: 333 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T02:29:10.0000 POWER: Power save mode: 333 nodes [2025-09-03T02:29:10.0000 POWER: Power save mode: 333 nodes [2025-09-03T02:29:10.0000 POWER,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T06:35:42.0000 Node cpu-8c-std-dy-c7i4xlarge-2 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T06:35:42.0000 Node cpu-8c-std-dy-c7i4xlarge-2 now responding now now]-c7i4xlarge-2 now responding-c7i4-,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T06:29:02.0000 Gres Name=gpu Type=l4 Count=3 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T06:29:02.0000 Gres] [2025-09-03T06:29:02.0000 Gres] [2025-09-03T06:29:02.0000 Gres] [2025-09-03T06:29:02.0000 Gres,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T00:52:43.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T00:52:43.0000 AUTH: authentication failure for user admin] [2025-09-03T00:52:43.0000 AUTH: authentication failure for user admin] [2025-09-03T00:52:43.0000,<*> AUTH: authentication failure for user admin,auth_failure
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T08:45:45.0000 sched: Allocate JobId=956 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-2-dy-c7i4xlarge-2]d-dy-c7i4xlarge-2] [2025-09-03T08:45:45.0000,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T13:22:55.0000 Node cpu-8c-std-dy-c7i4xlarge-2 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-2 now responding.]-c7i4xlarge-2 now responding-c7i4xlarge-2]-c7i4-5-09T13:22,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T12:55:01.0000 POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete,info,infra,"power,save,complete",1,] [2025-09-03T12:55:01.0000 POWER: no more nodes resume for job] [2025-09-03T12:55:01.0000 POWER: no more nodes resume for job] POWER: no more nodes resume for job,<*> POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T18:06:37.0000 _slurm_rpc_submit_batch_job: JobId=1007 InitPrio=30631 usec=5078,job_submit_batch,info,scheduler,"job,submit,batch",35,[2025-09-03T18:06:37.0000 _slurm_rpc_submit_batch_job: [2025-09-03T18:06:37.0000 _slurm_rpc_submit_batch,[2025-09-03T18:06:37.0000 _slurm_rpc_submit_batch_job: JobId=1007 InitPrio=30631 usec=5078,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T22:42:46.0000 _slurm_rpc_submit_batch_job: JobId=998 InitPrio=32442 usec=6064,job_submit_batch,info,scheduler,"job,submit,batch",36,5-09-03T22:42:46.0000 [2025-09-03T22:42:46.0000 _slurm_rpc_submit_batch_job: [2025-09-03T22:42:46.0000 _s,[2025-09-03T22:42:46.0000 _slurm_rpc_submit_batch_job: JobId=998 InitPrio=32442 usec=6064,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T05:08:23.0000 Node cpu-8c-std-dy-c7i4xlarge-4 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-4 now responding now now]-c7i4xlarge-4] [2025-09-03T05:08:23.0000 Node cpu-c7i4,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T23:04:55.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",8,] [2025-09-03T23:04:55.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding.]-c7i4xlarge-1 now responding-c7i4xlarge,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T03:39:53.0000 POWER: Power save mode: 320 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T03:39:53.0000 POWER: Power save mode: 320 nodes. [2025-09-03T03:39:53.0000 POWER: Power save mode: 320 nodes.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T16:31:49.0000 POWER: Power save mode: 329 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T16:31:49.0000 POWER: Power save mode: 329 nodes: [2025-09-03T16:31:49.0000 POWER: [2025-09-03T16:31:49.0000 POWER: Power save mode,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T03:19:06.0000 sched: Allocate JobId=1012 NodeList=cpu-8c-std-dy-c7i4xlarge-1 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T03:19:06.0000 sched: Allocate JobId=1012-dy-c7i4xlarge-1]-dy-c7i4xlarge-1] [2025-09-03T03:19,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T17:48:49.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold...].] [2025-09-03T17:48:49.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T17:48:49.0000.,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T03:50:43.0000 _slurm_rpc_submit_batch_job: JobId=988 InitPrio=30962 usec=5641,job_submit_batch,info,scheduler,"job,submit,batch",37,[2025-09-03T03:50:43.0000 _slurm_rpc_submit_batch_job: JobId=988 InitPrio=30962 usec=5641 _slurm_rpc_,[2025-09-03T03:50:43.0000 _slurm_rpc_submit_batch_job: JobId=988 InitPrio=30962 usec=5641,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T07:32:44.0000 _slurm_rpc_submit_batch_job: JobId=952 InitPrio=30423 usec=5707,job_submit_batch,info,scheduler,"job,submit,batch",38,5-09-03T07:32:44.0000 [2025-09-03T07:32:44.0000 _slurm_rpc_submit_batch_job: [2025-09-03T07:32:44.0000 _s,[2025-09-03T07:32:44.0000 _slurm_rpc_submit_batch_job: JobId=952 InitPrio=30423 usec=5707,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T14:06:49.0000 sched: Allocate JobId=1001 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-2-dy-c7i4xlarge-2]dy-c7i4xlarge-2] [2025-09-03T14:06:06.0000 sched:,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T18:30:16.0000 Gres Name=gpu Type=l4 Count=6 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T18:30:16.0000] [2025-09-03T18:30:16.0000 Gres4] [2025-09-03T18:30:16.0000 Gres]] [2025-09-03T18:,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T15:14:40.0000 _slurm_rpc_submit_batch_job: JobId=1017 InitPrio=35133 usec=5559,job_submit_batch,info,scheduler,"job,submit,batch",39,5-09-03T15:14:40.40.0000 _slurm_rpc_submit_batch_job: [2025-09-03T15:14:40.40.0000 _slurm_rpc_submit,[2025-09-03T15:14:40.0000 _slurm_rpc_submit_batch_job: JobId=1017 InitPrio=35133 usec=5559,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T23:37:29.0000 Node cpu-8c-std-dy-c7i4xlarge-2 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T23:37:29.0000 Node cpu-8c-std-dy-c7i4xlarge-2 now responding now now]-c7i4xlarge-2 now responding now-c7i4x,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T07:07:34.0000 DISK: /var is 98% full on node cpu-8c-std,disk_usage_high,warning,storage,"disk,usage,high",24,] [2025-09T07:07:34.0000 DISK: /var is 98% full on node cpu-8c-std]]] [2025-09T07:07:34.0000 DISK:],<*> DISK: /var is <*> full on node cpu-8c-std,disk_usage_high
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T05:11:49.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error,high,storage,"disk,scrubber,error",14,[2025-09-03T05:11:11.0000 DISK: scrubber error while cleaning temp files.] [2025-09-03T05:11:11.0000 DISK: scrubber error while cleaning temp files.,<*> DISK: scrubber error while cleaning temp files,disk_scrubber_error
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T16:53:03.0000 _slurm_rpc_submit_batch_job: JobId=968 InitPrio=35750 usec=5093,job_submit_batch,info,scheduler,"job,submit,batch",40,[2025-09-03T16:53:03.0000 _slurm_rpc_submit_batch_job: [2025-09-03T16:53:03.0000 _slurm_rpc_submit_batch_job,[2025-09-03T16:53:03.0000 _slurm_rpc_submit_batch_job: JobId=968 InitPrio=35750 usec=5093,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T04:53:28.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T04:53:28.0000 Running as primary controller.] [2025-09-03T04:53:28.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T12:14:49.0000 AUTH: user root failed password authentication,auth_failure,high,security,"auth,security,login_failure",30,[2025-09-03T12:14:49.0000 AUTH: user root failed password authentication.,<*> AUTH: user root failed password authentication,auth_failure
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T08:08:42.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T14:39:32.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,cpu=8c-std-dy-c7i4xlarge-4] [2025-09-03T14:39:32.0000]]--.00005-09-03T14:39:32.0000,<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T07:55:21.0000 Node cpu-8c-std-dy-c7i4xlarge-2 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu 8c-std-dy-c7i4xlarge-2 now responding.]-c7i4xlarge-2 now responding-c7i4-c7i4x5-09-03T07:55:21.,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T12:08:31.0000 Gres Name=gpu Type=l4 Count=3 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T12:08:31.0000 Gres] [2025-09-03T12:08:31.0000 Gres] [2025-09-03T12:08:31.0000 Gres] [2025-09-03T12:08:31,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T03:10:23.0000 POWER: Power save mode: 339 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T03:10:23.0000 POWER: Power save mode: 339 nodes] [2025-09-03T03:10:23.0000 POWER: Power save mode: 339 nodes] [2025-09-03T03:10:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T01:28:39.0000 _slurm_rpc_submit_batch_job: JobId=1018 InitPrio=34433 usec=5441,job_submit_batch,info,scheduler,"job,submit,batch",41,[2025-09-03T01:28:39.0000 _slurm_rpc_submit_batch_job: [2025-09-03T01:28:39.0000 _slurm_rpc_submit_batch,[2025-09-03T01:28:39.0000 _slurm_rpc_submit_batch_job: JobId=1018 InitPrio=34433 usec=5441,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T01:12:49.0000 sched: Allocate JobId=1010 NodeList=cpu-8c-std-dy-c7i4xlarge-1 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-1] [2025-09-03T01:12:49.0000 sched:-dy-c7i4xlarge-1]] [2025-09-03T01,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T20:18:44.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T20:18:44.0000 Running as primary controller.] [2025-09-03T20:18:44.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T14:03:34.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-3 now responding.]-c7i4xlarge-3] [2025-09T14:03:34.0000 Node cpu-8c-std-dy,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T00:35:17.0000 POWER: Power save mode: 333 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T00:35:17.0000 POWER: Power save mode: 333 nodes] [2025-09-03T00:35:17.0000 POWER: Power save mode: 333 nodes] [2025-09-03T00:35:,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T09:39:35.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T09:39:35.0000.2025-09-03T09:39:35.0000 State of 0 reservations recovered recovered.0] [2025-09-03T09:39:35.0000 State of 0 reservations recovered recovered.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T13:07:06.0000 Node cpu-8c-std-dy-c7i4xlarge-2 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-2 now responding.]-c7i4xlarge-2] [2025-09-03T13:07:06.0000 Node cpu-8c-std-d,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T11:43:27.0000 POWER: Power save mode: 333 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T11:43:27.0000 POWER: Power save mode: 333 nodes [2025-09-03T11:43:27.0000 POWER: Power save mode: 333 nodes [2025-09-03T11:43:27.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T13:39:36.0000 Node cpu-8c-std-dy-c7i4xlarge-2 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-2 now responding.]-c7i4xlarge-2 now responding.] [2025-09-03T13:39:36.0000-c7i4xlarge-2,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T13:57:49.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T13:57:49.0000 State of 0 reservations recovered recovered recovered recovered2025-09-03T13:57:49.0000 State of0000 State of 0 reservations recovered recovered0:57:49.00005-09-03,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T15:09:13.0000 DISK: /var is 97% full on node cpu-8c-std,disk_usage_high,warning,storage,"disk,usage,high",24,] [2025-09-03T15:09:13.0000 DISK: /var is 97% full on node cpu-8c-std]]]].0000] [2025-09-03T15:09:13,<*> DISK: /var is <*> full on node cpu-8c-std,disk_usage_high
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T21:59:53.0000 AUTH: user root failed password authentication,auth_failure,high,security,"auth,security,login_failure",30,[2025-09-03T21:59:53.0000 AUTH: user root failed password authentication.] [2025-09-03T21:59:53.0000 AUTH: user root failed password authentication.,<*> AUTH: user root failed password authentication,auth_failure
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T23:35:38.0000 POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",19,] [2025-09-03T23:35:38.0000 POWER: power_save: waking nodes]] [2025-09-03T23:35:38.0000 POWER: power_save: waking nodes],<*> POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T03:55:19.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T03:55:19.0000 AUTH: authentication failure for user admin] [2025-09-03T03:55:19.0000 AUTH: authentication failure for user admin] [2025-09-03T03:55:19.0000,<*> AUTH: authentication failure for user admin,auth_failure
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T00:41:18.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-3 now responding now now]-c7i4xlarge-3] [2025-09-03T00:41:18.0000 Node cpu-c7i4,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T14:28:20.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T14:28:20.0000 select/cons_tres: preparing for 19 partitions..]. [2025-09-03T14:28:20.0000 select/cons_tres: preparing for 19 partitions..,<*> select/cons_tres: preparing for 19 partitions,unknown
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T04:02:49.0000 POWER: Power save mode: 339 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T04:02:49.0000 POWER: Power save mode: 339 nodes: [2025-09-03T04:02:49.0000 POWER: [2025-09-03T04:02:49.0000 POWER: Power save mode,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T18:30:20.0000 Gres Name=gpu Type=l4 Count=5 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T18:30:30.0000 Gres] [2025-09-03T18:30:30.0000 Gres] [2025-09-03T18:30:30.0000 Gres] [2025-09-03T18:30:30,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T22:02:04.0000 _slurm_rpc_submit_batch_job: JobId=1015 InitPrio=32210 usec=5378,job_submit_batch,info,scheduler,"job,submit,batch",42,[2025-09-03T22:02:04.0000 _slurm_rpc_submit_batch_job: [2025-09-03T22:02:04.0000 _slurm_rpc_submit_batch_job,[2025-09-03T22:02:04.0000 _slurm_rpc_submit_batch_job: JobId=1015 InitPrio=32210 usec=5378,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T21:26:42.0000 Gres Name=gpu Type=l4 Count=3 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T21:26:42.0000 Gres] [2025-09-03T21:26:42.0000 Gres] [2025-09-03T21:26:42.0000 Gres] [2025-09-03T21:26:42,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T14:02:57.0000 Gres Name=gpu Type=l4 Count=8 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T14:02:57.0000 Gres] [2025-09-03T14:02:57.0000 Gres] [2025-09-03T14:02:57.0000 Gres] [2025-09-03T14:02:57,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T17:24:32.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",8,] [2025-09-03T17:24:32.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding..] [2025-09-03T17:24:32.0000 Node-,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T02:48:54.0000 Gres Name=gpu Type=l4 Count=7 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T02:48:54.0000] [2025-09-03T02:48:54.0000 Gres Name=gpu Type=l4 Count=74]]] [2025-09-03T02:48:54.,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T21:27:08.0000 Gres Name=gpu Type=l4 Count=8 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T21:27:08.0000 Gres] [2025-09-03T21:27:08.0000 Gres] [2025-09-03T21:27:08.0000 Gres] [2025-09-03T21:27:08.0000 Gres,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T06:54:17.0000 POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",19,] [2025-09-03T06:54:17.0000 POWER: power_save: waking nodes]] [2025-09-03T06:54:17.0000 POWER: power_save: waking nodes],<*> POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T10:09:59.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09-03T10:09:59.0000 select/cons_tres: preparing for 19 partitions. [2025-09-03T10:09:59.0000 select/cons_tres: preparing for 19 partitions.,<*> select/cons_tres: preparing for 19 partitions,unknown
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T13:13:02.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T13:13:02.0000 Running as primary controller.] [2025-09-03T13:13:02.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T12:51:29.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T12:51:29.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T12:51:29.0000] [","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T15:47:22.0000 AUTH: user root failed password authentication,auth_failure,high,security,"auth,security,login_failure",30,[2025-09-03T15:47:22.0000 AUTH: user root failed password authentication.] [2025-09-03T15:47:22.0000 AUTH: user root failed password authentication.,<*> AUTH: user root failed password authentication,auth_failure
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T06:56:56.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T06:56:56.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T06:56:56.0000 [202","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T22:18:31.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T22:18:31.0000 AUTH: authentication failure for user admin] [2025-09-03T22:18:31.0000 AUTH: authentication failure for user admin] [2025-09-03T22:18:31.0000,<*> AUTH: authentication failure for user admin,auth_failure
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T10:21:01.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,cpu=8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197..]]-c7i4xlarge-4-x2025-09-03T10:,<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T16:50:42.0000 POWER: Power save mode: 322 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T16:50:42.0000 POWER: Power save mode: 322 nodes. [2025-09-03T16:50:42.0000 POWER: Power save mode: 322 nodes.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T12:55:55.0000 Gres Name=gpu Type=l4 Count=7 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T12:55:55.0000] [2025-09-03T12:55:55.0000 Gres Name=gpu Type=l4 Count=7 [2025-09-03T12:55:55.0000 Gres Name=gpu,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T15:29:00.0000 _slurm_rpc_submit_batch_job: JobId=1016 InitPrio=35895 usec=6740,job_submit_batch,info,scheduler,"job,submit,batch",43,5-09-03T15:29:00.0000 _slurm_rpc_submit_batch_job: [2025-09-03T15:29:00.0000 _slurm_rpc_submit_batch_,[2025-09-03T15:29:00.0000 _slurm_rpc_submit_batch_job: JobId=1016 InitPrio=35895 usec=6740,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T20:06:46.0000 _slurm_rpc_submit_batch_job: JobId=958 InitPrio=31222 usec=6831,job_submit_batch,info,scheduler,"job,submit,batch",44,-] [2025-09-03T20:06:46.0000 _slurm_rpc_submit_batch_job:] [2025-09-03T20:06:46.0000 [2025-09-03T20:06,[2025-09-03T20:06:46.0000 _slurm_rpc_submit_batch_job: JobId=958 InitPrio=31222 usec=6831,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T21:27:46.0000 POWER: Power save mode: 333 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T21:27:46.0000 POWER: Power save mode: 333 nodes [2025-09-03T21:27:46.0000 POWER: Power save mode: 333 nodes [2025-09-03T21:27:46.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T11:10:04.0000 sched: Allocate JobId=963 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T11:10:04.0000 sched: Allocate JobId=963-dy-c7i4xlarge-3]-dy-c7i4xlarge-3] [2025-09-03T11:10,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T13:02:23.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T13:02:23.0000 select/cons_tres: preparing for 19 partitions..]. [2025-09-03T13:02:23.0000 select/cons_tres: preparing for 19 partitions..,<*> select/cons_tres: preparing for 19 partitions,unknown
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T01:07:41.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error,high,storage,"disk,scrubber,error",14,] [2025-09T01:07:41.0000 DISK: scrubber error while cleaning temp files..]..]. [2025-09T01:07:41.0000 DISK: scrubber error while cleaning temp files.,<*> DISK: scrubber error while cleaning temp files,disk_scrubber_error
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T16:33:06.0000 POWER: Power save mode: 330 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T16:33:06.0000 POWER: Power save mode: 330 nodes [2025-09-03T16:33:06.0000 POWER: Power save mode: 330 nodes [2025-09-03T16:33:06.0000 POWER,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T11:42:35.0000 DISK: /var is 86% full on node cpu-8c-std,disk_usage_high,warning,storage,"disk,usage,high",24,] [2025-09-03T11:42:35.0000 DISK:]]] [2025-09-03T11:42:35.0000 DISK:: /var is 86% full on node cpu-8c,<*> DISK: /var is <*> full on node cpu-8c-std,disk_usage_high
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T17:17:47.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,cpu=8c-std-dy-c7i4xlarge-4] [2025-09T17:17:47.0000]-c7i4xlarge-4---5-09T17:17:47.,<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T02:04:33.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T15:06:12.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding,info,infra,"node,recovered,online",8,] [2025-09-03T15:06:12.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding.] [2025-09-03T15:06:12.0000-c7i4x,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T00:41:41.0000 _slurm_rpc_submit_batch_job: JobId=979 InitPrio=31554 usec=5855,job_submit_batch,info,scheduler,"job,submit,batch",45,[2025-09-03T00:41:41.0000 _slurm_rpc_submit_batch_job: JobId=979 InitPrio=31554 usec=5855 [2025-09-03T00:41:,[2025-09-03T00:41:41.0000 _slurm_rpc_submit_batch_job: JobId=979 InitPrio=31554 usec=5855,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T00:22:34.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error,high,storage,"disk,scrubber,error",14,[2025-09-03T00:22:34.0000 DISK: scrubber error while cleaning temp files. [2025-09-03T00:22:34.0000 DISK: scrubber error while cleaning temp files.,<*> DISK: scrubber error while cleaning temp files,disk_scrubber_error
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T15:02:42.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T15:02:42.0000 select/cons_tres: preparing for 19 partitions..]. [2025-09-03T15:02:42.0000 select/cons_tres: preparing for 19 partitions..,<*> select/cons_tres: preparing for 19 partitions,unknown
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T00:58:51.0000 POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",19,] [2025-09-03T00:58:58.0000 POWER: power_save: waking nodes]]-c7i4xlarge-4] [2025-09-03T00:58:58.0000 POWER: power_s,<*> POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T21:04:51.0000 _slurm_rpc_submit_batch_job: JobId=999 InitPrio=35504 usec=6817,job_submit_batch,info,scheduler,"job,submit,batch",46,-] [2025-09-03T21:04:51.0000 _slurm_rpc_submit_batch_job:] [2025-09-03T21:04:51.0000 [2025-09-03T21:04,[2025-09-03T21:04:51.0000 _slurm_rpc_submit_batch_job: JobId=999 InitPrio=35504 usec=6817,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T15:08:44.0000 sched: Allocate JobId=1018 NodeList=cpu-8c-std-dy-c7i4xlarge-5 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T15:08:44.0000 sched:-dy-c7i4xlarge-5]d-dy-c7i4xlarge-5] [2025-09-03T15:08:44.0000 sched,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T18:49:25.0000 Node cpu-8c-std-dy-c7i4xlarge-2 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-2 now responding now now]-c7i4xlarge-2 now responding now-c7i4xlarge-2-c7i4x-5-09T18,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T10:25:26.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T10:25:26.0000.2025-09-03T10:25:26.0000 State of 0 reservations recovered recovered.0000.-03T10:25:26.00005-09-03T10:25:,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T11:31:43.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error,high,storage,"disk,scrubber,error",14,] [2025-09-03T11:31:43.0000 DISK: scrubber error while cleaning temp files..]] [2025-09-03T11:31:43.0000 DISK: scrubber error while cleaning temp files..,<*> DISK: scrubber error while cleaning temp files,disk_scrubber_error
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T02:36:48.0000 _slurm_rpc_submit_batch_job: JobId=959 InitPrio=31071 usec=5827,job_submit_batch,info,scheduler,"job,submit,batch",47,[2025-09-03T02:36:48.0000 _slurm_rpc_submit_batch_job: [2025-09-03T02:36:48.0000 _slurm_rpc_submit_batch,[2025-09-03T02:36:48.0000 _slurm_rpc_submit_batch_job: JobId=959 InitPrio=31071 usec=5827,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T01:08:52.0000 _slurm_rpc_submit_batch_job: JobId=1028 InitPrio=31783 usec=6993,job_submit_batch,info,scheduler,"job,submit,batch",48,[2025-09-03T01:08:52.0000 _slurm_rpc_submit_batch_job: [2025-09-03T01:08:52.0000 _slurm_rpc_submit_batch,[2025-09-03T01:08:52.0000 _slurm_rpc_submit_batch_job: JobId=1028 InitPrio=31783 usec=6993,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T17:47:31.0000 sched: Allocate JobId=988 NodeList=cpu-8c-std-dy-c7i4xlarge-1 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T17:47:31.0000 sched:-dy-c7i4xlarge-1]d-dy-c7i4xlarge-1] [2025-09-03T17:47:31.0000 sched,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T10:14:24.0000 _slurm_rpc_submit_batch_job: JobId=1022 InitPrio=33714 usec=5207,job_submit_batch,info,scheduler,"job,submit,batch",49,[2025-09-03T10:14:24.0000 _slurm_rpc_submit_batch_job: [2025-09-03T10:14:24.0000 _slurm_rpc_submit_batch,[2025-09-03T10:14:24.0000 _slurm_rpc_submit_batch_job: JobId=1022 InitPrio=33714 usec=5207,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T19:07:55.0000 POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",19,] [2025-09-03T19:07:07.0000 POWER: power_save: waking nodes]] [2025-09-03T19:07:07.0000 POWER:-c7i4xlarge-4] [2025-09-,<*> POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T05:00:32.0000 POWER: Power save mode: 339 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T05:00:32.0000 POWER: Power save mode: 339 nodes. [2025-09-03T05:00:32.0000 POWER: Power save mode: 339 nodes.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T18:02:19.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09-03T18:02:19.0000 select/cons_tres: preparing for 19 partitions..]] [2025-09-03T18:02:19.0000 select/cons_tres: preparing for 19 partitions,<*> select/cons_tres: preparing for 19 partitions,unknown
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T14:42:05.0000 _slurm_rpc_submit_batch_job: JobId=984 InitPrio=35508 usec=6679,job_submit_batch,info,scheduler,"job,submit,batch",50,--] [2025-09-03T14:42:05.0000 [2025-09-03T14:42:05.0000 [2025-09-03T14:42:05.0000 _slurm_rpc_submit_b,[2025-09-03T14:42:05.0000 _slurm_rpc_submit_batch_job: JobId=984 InitPrio=35508 usec=6679,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T22:51:19.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T22:51:19.0000 Running as primary controller.] [2025-09-03T22:51:19.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T02:08:43.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T02:08:43.0000.2025-09-03T02:08:43.0000 State of0000] [2025-09-03T02:08:43.0000 State of 0 reservations recovered recovered.5-09-03T,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T10:47:02.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold.....2025-09-03T10:47:02.0000 NETWORK: latency to storage cluster exceeded threshold..2025-09-03T10:47:02.00005-09-03T,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T23:04:11.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,[2025-09-03T23:04:11.0000 NETWORK: latency to storage cluster exceeded threshold.] [2025-09-03T23:04:11.0000 NETWORK: latency to storage cluster exceeded threshold.,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T20:34:53.0000 sched: Allocate JobId=962 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-3]-dy-c7i4xlarge-3]d-dy-c7i4xlarge-3] [2025-09-03T20:34:53.0000,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T01:47:24.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error,high,storage,"disk,scrubber,error",14,[2025-09-03T01:47:24.0000 DISK: scrubber error while cleaning temp files.] [2025-09-03T01:47:24.0000 DISK: scrubber error while cleaning temp files.,<*> DISK: scrubber error while cleaning temp files,disk_scrubber_error
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T03:18:22.0000 sched: Allocate JobId=990 NodeList=cpu-8c-std-dy-c7i4xlarge-1 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-1-dy-c7i4xlarge-1]-dy-c7i4xlarge-1] [2025-09-03T03:18:22.0000 s,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T08:12:32.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold..].. latency to storage cluster exceeded threshold..2025-09-03T08:12:32.0000] [2025-09-03T08:12:32.00005-09-03T,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T19:25:09.0000 AUTH: failed password for ubuntu,auth_failure,high,security,"auth,security,login_failure",23,2025-09-03T19:25:09.0000 AUTH: failed password for ubuntu] [2025-09-03T19:25:09.0000 AUTH: failed password for ubuntu]] [2025-09-03T,<*> AUTH: failed password for ubuntu,auth_failure
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T12:25:45.0000 POWER: Power save mode: 322 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T12:25:45.0000 POWER: Power save mode: 322 nodes [2025-09-03T12:25:45.0000 POWER: Power save mode: 322 nodes [2025-09-03T12:25:45.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T01:38:04.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,to storage cluster exceeded threshold.. latency to storage cluster exceeded threshold... latency to storage cluster exceeded threshold.2025-09T01:38:04.0000 NETWORK: latency to storage cluster exceeded threshold..2025-09T01:38,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T18:01:24.0000 POWER: Power save mode: 323 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T18:01:24.0000 POWER: Power save mode: 323 nodes: [2025-09-03T18:01:24.0000 POWER: [2025-09-03T18:01:24.0000 POWER: Power save mode,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T02:53:30.0000 POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete,info,infra,"power,save,complete",1,] [2025-09-03T02:53:30.0000 POWER: no more nodes resume for job]]5-09-03T02:53:30.0000 POWER: no more nodes resume for job JobId=986] [2025-09-,<*> POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T09:42:15.0000 POWER: Power save mode: 337 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T09:42:15.0000 POWER: Power save mode: 337 nodes. [2025-09-03T09:42:15.0000 POWER: Power save mode: 337 nodes.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T18:28:19.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T18:28:19.0000 State of 0 reservations recovered recovered recovered recovered2025-09-03T18:28:19.0000 State of 0 reservations recovered recovered0000 State of 0 reservations recovered recovered]-03T18:28,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T18:54:27.0000 sched: Allocate JobId=952 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8-dy-c7i4xlarge-3]d-dy-c7i4xlarge-3] [2025-09-03T18:,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T22:23:18.0000 POWER: Power save mode: 329 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T22:23:18.0000 POWER: Power save mode: 329 nodes. [2025-09-03T22:23:18.0000 POWER: Power save mode: 329 nodes.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T03:02:09.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-3 now responding.]-c7i4xlarge-3] [2025-09T03:02:09.0000 Node cpu-8c-std-dy-,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T13:22:53.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,cpu=8c-std-dy-c7i4xlarge-4]].0000] [2025-09-03T13:22:53.0000--.0000] [2025-09-03T13:22:,<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T21:48:18.0000 sched: Allocate JobId=1017 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-3-dy-c7i4xlarge-3]dy-c7i4xlarge-3] [2025-09-03T21:48:18.0000 sched,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T00:52:00.0000 POWER: Power save mode: 332 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T00:52:00.0000 POWER: Power save mode: 332 nodes: [2025-09-03T00:52:00.0000 POWER: [2025-09-03T00:52:00.0000 POWER: Power save mode: 332,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T06:17:21.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T06:17:21.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T06:17:21.0000] [","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T00:06:48.0000 Gres Name=gpu Type=l4 Count=8 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09T00:06:48.0000] [2025-09-03T00:06:48.0000 Gres Name=gpu Type=l4 Count=8 Gres Name=gpu] [2025-09-03T00:06:48.0000 Gre,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T16:17:23.0000 POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",19,] [2025-09-03T16:17:23.0000 POWER: power_save: waking nodes]] [2025-09-03T16:17:23.0000 POWER: power_save: waking nodes],<*> POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T07:46:34.0000 Gres Name=gpu Type=l4 Count=5 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T07:46:34.0000 Gres] [2025-09-03T07:46:34.0000 Gres] [2025-09-03T07:46:34.0000 Gres] [2025-09-03T07:46:34,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T22:32:58.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T06:40:49.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09-03T06:40:49.0000 select/cons_tres: preparing for 19 partitions. [2025-09-03T06:40:49.0000 select/cons_tres: preparing for 19 partitions.,<*> select/cons_tres: preparing for 19 partitions,unknown
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T08:45:03.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T08:45:03.0000 select/cons_tres: preparing for 19 partitions..] [2025-09T08:45:03.0000 select/cons_tres: preparing for 19 partitions] [2025-09-03T,<*> select/cons_tres: preparing for 19 partitions,unknown
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T11:52:42.0000 _slurm_rpc_submit_batch_job: JobId=965 InitPrio=33623 usec=6540,job_submit_batch,info,scheduler,"job,submit,batch",51,[2025-09-03T11:52:42.0000 _slurm_rpc_submit_batch_job: [2025-09-03T11:52:42.0000 _slurm_rpc_submit_batch,[2025-09-03T11:52:42.0000 _slurm_rpc_submit_batch_job: JobId=965 InitPrio=33623 usec=6540,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T14:46:29.0000 Gres Name=gpu Type=l4 Count=2 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T14:46:29.0000 Gres] [2025-09-03T14:46:29.0000 Gres] [2025-09-03T14:46:29.0000 Gres] [2025-09-03T14:46:29,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T11:29:15.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T11:29:15.0000.2025-09-03T11:29:15.00000000] [2025-09-03T11:29:15.0000 State of 0 reservations recovered recovered.]:29:15.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T21:31:33.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T21:31:33.0000.2025-09-03T21:31:33.0000 State of 0 reservations recovered recovered.0000] [2025-09-03T21:31:33.0000 State of 0 reservations recovered recovered.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T15:41:47.0000 _slurm_rpc_submit_batch_job: JobId=977 InitPrio=35779 usec=5532,job_submit_batch,info,scheduler,"job,submit,batch",52,5-09-03T15:41:40.00 _slurm_rpc_submit_batch_job: [2025-09-03T15:41:40.00 _slurm_rpc_submit_batch_job:,[2025-09-03T15:41:47.0000 _slurm_rpc_submit_batch_job: JobId=977 InitPrio=35779 usec=5532,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T14:15:05.0000 POWER: Power save mode: 335 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T14:15:05.0000 POWER: Power save mode: 335 nodes [2025-09-03T14:15:05.0000 POWER: Power save mode: 335 nodes [2025-09-03T14:15:05.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T18:13:48.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T18:13:13.0000] [2025-09-03T18:13:13.0000 [2025-09-03T18:13:13.0000 SchedulerParameters=preempt_youngest_first,bf_re","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T03:00:54.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error,high,storage,"disk,scrubber,error",14,[2025-09-03T03:00:54.0000 DISK: scrubber error while cleaning temp files.] [2025-09-03T03:00:54.0000 DISK: scrubber error while cleaning temp files.,<*> DISK: scrubber error while cleaning temp files,disk_scrubber_error
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T10:29:37.0000 _slurm_rpc_submit_batch_job: JobId=960 InitPrio=34740 usec=5010,job_submit_batch,info,scheduler,"job,submit,batch",53,5-09-03T10:29:37.0000 _slurm_rpc_submit_batch_job:] [2025-09-03T10:29:37.0000 _slurm_rpc_submit_job,[2025-09-03T10:29:37.0000 _slurm_rpc_submit_batch_job: JobId=960 InitPrio=34740 usec=5010,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T05:21:26.0000 POWER: Power save mode: 337 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T05:21:26.0000 POWER: Power save mode: 337 nodes: [2025-09-03T05:21:26.0000 POWER: [2025-09-03T05:21:26.0000 POWER: Power save mode,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T01:23:28.0000 _slurm_rpc_submit_batch_job: JobId=1024 InitPrio=34747 usec=6494,job_submit_batch,info,scheduler,"job,submit,batch",54,[2025-09-03T01:23:28.0000 _slurm_rpc_submit_batch_job: [2025-09-03T01:23:28.0000 _slurm_rpc_submit_batch,[2025-09-03T01:23:28.0000 _slurm_rpc_submit_batch_job: JobId=1024 InitPrio=34747 usec=6494,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T06:58:29.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T06:58:29.0000 Running as primary controller.] [2025-09-03T06:58:29.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T00:43:48.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T00:43:48.0000 Running as primary controller.] [2025-09-03T00:43:48.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T19:40:47.0000 sched: Allocate JobId=977 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,std-dy-c7i4xlarge-2-dy-c7i4xlarge-2]-dy-c7i4xlarge-2] [2025-09-03T19:40:03.0000 sched: Allocate Job,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T00:37:43.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T00:37:43.0000 select/cons_tres: preparing for 19 partitions..]] [2025-09-03T00:37:43.0000 select/cons_tres: preparing for 19 partitions..,<*> select/cons_tres: preparing for 19 partitions,unknown
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T21:19:31.0000 POWER: Power save mode: 338 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T21:19:31.0000 POWER: Power save mode: 338 nodes. [2025-09-03T21:19:31.0000 POWER: Power save mode: 338 nodes.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T22:24:43.0000 POWER: Power save mode: 323 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T22:24:43.0000 POWER: Power save mode: 323 nodes: [2025-09-03T22:24:43.0000 POWER: [2025-09-03T22:24:43.0000 POWER: Power save mode,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T11:29:34.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T11:29:34.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T11:29:34.0000 [202","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T12:58:04.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-3 now responding.]-c7i4xlarge-3] [2025-09-03T12:58:04.0000 Node cpu-8c-std-d,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T10:06:01.0000 POWER: Power save mode: 331 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T10:06:01.0000 POWER: Power save mode: 331 nodes [2025-09-03T10:06:01.0000 POWER: Power save mode: 331 nodes [2025-09-03T10:06:01.0000 POWER,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T05:04:53.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T05:04:53.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T05:04:53.0000] [","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T18:10:52.0000 _slurm_rpc_submit_batch_job: JobId=961 InitPrio=30777 usec=5525,job_submit_batch,info,scheduler,"job,submit,batch",55,5-09-03T18:10:10.0000 _slurm_rpc_submit_batch_job: [2025-09-03T18:10:10.0000 _slurm_rpc_submit_batch_job,[2025-09-03T18:10:52.0000 _slurm_rpc_submit_batch_job: JobId=961 InitPrio=30777 usec=5525,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T21:10:29.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T21:10:29.0000 AUTH: authentication failure for user admin] [2025-09-03T21:10:29.0000 AUTH: authentication failure for user admin] [2025-09-03T21:10:29.0000,<*> AUTH: authentication failure for user admin,auth_failure
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T23:42:06.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,[2025-09-03T23:42:06.0000 State of 0 reservations recovered. [2025-09-03T23:42:06.0000 State of 0 reservations recovered.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T23:46:31.0000 POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete,info,infra,"power,save,complete",1,] [2025-09-03T23:46:31.0000 POWER: no more nodes resume for job] [2025-09-03T23:46:31.0000 POWER: no more nodes resume for job] POWER: no more nodes:,<*> POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T23:15:08.0000 _slurm_rpc_submit_batch_job: JobId=972 InitPrio=32933 usec=6989,job_submit_batch,info,scheduler,"job,submit,batch",56,[2025-09-03T23:15:08.0000 _slurm_rpc_submit_batch_job: JobId=972 InitPrio=32933 usec=6989] [2025-09-03T23:15:,[2025-09-03T23:15:08.0000 _slurm_rpc_submit_batch_job: JobId=972 InitPrio=32933 usec=6989,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T10:58:32.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error,high,storage,"disk,scrubber,error",14,[2025-09-03T10:58:32.0000 DISK: scrubber error while cleaning temp files.] [2025-09-03T10:58:32.0000 DISK: scrubber error while cleaning temp files.,<*> DISK: scrubber error while cleaning temp files,disk_scrubber_error
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T05:19:35.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,cpu=8c-std-dy-c7i4xlarge-4.]]-c7i4xlarge-4]-.00005-09-03T05:19:35.0000 POWER: Node:,<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T05:25:16.0000 sched: Allocate JobId=972 NodeList=cpu-8c-std-dy-c7i4xlarge-1 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-1-dy-c7i4xlarge-1]d-dy-c7i4xlarge-1] [2025-09-03T05:25:16.0000,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T02:29:18.0000 POWER: Power save mode: 335 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T02:29:18.0000 POWER: Power save mode: 335 nodes. [2025-09-03T02:29:18.0000 POWER: Power save mode: 335 nodes.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T16:28:51.0000 POWER: Power save mode: 325 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,] [2025-09-03T16:28:28.0000 POWER: Power save mode: 325 nodes]] [2025-09-03T16:28:28.0000 POWER: Power save mode: 325 nodes] [2025-09-03,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T03:27:06.0000 Gres Name=gpu Type=l4 Count=6 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09T03:27:06.0000 Gres] [2025-09-03T03:27:06.0000 Gres] [2025-09-03T03:27:06.0000 Gres] [2025-09-03T03:27:06.0000 Gres] [,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T08:10:00.0000 POWER: Power save mode: 324 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T08:10:00.0000 POWER: Power save mode: 324 nodes [2025-09-03T08:10:00.0000 POWER: Power save mode: 324 nodes [2025-09-03T08:10:00.0000 POWER,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T12:16:05.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T12:16:05.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding now now]-c7i4xlarge-1 now responding now-c7i4x,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T04:17:39.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T04:17:39.0000.2025-09-03T04:17:39.00000000] [2025-09-03T04:17:39.0000 [2025-09-03T04:17:39.0000 State of,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T22:00:44.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T22:00:44.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding. [2025-09-03T22:00:44.0000 Node cpu-8c-std-,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T03:43:04.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T22:03:29.0000 sched: Allocate JobId=1013 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-3] [2025-09-03T22:03:29.0000 sched: Allocate JobId=1013-dy-c7i4xlarge-3],<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T20:08:18.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-1 now responding.]-c7i4xlarge-1 now responding-c7i4xlarge-1]-c7i4x5-09-03T20:,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T06:33:13.0000 Gres Name=gpu Type=l4 Count=2 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T06:33:13.0000 Gres] [2025-09-03T06:33:13.0000 Gres] [2025-09-03T06:33:13.0000 Gres] [2025-09-03T06:33:13,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T16:49:02.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T16:49:02.0000 AUTH: authentication failure for user admin] [2025-09-03T16:49:02.0000 AUTH: authentication failure for user admin] [2025-09-03T16:49:02.0000 AUTH:,<*> AUTH: authentication failure for user admin,auth_failure
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T15:19:45.0000 AUTH: user root failed password authentication,auth_failure,high,security,"auth,security,login_failure",30,[2025-09-03T15:19:45.0000 AUTH: user root failed password authentication.] [2025-09-03T15:19:45.0000 AUTH: user root failed password authentication.,<*> AUTH: user root failed password authentication,auth_failure
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T22:35:50.0000 Node cpu-8c-std-dy-c7i4xlarge-4 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-4 now responding..c7i4xlarge-4 now responding.c7i4xlarge-4 now responding.-c7i4xlarge-4-c7i4,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T00:16:31.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T00:16:31.0000 select/cons_tres: preparing for 19 partitions..2025-09T00:16:31.0000 select/cons_tres: preparing for 19 partitions.] [2025-09T00,<*> select/cons_tres: preparing for 19 partitions,unknown
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T04:19:35.0000 AUTH: failed password for ubuntu,auth_failure,high,security,"auth,security,login_failure",23,] [2025-09-03T04:19:35.0000 AUTH: failed password for ubuntu.]. [2025-09-03T04:19:35.0000 AUTH: failed password for ubuntu..,<*> AUTH: failed password for ubuntu,auth_failure
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T17:21:20.0000 POWER: Power save mode: 335 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T17:21:20.0000 POWER: Power save mode: 335 nodes [2025-09-03T17:21:20.0000 POWER: Power save mode: 335 nodes [2025-09-03T17:21:20.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T16:42:53.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold..].] [2025-09-03T16:42:53.0000 [2025-09-03T16:42:53.0000] [2025-09-03T16:42:53.0000.],<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T05:34:28.0000 sched: Allocate JobId=954 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T05:34:28.0000 sched:-dy-c7i4xlarge-2]d-dy-c7i4xlarge-2] [2025-09-03T05:34:28.0000 sched,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T05:25:20.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T05:25:20.0000 select/cons_tres: preparing for 19 partitions..]. [2025-09-03T05:25:20.0000 select/cons_tres: preparing for 19 partitions..,<*> select/cons_tres: preparing for 19 partitions,unknown
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T20:18:25.0000 _slurm_rpc_submit_batch_job: JobId=975 InitPrio=32293 usec=5314,job_submit_batch,info,scheduler,"job,submit,batch",57,5-09-03T20:18:25.0000 [2025-09-03T20:18:25.0000 _slurm_rpc_submit_batch_job: [2025-09-03T20:18:25.0000 _s,[2025-09-03T20:18:25.0000 _slurm_rpc_submit_batch_job: JobId=975 InitPrio=32293 usec=5314,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T01:53:48.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T01:53:48.0000 AUTH: authentication failure for user admin] [2025-09-03T01:53:48.0000 AUTH: authentication failure for user admin] [2025-09-03T01:53:48.0000,<*> AUTH: authentication failure for user admin,auth_failure
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T02:58:35.0000 POWER: Power save mode: 323 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T02:58:35.0000 POWER: Power save mode: 323 nodes: [2025-09-03T02:58:35.0000 POWER: [2025-09-03T02:58:35.0000 POWER: Power save mode,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T17:50:44.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold...].] [2025-09-03T17:50:44.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T17:50:44.0000 NETWORK,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T06:18:38.0000 POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",19,] [2025-09-03T06:18:38.0000 POWER: power_save: waking nodes]] [2025-09-03T06:18:38.0000 POWER: power_save: waking nodes],<*> POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T01:21:25.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T01:21:25.0000 Running as primary controller.] [2025-09-03T01:21:25.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T05:20:06.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error,high,storage,"disk,scrubber,error",14,[2025-09-03T05:20:06.0000 DISK: scrubber error while cleaning temp files.] [2025-09-03T05:20:06.0000 DISK: scrubber error while cleaning temp files.,<*> DISK: scrubber error while cleaning temp files,disk_scrubber_error
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T14:39:49.0000 _slurm_rpc_submit_batch_job: JobId=954 InitPrio=34048 usec=6726,job_submit_batch,info,scheduler,"job,submit,batch",58,] [2025-09-03T14:39:39.0000 _slurm_rpc_submit_batch_job: [2025-09-03T14:39:39.0000 _slurm_rpc_submit_,[2025-09-03T14:39:49.0000 _slurm_rpc_submit_batch_job: JobId=954 InitPrio=34048 usec=6726,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T11:01:49.0000 POWER: Power save mode: 323 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T11:01:49.0000 POWER: Power save mode: 323 nodes: [2025-09-03T11:01:49.0000 POWER: [2025-09-03T11:01:49.0000 POWER: Power save mode,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T04:25:43.0000 POWER: Power save mode: 320 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T04:25:43.0000 POWER: Power save mode: 320 nodes. [2025-09-03T04:25:43.0000 POWER: [2025-09-03T04:25:43.0000 POWER: Power save mode,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T20:47:56.0000 Gres Name=gpu Type=l4 Count=2 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T20:47:56.0000] [2025-09-03T20:47:56.0000 Gres4]]] [2025-09-03T20:47:56.0000 Gres=gpu] [2025-0,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T19:22:47.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T19:22:47.0000.2025-09-03T19:22:47.0000 State of 0 reservations recovered recovered.0000] [2025-09-03T19:22:47.0000 State of 0 reservations recovered recovered.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T10:51:33.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,cpu=8c-std-dy-c7i4xlarge-4]]]-c7i4xlarge-4]-.00005-09-03T10:51:33.00005-09-03T10,<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T09:51:25.0000 Node cpu-8c-std-dy-c7i4xlarge-4 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T09:51:25.0000 Node cpu-8c-std-dy-c7i4xlarge-4 now responding. [2025-09-03T09:51:25.0000 Node cpu-8c-st,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T03:35:48.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T03:35:48.0000 select/cons_tres: preparing for 19 partitions..]. [2025-09T03:35:48.0000 select/cons_tres: preparing for 19 partitions..,<*> select/cons_tres: preparing for 19 partitions,unknown
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T15:24:01.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,cpu=8c-std-dy-c7i4xlarge-4.]]-c7i4xlarge-4]-i42025-09-03T15:24:01.0000]5:005-09-,<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T11:04:36.0000 _slurm_rpc_submit_batch_job: JobId=997 InitPrio=33665 usec=5225,job_submit_batch,info,scheduler,"job,submit,batch",59,5-09-03T11:04:36.0000 [2025-09-03T11:04:36.0000 _slurm_rpc_submit_batch_job: [2025-09-03T11:04:36.0000 _s,[2025-09-03T11:04:36.0000 _slurm_rpc_submit_batch_job: JobId=997 InitPrio=33665 usec=5225,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T13:50:13.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,.]..].] [2025-09T13:50:13.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T13:50:13.0000 NETWORK:].] [202,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T09:57:45.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold...].] [2025-09-03T09:57:45.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T09:57:45.0000 NETWORK,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T20:27:35.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,...].] [2025-09-03T20:27:35.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T20:27:35.0000 NETWORK: latency to storage cluster exceeded,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T09:22:20.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T09:22:20.0000.2025-09-03T09:22:20.0000 State of 0 reservations recovered recovered.0000] [2025-09-03T09:22:20.0000 State of 0 reservations recovered recovered.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T20:58:12.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T20:58:12.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T20:58:12.0000 [2025-09","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T18:19:30.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,[2025-09-03T18:19:30.0000 State of 0 reservations recovered recovered. [2025-09-03T18:19:30.0000 State of 0 reservations recovered.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T06:37:04.0000 POWER: Power save mode: 340 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T06:37:04.0000 POWER: Power save mode: 340 nodes [2025-09-03T06:37:04.0000 POWER: Power save mode: 340 nodes [2025-09-03T06:37:04.0000 POWER,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T23:22:29.0000 Running as primary controller,controller_primary,info,infra,"controller,primary",6,[2025-09-03T23:22:29.0000 Running as primary controller.] [2025-09-03T23:22:29.0000 Running as primary controller.,<*> Running as primary controller,controller_primary
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T05:12:23.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T05:12:23.0000 State of 0 reservations recovered recovered recovered recovered2025-09-03T05:12:23.0000 State of 0 reservations recovered recovered recovered0000]-03T05:12:23.00005-0,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T13:48:22.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T13:48:22.0000 AUTH: authentication failure for user admin] [2025-09-03T13:48:22.0000 AUTH: authentication failure for user admin] [2025-09-03T13:48:22.0000,<*> AUTH: authentication failure for user admin,auth_failure
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T05:37:02.0000 sched: Allocate JobId=1025 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T05:37:02.0000 sched:-dy-c7i4xlarge-2]dy-c7i4xlarge-2] [2025-09-03T05:37:02.0000 sched: Allocate,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T05:45:09.0000 sched: Allocate JobId=979 NodeList=cpu-8c-std-dy-c7i4xlarge-4 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T05:45:09.0000 sched: Allocate JobId=979-dy-c7i4xlarge-4]c7i4xlarge-4] [2025-09-03T05:45:09.0000,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T09:17:02.0000 _slurm_rpc_submit_batch_job: JobId=1014 InitPrio=31247 usec=6682,job_submit_batch,info,scheduler,"job,submit,batch",60,[2025-09-03T09:17:02.0000 _slurm_rpc_submit_batch_job: [2025-09-03T09:17:02.0000 _slurm_rpc_submit_batch_job,[2025-09-03T09:17:02.0000 _slurm_rpc_submit_batch_job: JobId=1014 InitPrio=31247 usec=6682,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T12:08:19.0000 Gres Name=gpu Type=l4 Count=6 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T12:08:19.0000 Gres] [2025-09-03T12:08:19.0000 Gres] [2025-09-03T12:08:19.0000 Gres] [2025-09-03T12:08:19,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T08:03:33.0000 Gres Name=gpu Type=l4 Count=7 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09T08:03:33.0000] [2025-09-03T08:03:33.0000 Gres Name=gpu Type=l4 Count=74]2025-09T08:03:33.0000 Gres]202,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T00:29:51.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,node cpu=8c-std-dy-c7i4xlarge-4]]] POWER: Node cpu-8c-std-dy-c7i4xlarge-4--.0000,<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T10:58:42.0000 sched: Allocate JobId=1022 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T10:58:42.0000-dy-c7i4xlarge-3]dy-c7i4xlarge-3] [2025-09-03T10:58:42.0000 sched: Allocate JobI,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T05:59:34.0000 POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete,info,infra,"power,save,complete",1,] [2025-09-03T05:59:34.0000 POWER: no more nodes resume for job] [2025-09-03T05:59:34.0000 POWER: no more nodes resume for job]:59:34.0000 POWER,<*> POWER: no more nodes to resume for job JobId=986,cluster_power_save_complete
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T06:22:38.0000 Gres Name=gpu Type=l4 Count=8 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T06:22:38.0000 Gres] [2025-09-03T06:22:38.0000 Gres] [2025-09-03T06:22:38.0000 Gres] [2025-09-03T06:22:38,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T11:56:30.0000 sched: Allocate JobId=998 NodeList=cpu-8c-std-dy-c7i4xlarge-5 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-5-dy-c7i4xlarge-5]dy-c7i4xlarge-5] [2025-09-03T11:56:30.0000 sched,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T20:02:06.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T20:02:06.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T20:02:06.0000] [2025-0","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T01:23:06.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,[2025-09-03T01:23:06.0000 NETWORK: latency to storage cluster exceeded threshold. [2025-09-03T01:23:06.0000 NETWORK: latency to storage cluster exceeded threshold.,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T10:40:25.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T10:40:25.0000 select/cons_tres: preparing for 19 partitions.. [2025-09-03T10:40:25.0000 select/cons_tres: preparing for 19 partitions]..,<*> select/cons_tres: preparing for 19 partitions,unknown
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T04:55:53.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold..].] [2025-09-03T04:55:53.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T04:55:53.0000 to storage cluster,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T05:08:16.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold..].] [2025-09-03T05:08:16.0000 [2025-09-03T05:08:16.0000 NETWORK: latency to storage cluster exceeded threshold..].0000,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T16:54:41.0000 DISK: /var is 93% full on node cpu-8c-std,disk_usage_high,warning,storage,"disk,usage,high",24,] [2025-09-03T16:54:41.0000 DISK:] [2025-09-03T16:54:41.0000 DISK:]] [2025-09-03T16:54:41.0000 DISK:,<*> DISK: /var is <*> full on node cpu-8c-std,disk_usage_high
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T00:34:37.0000 AUTH: user root failed password authentication,auth_failure,high,security,"auth,security,login_failure",30,[2025-09-03T00:34:37.0000 AUTH: user root failed password authentication.,<*> AUTH: user root failed password authentication,auth_failure
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T16:18:04.0000 AUTH: user root failed password authentication,auth_failure,high,security,"auth,security,login_failure",30,2025-09-03T16:18:04.0000 AUTH: user root failed password authentication..: user root failed password authentication]] [2025-09-03T16:18:04.0000 AUTH: user root failed password authentication..2025-0,<*> AUTH: user root failed password authentication,auth_failure
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T01:31:19.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T01:31:19.0000 AUTH: authentication failure for user admin] [2025-09-03T01:31:19.0000 AUTH: authentication failure for user admin] [2025-09-03T01:31:19.0000,<*> AUTH: authentication failure for user admin,auth_failure
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T06:56:09.0000 POWER: Power save mode: 339 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T06:56:09.0000 POWER: Power save mode: 339 nodes [2025-09-03T06:56:09.0000 POWER: Power save mode: 339 nodes] [2025-09-03T06:56:09.0000,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T04:57:16.0000 select/cons_tres: preparing for 19 partitions,unknown,info,unknown,,20,[2025-09T04:57:16.0000 select/cons_tres: preparing for 19 partitions..]. [2025-09-03T04:57:16.0000 select/cons_tres: preparing for 19 partitions..,<*> select/cons_tres: preparing for 19 partitions,unknown
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T09:55:30.0000 sched: Allocate JobId=971 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,std-dy-c7i4xlarge-3-dy-c7i4xlarge-3]d-dy-c7i4xlarge-3-std-dy-c7i4xlarge-3]cpu,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T08:12:04.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T08:12:04.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding.]-c7i4xlarge-5] [2025-09-03T08:12:,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T14:27:13.0000 Gres Name=gpu Type=l4 Count=2 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T14:27:13.0000 Gres] [2025-09-03T14:27:13.0000 Gres] [2025-09-03T14:27:13.0000 Gres] [2025-09-03T14:27:13,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T13:33:35.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T13:33:35.0000.2025-09-03T13:33:35.0000 State of 0 reservations recovered recovered.0000] [2025-09-03T13:33:35.0000 State of 0 reservations recovered recovered.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T03:12:32.0000 sched: Allocate JobId=989 NodeList=cpu-8c-std-dy-c7i4xlarge-4 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-4-dy-c7i4xlarge-4]-dy-c7i4xlarge-4] [2025-09-03T03:12:32.0000 s,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T05:02:10.0000 DISK: /var is 89% full on node cpu-8c-std,disk_usage_high,warning,storage,"disk,usage,high",24,] [2025-09-03T05:02:10.0000 DISK: /var is 89% full on node cpu-8c-std]]] [2025-09-03T05:02:10.0000 DISK:],<*> DISK: /var is <*> full on node cpu-8c-std,disk_usage_high
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T19:21:53.0000 Node cpu-8c-std-dy-c7i4xlarge-2 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T19:21:53.0000 Node cpu-8c-std-dy-c7i4xlarge-2 now responding.]-c7i4xlarge-2 now responding-c7i4xlarge-2,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T08:18:14.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T08:18:14.0000.2025-09-03T08:18:14.0000 State of 0 reservations recovered recovered.0000] [2025-09-03T08:18:14.0000 State of 0 reservations recovered recovered.,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T19:02:54.0000 sched: Allocate JobId=1028 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T19:02:54.0000 sched:-dy-c7i4xlarge-3]d-dy-c7i4xlarge-3] [2025-09-03T19:02:54.0000 sched,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T13:57:28.0000 Gres Name=gpu Type=l4 Count=5 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T13:57:28.0000 Gres] [2025-09-03T13:57:28.0000 Gres] [2025-09-03T13:57:28.0000 Gres] [2025-09-03T13:57:28,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T03:38:25.0000 sched: Allocate JobId=974 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,cpu-8c-std-dy-c7i4xlarge-2-dy-c7i4xlarge-2]-dy-c7i4xlarge-2] [2025-09-03T03:38:25.0000 s,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T09:53:15.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T09:53:15.0000 AUTH: authentication failure for user admin] [2025-09-03T09:53:15.0000 AUTH: authentication failure for user admin] [2025-09-03T09:53:15.0000,<*> AUTH: authentication failure for user admin,auth_failure
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T18:14:17.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding,node_now_responding,info,infra,"node,recovered,online",8,cpu-8c-std-dy-c7i4xlarge-1 now responding now now]-c7i4xlarge-1 now responding now-c7i4xlarge-1]-c7i4xlarge-1-5-0,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T21:22:08.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online,info,infra,"power,node,online",15,cpu=8c-std-dy-c7i4xlarge-4.]]-c7i4xlarge-4]-i4xlarge-42025-09-03T21:22:08.0000] [5-0,<*> POWER: Node cpu-8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197,cluster_power_node_online
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T06:40:51.0000 _slurm_rpc_submit_batch_job: JobId=982 InitPrio=30833 usec=6290,job_submit_batch,info,scheduler,"job,submit,batch",61,[2025-09-03T06:40:40.0000 _slurm_rpc_submit_batch_job: JobId=982 InitPrio=30833 usec=6290 [2025-09-03T06:40:,[2025-09-03T06:40:51.0000 _slurm_rpc_submit_batch_job: JobId=982 InitPrio=30833 usec=6290,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T15:29:03.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T15:29:03.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T15:29:03.0000 SchedulerPara","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T17:13:55.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T17:13:55.0000 State of.2025-09-03T17:13:55.0000 State of 0 reservations recovered recovered.0000] [2025-09-03T17:13:55.0000 State of 0 reservations recovered,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T01:52:53.0000 DISK: scrubber error while cleaning temp files,disk_scrubber_error,high,storage,"disk,scrubber,error",14,] [2025-09T01:52:53.0000 DISK: scrubber error while cleaning temp files..]] [2025-09-03T01:52:53.0000 DISK: scrubber error while cleaning temp files..],<*> DISK: scrubber error while cleaning temp files,disk_scrubber_error
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T18:45:37.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T18:45:37.0000 AUTH: authentication failure for user admin] [2025-09-03T18:45:37.0000 AUTH: authentication failure for user admin] [2025-09-03T18:45:37.0000,<*> AUTH: authentication failure for user admin,auth_failure
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T14:55:01.0000 _slurm_rpc_submit_batch_job: JobId=963 InitPrio=31283 usec=6299,job_submit_batch,info,scheduler,"job,submit,batch",62,[2025-09-03T14:55:01.0000 _slurm_rpc_submit_batch_job: JobId=963 InitPrio=31283 usec=6299 [2025-09-03T14:55:01.,[2025-09-03T14:55:01.0000 _slurm_rpc_submit_batch_job: JobId=963 InitPrio=31283 usec=6299,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T18:06:45.0000 Gres Name=gpu Type=l4 Count=5 Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update,info,infra,"gpu,l4,inventory",12,2025-09-03T18:06:06.0000 Gres] [2025-09-03T18:06:05.0000 Gres] [2025-09-03T18:06:06.0000 Gres] [2025-09-03T18:06:05.0000,"<*> Gres Name=gpu Type=l4 <*> Flags=HAS_FILE,HAS_TYPE,ENV_NVML",gpu_inventory_update
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T00:51:55.0000 POWER: Power save mode: 325 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T00:51:55.0000 POWER: Power save mode: 325 nodes: [2025-09-03T00:51:55.0000 POWER: [2025-09-03T00:51:55.0000 POWER: Power save mode,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T17:41:32.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T17:41:32.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding now now]-c7i4xlarge-3]-c7i4x-,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T08:52:35.0000 sched: Allocate JobId=1016 NodeList=cpu-8c-std-dy-c7i4xlarge-5 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T08:52:35.0000 sched:-dy-c7i4xlarge-5]d-dy-c7i4xlarge-5] [2025-09-03T08:52:35.0000 sched,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T04:24:19.0000 State of 0 reservations recovered,cluster_state_recovered_no_reservations,info,scheduler,"cluster,state,recovered,no_reservations",5,2025-09-03T04:24:19.0000 State of 0 reservations recovered recovered recovered recovered2025-09-03T04:24:19.0000 State of0000] [2025-09-03T04:24:19.0000 State of 0 reservations recovered,<*> State of 0 reservations recovered,cluster_state_recovered_no_reservations
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T16:00:45.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T16:00:00.0000] [2025-09-03T16:00:00.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320,bf_","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T20:05:42.0000 sched: Allocate JobId=966 NodeList=cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,[2025-09-03T20:05:42.0000 sched: Allocate JobId=966-dy-c7i4xlarge-3]d-dy-c7i4xlarge-3] [2025-09-03T,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T21:35:04.0000 DISK: /var is 86% full on node cpu-8c-std,disk_usage_high,warning,storage,"disk,usage,high",24,/var is 86% full on node cpu-8c-std] [2025-09-03T21:35:04.0000 DISK:] /var is 86% full on node cpu-8c-std],<*> DISK: /var is <*> full on node cpu-8c-std,disk_usage_high
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T17:51:05.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold..].] [2025-09-03T17:51:05.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T17:51:05.0000 NETWORK:,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T21:06:52.0000 sched: Allocate JobId=965 NodeList=cpu-8c-std-dy-c7i4xlarge-2 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,2025-09-03T21:06:52.0000-dy-c7i4xlarge-2]dy-c7i4xlarge-2] [2025-09-03T21:06:52.0000 sched: Allocate JobI,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T23:56:12.0000 POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",19,] [2025-09-03T23:56:12.0000 POWER: power_save: waking nodes]] [2025-09-03T23:56:12.0000 POWER: power_save: waking nodes]-,<*> POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,"[2025-09-03T15:42:01.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters,info,scheduler,"scheduler,config",3,"2025-09-03T15:42:01.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T15:42:01.0000 SchedulerPara","<*> SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320",scheduler_parameters
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T17:06:53.0000 POWER: Power save mode: 329 nodes,cluster_power_save_mode,info,infra,"power,save,cluster",18,[2025-09-03T17:06:53.0000 POWER: Power save mode: 329 nodes. [2025-09-03T17:06:53.0000 POWER: Power save mode: 329 nodes.,<*> POWER: Power save mode: <*> nodes,cluster_power_save_mode
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T10:10:21.0000 sched: Allocate JobId=999 NodeList=cpu-8c-std-dy-c7i4xlarge-1 #CPUs=8 Partition=cpu-8c-std,job_allocate,info,scheduler,"job,allocate,node,cpu-8c-std",13,] [2025-09-03T10:10:21.0000 sched: Allocate JobId=999-dy-c7i4xlarge-1]-dy-c7i4xlarge-1] [2025-09-03T10,<*> sched: Allocate <*> <*> #CPUs=8 Partition=cpu-8c-std,job_allocate
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T19:15:02.0000 NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster,warning,network,"network,latency,storage",2,latency to storage cluster exceeded threshold.....2025-09-03T19:15:02.0000] [2025-09-03T19:15:02.0000 NETWORK: latency to storage cluster exceeded threshold..2025-09-,<*> NETWORK: latency to storage cluster exceeded threshold,network_latency_storage_cluster
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T15:23:44.0000 AUTH: authentication failure for user admin,auth_failure,high,security,"auth,security,login_failure",7,[2025-09-03T15:23:44.0000 AUTH: authentication failure for user admin] [2025-09-03T15:23:44.0000 AUTH: authentication failure for user admin] [2025-09-03T15:23:44.0000,<*> AUTH: authentication failure for user admin,auth_failure
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T13:28:31.0000 _slurm_rpc_submit_batch_job: JobId=1025 InitPrio=30573 usec=6786,job_submit_batch,info,scheduler,"job,submit,batch",63,] [2025-09-03T13:28:31.0000 [2025-09-03T13:28:31.0000 _slurm_rpc_submit_batch_job: [2025-09-03T13:28:31.0000,[2025-09-03T13:28:31.0000 _slurm_rpc_submit_batch_job: JobId=1025 InitPrio=30573 usec=6786,job_submit_batch
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T05:06:11.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding,node_now_responding,info,infra,"node,recovered,online",8,[2025-09-03T05:06:11.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding now now]-c7i4xlarge-5] [2025-09-03T05:06,<*> Node <*> now responding,node_now_responding
828dc792-627a-47e7-8f5f-403bd06d22c0,2025-12-12T12:00:52.862975,noisy_400_logs.log,both,,[2025-09-03T23:04:11.0000 POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake,info,infra,"power,save,wakeup",19,] [2025-09-03T23:04:11.0000 POWER: power_save: waking nodes]] POWER: power_save: waking nodes] POWER: power_save: waking nodes-,<*> POWER: power_save: waking nodes cpu-8c-std-dy-c7i4xlarge-4,cluster_power_save_wake
