{
  "] [2025-09-03T02:09:33.0000 POWER: no more nodes resume for job] [2025-09-03T02:09:33.0000 POWER: no more nodes resume for job] POWER: no more nodes:": "cluster_power_save_complete",
  "latency to storage cluster exceeded threshold...].] [2025-09-03T15:09:31.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T15:09:31.0000 NETWORK": "network_latency_storage_cluster",
  "2025-09-03T15:00:57.0000] [2025-09-03T15:00:57.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-0": "scheduler_parameters",
  "[2025-09-03T11:46:13.0000 _slurm_rpc_submit_batch_job: [2025-09-03T11:46:13.0000 _slurm_rpc_submit_batch": "job_submit_batch",
  "2025-09-03T19:18:59.0000.2025-09-03T19:18:59.0000 State of 0 reservations recovered recovered.0000] [2025-09-03T19:18:59.0000 State of 0 reservations recovered recovered.": "cluster_state_recovered_no_reservations",
  "2025-09T03:47:41.0000] [2025-09-03T03:47:41.0000,bf_resolution=60,bf_window=4320]5-09-03T03:47:41.0000] [202": "scheduler_parameters",
  "[2025-09-03T21:31:32.0000 Running as primary controller.] [2025-09-03T21:31:32.0000 Running as primary controller.": "controller_primary",
  "] [2025-09-03T02:13:11.0000 POWER: no more nodes resume for job] [2025-09-03T02:13:11.0000 POWER: no more nodes resume for job] POWER: no more nodes resume for job": "cluster_power_save_complete",
  "[2025-09-03T19:57:47.0000 AUTH: authentication failure for user admin] [2025-09-03T19:57:47.0000 AUTH: authentication failure for user admin] [2025-09-03T19:57:47.0000": "auth_failure",
  "[2025-09-03T08:13:32.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding now now]-c7i4xlarge-3]-c7i4x-": "node_now_responding",
  "5-09-03T17:27:32.0000 _slurm_rpc_submit_batch_job: [2025-09-03T17:27:32.0000 _slurm_rpc_submit_batch_": "job_submit_batch",
  "cpu-8c-std-dy-c7i4xlarge-3 now responding.]-c7i4xlarge-3] [2025-09-03T17:38:06.0000 Node cpu-8c-std-d": "node_now_responding",
  "[2025-09-03T21:13:13.0000 _slurm_rpc_submit_batch_job: JobId=991 InitPrio=32548 usec=5610 _slurm_rpc_": "job_submit_batch",
  "] [2025-09-03T22:27:40.0000 POWER: no more nodes resume for job] [2025-09-03T22:27:40.0000 POWER: no more nodes resume for job]:40.0000 POWER: no": "cluster_power_save_complete",
  "[2025-09-03T13:20:13.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding. [2025-09-03T13:20:13.0000 Node cpu-8c-st": "node_now_responding",
  "[2025-09-03T01:27:18.0000 _slurm_rpc_submit_batch_job: [2025-09-03T01:27:18.0000 _slurm_rpc_submit_batch": "job_submit_batch",
  "[2025-09-03T13:15:14.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding now now]-c7i4xlarge-5] [2025-09-03T13:": "node_now_responding",
  "2025-09-03T16:53:25.0000 Gres] [2025-09-03T16:53:25.0000 Gres] [2025-09-03T16:53:25.0000 Gres] [2025-09-03T16:53:25": "gpu_inventory_update",
  "2025-09-03T13:18:20.0000 sched:-dy-c7i4xlarge-2]d-dy-c7i4xlarge-2] [2025-09-03T13:18:20.0000 sched": "job_allocate",
  "] [2025-09T21:30:46.0000 DISK: scrubber error while cleaning temp files..]..]. [2025-09T21:30:46.0000 DISK: scrubber error while cleaning temp files.05": "disk_scrubber_error",
  "2025-09-03T16:18:49.0000] [2025-09-03T16:18:49.0000 Gres [2025-09-03T16:18:49.0000 Gres [2025-09-03T16:18:49.0000 Gres": "gpu_inventory_update",
  "] [2025-09-03T00:43:40.00 DISK: scrubber error while cleaning temp files..]] [2025-09-03T00:43:40.00 DISK: scrubber error while cleaning temp files..] [202": "disk_scrubber_error",
  "latency to storage cluster exceeded threshold.. latency to storage cluster exceeded threshold..] [2025-09-03T23:08:09.0000 NETWORK: latency to storage cluster exceeded threshold.. Storage cluster exceeded threshold. Storage cluster exceeded threshold.:": "network_latency_storage_cluster",
  "2025-09-03T22:39:32.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T22:39:32.0000] [": "scheduler_parameters",
  "2025-09-03T18:34:05.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T18:34:05.0000 [202": "scheduler_parameters",
  "2025-09-03T19:46:44.0000.2025-09-03T19:46:44.0000 State of 0 reservations recovered recovered.0000] [2025-09-03T19:46:44.0000 State of 0 reservations recovered recovered.": "cluster_state_recovered_no_reservations",
  "_id=i-1985197] [2025-09-03T15:10:10.0000] [2025-09-03T15:10:10.0000]-c7i4xlarge-4-c7i4xlarge-4-c7i": "cluster_power_node_online",
  "2025-09-03T14:25:09.0000 sched:-dy-c7i4xlarge-5]dy-c7i4xlarge-5] [2025-09-03T14:25:09.0000 sched: Allocate": "job_allocate",
  "2025-09-03T16:34:22.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T16:34:22.0000 [202": "scheduler_parameters",
  "[2025-09-03T14:54:38.0000 DISK: scrubber error while cleaning temp files.] [2025-09-03T14:54:38.0000 DISK: scrubber error while cleaning temp files.": "disk_scrubber_error",
  "2025-09-03T15:11:34.0000 sched:-dy-c7i4xlarge-3]d-dy-c7i4xlarge-3] [2025-09-03T15:11:34.0000 sched": "job_allocate",
  "cpu-8c-std-dy-c7i4xlarge-2 now responding.]-c7i4xlarge-2 now responding-c7i4-c75-09-03T01:55:12.00005-09": "node_now_responding",
  "2025-09-03T06:25:23.0000 Gres] [2025-09-03T06:25:23.0000 Gres] [2025-09-03T06:25:23.0000 Gres] [2025-09-03T06:25:23": "gpu_inventory_update",
  "[2025-09-03T02:30:47.0000 _slurm_rpc_submit_batch_job: [2025-09-03T02:30:47.0000 _slurm_rpc_submit_batch_job": "job_submit_batch",
  "5-09-03T23:57:16.0000 [2025-09-03T23:57:16.0000 _slurm_rpc_submit_batch_job:]] [2025-09-03T23:57:16.": "job_submit_batch",
  "cpu=8c-std-dy-c7i4xlarge-4] [2025-09-03T04:16:20.0000]-c7i4xlarge-4]-c7i4xlarge-4]-": "cluster_power_node_online",
  "] [2025-09-03T18:45:04.0000 POWER: Power save mode: 338 nodes]2025-09-03T18:45:04.0000 POWER:: Power save mode: 338 nodes] [2025-09-03T18": "cluster_power_save_mode",
  "cpu-8c-std-dy-c7i4xlarge-3 now responding.]c7i4xlarge-3]-c7i4xlarge-3]-c7i4xlarge-3]-5-09T05": "node_now_responding",
  "2025-09-03T05:54:09.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T05:54:09.0000] [2025-0": "scheduler_parameters",
  "latency to storage cluster exceeded threshold....] [2025-09-03T09:50:03.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T09:50:03.0000 NETWORK: latency": "network_latency_storage_cluster",
  "] [2025-09-03T10:31:17.0000 sched: Allocate JobId=1027-dy-c7i4xlarge-3]-dy-c7i4xlarge-3] [2025-09-03T": "job_allocate",
  "] [2025-09-03T17:58:56.0000 POWER: power_save: waking nodes]] [2025-09-03T17:58:56.0000 POWER: power_save: waking nodes]": "cluster_power_save_wake",
  "[2025-09-03T01:48:13.0000 Running as primary controller.] [2025-09-03T01:48:13.0000 Running as primary controller.": "controller_primary",
  "latency to storage cluster exceeded threshold..].] [2025-09-03T13:51:07.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T13:51:07.0000 NETWORK: latency": "network_latency_storage_cluster",
  "latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded threshold latency to storage cluster exceeded": "network_latency_storage_cluster",
  "2025-09-03T14:27:03.0000 sched: Allocate JobId=1021-dy-c7i4xlarge-5]-dy-c7i4xlarge-5] [2025-09-03T14:27": "job_allocate",
  "[2025-09-03T03:43:14.0000 POWER: Power save mode: 330 nodes. [2025-09-03T03:43:14.0000 POWER: Power save mode: 330 nodes.": "cluster_power_save_mode",
  "[2025-09-03T03:01:27.0000 _slurm_rpc_submit_batch_job: [2025-09-03T03:01:27.0000 _slurm_rpc_submit_batch": "job_submit_batch",
  "[2025-09-03T16:49:05.0000 POWER: Power save mode: 338 nodes. [2025-09-03T16:49:05.0000 POWER: Power save mode: 338 nodes.": "cluster_power_save_mode",
  "2025-09-03T00:51:52.0000] [2025-09-03T00:51:52.0000 [2025-09-03T00:51:52.0000 [2025-09-03T00:51:52.0000] [2025-09-": "scheduler_parameters",
  "[2025-09-03T06:20:19.0000 _slurm_rpc_submit_batch_job: [2025-09-03T06:20:19.0000 _slurm_rpc_submit_batch": "job_submit_batch",
  "[2025-09-03T09:20:54.0000 Running as primary controller.] [2025-09-03T09:20:54.0000 Running as primary controller.": "controller_primary",
  "[2025-09-03T19:12:05.0000 sched: Allocate JobId=1002-dy-c7i4xlarge-4]-dy-c7i4xlarge-4] [2025-09-03T19": "job_allocate",
  "[2025-09-03T17:05:57.0000 Running as primary controller.] [2025-09-03T17:05:57.0000 Running as primary controller.": "controller_primary",
  "[2025-09-03T01:16:28.0000 State of 0 reservations recovered recovered. [2025-09-03T01:16:28.0000 State of 0 reservations recovered.": "cluster_state_recovered_no_reservations",
  "cpu-8c-std-dy-c7i4xlarge-1-dy-c7i4xlarge-1]-dy-c7i4xlarge-1] [2025-09-03T22:10:10.0000 s": "job_allocate",
  "2025-09-03T00:41:12.0000 AUTH: failed password for ubuntu] [2025-09-03T00:41:12.0000 AUTH: failed password for ubuntu]] [2025-09-03T": "auth_failure",
  "[2025-09-03T09:23:42.0000 POWER: Power save mode: 335 nodes: [2025-09-03T09:23:42.0000 POWER: [2025-09-03T09:23:42.0000 POWER: Power save mode": "cluster_power_save_mode",
  "2025-09-03T12:45:32.0000 sched:-dy-c7i4xlarge-5]dy-c7i4xlarge-5] [2025-09-03T12:45:32.0000 sched:": "job_allocate",
  "_id=i-1985197] [2025-09-03T02:06:39.0000 POWER: Node cpu-8c-std-dy-c7i4xlarge-4]]-c7i4xlarge-4]": "cluster_power_node_online",
  "] [2025-09-03T21:05:05.0000 POWER: power_save: waking nodes]]-c7i4xlarge-4] [2025-09-03T21:05:05.0000 POWER: power_s": "cluster_power_save_wake",
  "] [2025-09-03T20:48:23.0000 POWER: no more nodes resume for job] [2025-09-03T20:48:23.0000 POWER: no more nodes resume for job] POWER: no more nodes:": "cluster_power_save_complete",
  "[2025-09-03T22:23:17.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding now now]-c7i4xlarge-5] [2025-09-03T22:": "node_now_responding",
  "cpu-8c-std-dy-c7i4xlarge-1 now responding.]-c7i4xlarge-1 now responding-c7i4-c7i45-09-03T23:07:33.0000": "node_now_responding",
  "2025-09-03T00:05:10.0000 sched:-dy-c7i4xlarge-2] [2025-09-03T00:05:10.0000 sched: Allocate JobId=978] [2025-09-": "job_allocate",
  "cpu-8c-std-dy-c7i4xlarge-5 now responding now now [2025-09-03T09:49:25.0000 Node cpu-c7i4xlarge-5 now responding now-c7i": "node_now_responding",
  "[2025-09-03T13:19:03.0000 POWER: Power save mode: 331 nodes [2025-09-03T13:19:03.0000 POWER: Power save mode: 331 nodes [2025-09-03T13:19:03.0000 POWER": "cluster_power_save_mode",
  "] [2025-09-03T22:12:36.0000 POWER: no more nodes resume for job] [2025-09-03T22:12:36.0000 POWER: no more nodes resume for job]:36.0000 POWER: no": "cluster_power_save_complete",
  "[2025-09-03T22:44:40.00 DISK: /var is 97% full on node cpu-8c-std]]] [2025-09-03T22:44:40.00 DISK:]": "disk_usage_high",
  "] [2025-09-03T12:25:16.0000 POWER: no more nodes resume for job] [2025-09-03T12:25:16.0000 POWER: no more nodes resume for job POWER: no more nodes resume for job": "cluster_power_save_complete",
  "[2025-09-03T08:42:11.0000 Running as primary controller.] [2025-09-03T08:42:11.0000 Running as primary controller.": "controller_primary",
  "[2025-09-03T10:06:09.0000 State of 0 reservations recovered recovered. [2025-09-03T10:06:09.0000 State of 0 reservations recovered.": "cluster_state_recovered_no_reservations",
  "[2025-09-03T11:34:43.0000 AUTH: authentication failure for user admin] [2025-09-03T11:34:43.0000 AUTH: authentication failure for user admin] [2025-09-03T11:34:43.0000": "auth_failure",
  "[2025-09-03T13:05:52.0000 _slurm_rpc_submit_batch_job: [2025-09-03T13:05:52.0000 _slurm_rpc_submit_batch": "job_submit_batch",
  "[2025-09-03T23:07:39.0000 DISK: /var is 96% full on node cpu-8c-std] [2025-09-03T23:07:39.0000 DISK: /var is 96% full": "disk_usage_high",
  "[2025-09-03T19:51:56.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding now now]-c7i4xlarge-5] [2025-09-03T19:": "node_now_responding",
  "2025-09-03T08:29:54.0000 Gres] [2025-09-03T08:29:54.0000 Gres] [2025-09-03T08:29:54.0000 Gres] [2025-09-03T08:29:54": "gpu_inventory_update",
  "[2025-09-03T22:06:07.0000 NETWORK: latency to storage cluster exceeded threshold.] [2025-09-03T22:06:07.0000 NETWORK: latency to storage cluster exceeded threshold.": "network_latency_storage_cluster",
  "[2025-09-03T21:09:01.0000 POWER: Power save mode: 336 nodes [2025-09-03T21:09:01.0000 POWER: Power save mode: 336 nodes [2025-09-03T21:09:01.0000 POWER": "cluster_power_save_mode",
  "cpu=8c-std-dy-c7i4xlarge-4] [2025-09-03T10:43:10.0000]]-c7i4xlarge-4-x2025-09-03T10:": "cluster_power_node_online",
  "2025-09T22:03:30.0000].]..2025-09T22:03:30.0000 Running as primary controller] [2025-09-03T22:03:30.0000 [2025-09-03T22:03:30.0000": "controller_primary",
  "2025-09-03T07:46:53.0000.2025-09-03T07:46:53.0000 State of 0 reservations recovered recovered.0000] [2025-09-03T07:46:53.0000 State of 0 reservations recovered recovered.": "cluster_state_recovered_no_reservations",
  "[2025-09-03T03:14:10.0000 POWER: Power save mode: 328 nodes [2025-09-03T03:14:10.0000 POWER: Power save mode: 328 nodes [2025-09-03T03:14:10.0000 POWER": "cluster_power_save_mode",
  "..].] [2025-09T07:34:34.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T07:34:34.0000 NETWORK: latency to storage cluster exceeded threshold.": "network_latency_storage_cluster",
  "cpu-8c-std-dy-c7i4xlarge-1 now responding now now]-c7i4xlarge-1 now responding now-c7i4xlarge-1]-c7i4-5-09T15": "node_now_responding",
  "[2025-09-03T01:31:10.0000 _slurm_rpc_submit_batch_job: [2025-09-03T01:31:10.0000 _slurm_rpc_submit_batch_job": "job_submit_batch",
  "latency to storage cluster exceeded threshold.. latency to storage cluster exceeded threshold..] [2025-09-03T11:05:09.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T11:05:09.": "network_latency_storage_cluster",
  "2025-09T09:30:15.0000] [2025-09-03T09:30:15.0000 Gres Name=gpu Type=l4 Count=74]2025-09-03T09:30:15.0000]] [2025-09-": "gpu_inventory_update",
  "5-09-03T07:57:15.0000 [2025-09-03T07:57:15.0000 _slurm_rpc_submit_batch_job: [2025-09-03T07:57:15.0000 _s": "job_submit_batch",
  "5-09-03T08:24:28.0000 _slurm_rpc_submit_batch_job: [2025-09-03T08:24:28.0000 _slurm_rpc_submit_batch_": "job_submit_batch",
  "cpu-8c-std-dy-c7i4xlarge-4 now responding now now [2025-09-03T18:47:37.0000 Node cpu-c7i4xlarge-4 now responding now-c7i": "node_now_responding",
  "cpu-8c-std-dy-c7i4xlarge-5 now responding.]-c7i4xlarge-5] [2025-09T03:36:01.0000 Node cpu-8c-std-dy-": "node_now_responding",
  "2025-09-03T05:37:09.0000 Gres] [2025-09-03T05:37:09.0000 Gres] [2025-09-03T05:37:09.0000 Gres] [2025-09-03T05:37:09.0000 Gres": "gpu_inventory_update",
  "2025-09-03T21:55:32.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T21:55:32.0000] [": "scheduler_parameters",
  "[2025-09-03T16:45:36.0000 POWER: Power save mode: 339 nodes. [2025-09-03T16:45:36.0000 POWER: Power save mode: 339 nodes.": "cluster_power_save_mode",
  "[2025-09-03T21:06:33.0000 AUTH: authentication failure for user admin] [2025-09-03T21:06:33.0000 AUTH: authentication failure for user admin] [2025-09-03T21:06:33.0000": "auth_failure",
  "2025-09T00:22:55.0000] [2025-09-03T00:22:55.0000 Gres4] [2025-09-03T00:22:55.0000 Gres55]0] [2025-09-03T00:": "gpu_inventory_update",
  "[2025-09-03T11:20:48.48.0000 _slurm_rpc_submit_batch_job: [2025-09-03T11:20:48.48.0000 _slurm_rpc_sub": "job_submit_batch",
  "[2025-09-03T10:38:31.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding. [2025-09-03T10:38:31.0000 Node cpu-8c-st": "node_now_responding",
  "latency to storage cluster exceeded threshold.2025-09-03T09:13:08.0000.. latency to storage cluster exceeded threshold..2025-09-03T09:13:08.0000]5-09-03T09:13:08.": "network_latency_storage_cluster",
  "2025-09-03T10:31:3.0000 Gres] [2025-09-03T10:31:3.0000 Gres] [2025-09-03T10:31:3.0000 Gres] [2025-09-03T10:31:03.0000 Gres": "gpu_inventory_update",
  "cpu=8c-std-dy-c7i4xlarge-4] [2025-09-03T13:00:54.0000]-c7i4xlarge-4---5-09-03T13:": "cluster_power_node_online",
  "[2025-09-03T00:32:42.0000 POWER: Power save mode: 338 nodes. [2025-09-03T00:32:42.0000 POWER: Power save mode: 338 nodes.": "cluster_power_save_mode",
  "cpu=8c-std-dy-c7i4xlarge-4] [2025-09T14:49:00.0000]-c7i4xlarge-4]---5-09-03T14:49": "cluster_power_node_online",
  "latency to storage cluster exceeded threshold..] [2025-09-03T06:07:47.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T06:07:47.0000 NETWORK: latency to": "network_latency_storage_cluster",
  "2025-09-03T15:01:58.0000 sched:-dy-c7i4xlarge-3]d-dy-c7i4xlarge-3] [2025-09-03T15:01:58.0000 sched": "job_allocate",
  "[2025-09-03T22:35:25.0000 POWER: Power save mode: 331 nodes: [2025-09-03T22:35:25.0000 POWER: [2025-09-03T22:35:25.0000 POWER: Power save mode": "cluster_power_save_mode",
  "2025-09-03T05:42:34.0000 sched:-dy-c7i4xlarge-2]d-dy-c7i4xlarge-2] [2025-09-03T05:42:34.0000 sched": "job_allocate",
  "[2025-09-03T05:33:32.0000 Running as primary controller.] [2025-09-03T05:33:32.0000 Running as primary controller.": "controller_primary",
  "[2025-09-03T01:27:01.0000 AUTH: user root failed password authentication.] [2025-09-03T01:27:01.0000 AUTH: user root failed password authentication.": "auth_failure",
  "[2025-09-03T18:34:02.0000 _slurm_rpc_submit_batch_job: JobId=953 InitPrio=30192 usec=5880 [2025-09-03T18:34:02.0000": "job_submit_batch",
  "] [2025-09-03T01:31:37.0000 DISK: /var is 93% full on node cpu-8c-std]]] [2025-09-03T01:31:37.0000 DISK:": "disk_usage_high",
  "[2025-09-03T20:43:55.0000 POWER: Power save mode: 330 nodes [2025-09-03T20:43:55.0000 POWER: Power save mode: 330 nodes [2025-09-03T20:43:55.": "cluster_power_save_mode",
  "2025-09T03:06:32.0000 Gres] [2025-09-03T03:06:32.0000 Gres] [2025-09-03T03:06:32.0000 Gres] [2025-09-03T03:06:32.0000": "gpu_inventory_update",
  "2025-09-03T22:07:01.0000 Gres] [2025-09-03T22:07:01.0000 Gres] [2025-09-03T22:07:01.0000 Gres] [2025-09-03T22:07:01.0000 Gres": "gpu_inventory_update",
  "latency to storage cluster exceeded threshold.. latency to storage cluster exceeded threshold...] [2025-09-03T22:54:07.0000 NETWORK: latency to storage cluster exceeded threshold..2025-09-03T22:54": "network_latency_storage_cluster",
  "[2025-09-03T01:29:03.0000 State of 0 reservations recovered recovered. [2025-09-03T01:29:03.0000 State of 0 reservations recovered.": "cluster_state_recovered_no_reservations",
  "[2025-09-03T02:49:47.0000 _slurm_rpc_submit_batch_job: [2025-09-03T02:49:47.0000 _slurm_rpc_submit_batch": "job_submit_batch",
  "[2025-09-03T19:56:11.0000 Running as primary controller. [2025-09-03T19:56:11.0000 Running as primary controller.": "controller_primary",
  "[2025-09-03T16:53:46.0000 _slurm_rpc_submit_batch_job: [2025-09-03T16:53:46.0000 _slurm_rpc_submit_batch": "job_submit_batch",
  "2025-09-03T07:01:14.0000 sched:-dy-c7i4xlarge-4]d-dy-c7i4xlarge-4] [2025-09-03T07:01:14.0000 sched": "job_allocate",
  "] [2025-09-03T08:44:29.0000 DISK:] [2025-09-03T08:44:29.0000 DISK:]] [2025-09-03T08:44:29.0000 DISK:": "disk_usage_high",
  "[2025-09-03T14:06:26.0000 _slurm_rpc_submit_batch_job: [2025-09-03T14:06:26.0000 _slurm_rpc_submit_batch": "job_submit_batch",
  "latency to storage cluster exceeded threshold..].] [2025-09-03T11:41:58.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T11:41:58.0000.0000": "network_latency_storage_cluster",
  "[2025-09-03T03:41:25.0000 AUTH: authentication failure for user admin] [2025-09-03T03:41:25.0000 AUTH: authentication failure for user admin] [2025-09-03T03:41:25.0000": "auth_failure",
  "2025-09-03T04:26:06.0000 DISK: scrubber error while cleaning temp files..]..5-09-03T04:26:0000.5-09-03T04:26:00005-09-03T": "disk_scrubber_error",
  "cpu-8c-std-dy-c7i4xlarge-2-dy-c7i4xlarge-2]d-dy-c7i4xlarge-2] [2025-09-03T09:40:24.0000": "job_allocate",
  "[2025-09-03T05:07:59.0000 Running as primary controller.] [2025-09-03T05:07:59.0000 Running as primary controller.": "controller_primary",
  "[2025-09-03T10:27:52.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding. [2025-09-03T10:27:52.0000 Node cpu-8c-st": "node_now_responding",
  "cpu-8c-std-dy-c7i4xlarge-3-dy-c7i4xlarge-3]dy-c7i4xlarge-3] [2025-09-03T21:59:59.0000 sched": "job_allocate",
  "] [2025-09-03T13:58:22.0000 DISK:]]] [2025-09-03T13:58:22.0000 DISK: /var is 86% full on node cpu-8c-std": "disk_usage_high",
  "[2025-09-03T22:59:27.0000 POWER: Power save mode: 326 nodes: [2025-09-03T22:59:27.0000 POWER: [2025-09-03T22:59:27.0000 POWER: Power save mode": "cluster_power_save_mode",
  "[2025-09-03T13:58:39.0000 POWER: Power save mode: 322 nodes. [2025-09-03T13:58:39.0000 POWER: Power save mode: 322 nodes.": "cluster_power_save_mode",
  "2025-09-03T22:51:10.0000 sched: Allocate JobId=967-dy-c7i4xlarge-4]dy-c7i4xlarge-4] [2025-09-03T22:51:": "job_allocate",
  "[2025-09-03T09:45:43.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding now now]-c7i4xlarge-3]-c7i4x-": "node_now_responding",
  "[2025-09-03T02:29:10.0000 POWER: Power save mode: 333 nodes [2025-09-03T02:29:10.0000 POWER: Power save mode: 333 nodes [2025-09-03T02:29:10.0000 POWER": "cluster_power_save_mode",
  "[2025-09-03T06:35:42.0000 Node cpu-8c-std-dy-c7i4xlarge-2 now responding now now]-c7i4xlarge-2 now responding-c7i4-": "node_now_responding",
  "2025-09-03T06:29:02.0000 Gres] [2025-09-03T06:29:02.0000 Gres] [2025-09-03T06:29:02.0000 Gres] [2025-09-03T06:29:02.0000 Gres": "gpu_inventory_update",
  "[2025-09-03T00:52:43.0000 AUTH: authentication failure for user admin] [2025-09-03T00:52:43.0000 AUTH: authentication failure for user admin] [2025-09-03T00:52:43.0000": "auth_failure",
  "cpu-8c-std-dy-c7i4xlarge-2-dy-c7i4xlarge-2]d-dy-c7i4xlarge-2] [2025-09-03T08:45:45.0000": "job_allocate",
  "cpu-8c-std-dy-c7i4xlarge-2 now responding.]-c7i4xlarge-2 now responding-c7i4xlarge-2]-c7i4-5-09T13:22": "node_now_responding",
  "] [2025-09-03T12:55:01.0000 POWER: no more nodes resume for job] [2025-09-03T12:55:01.0000 POWER: no more nodes resume for job] POWER: no more nodes resume for job": "cluster_power_save_complete",
  "[2025-09-03T18:06:37.0000 _slurm_rpc_submit_batch_job: [2025-09-03T18:06:37.0000 _slurm_rpc_submit_batch": "job_submit_batch",
  "5-09-03T22:42:46.0000 [2025-09-03T22:42:46.0000 _slurm_rpc_submit_batch_job: [2025-09-03T22:42:46.0000 _s": "job_submit_batch",
  "cpu-8c-std-dy-c7i4xlarge-4 now responding now now]-c7i4xlarge-4] [2025-09-03T05:08:23.0000 Node cpu-c7i4": "node_now_responding",
  "] [2025-09-03T23:04:55.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding.]-c7i4xlarge-1 now responding-c7i4xlarge": "node_now_responding",
  "[2025-09-03T03:39:53.0000 POWER: Power save mode: 320 nodes. [2025-09-03T03:39:53.0000 POWER: Power save mode: 320 nodes.": "cluster_power_save_mode",
  "[2025-09-03T16:31:49.0000 POWER: Power save mode: 329 nodes: [2025-09-03T16:31:49.0000 POWER: [2025-09-03T16:31:49.0000 POWER: Power save mode": "cluster_power_save_mode",
  "2025-09-03T03:19:06.0000 sched: Allocate JobId=1012-dy-c7i4xlarge-1]-dy-c7i4xlarge-1] [2025-09-03T03:19": "job_allocate",
  "latency to storage cluster exceeded threshold...].] [2025-09-03T17:48:49.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T17:48:49.0000.": "network_latency_storage_cluster",
  "[2025-09-03T03:50:43.0000 _slurm_rpc_submit_batch_job: JobId=988 InitPrio=30962 usec=5641 _slurm_rpc_": "job_submit_batch",
  "5-09-03T07:32:44.0000 [2025-09-03T07:32:44.0000 _slurm_rpc_submit_batch_job: [2025-09-03T07:32:44.0000 _s": "job_submit_batch",
  "cpu-8c-std-dy-c7i4xlarge-2-dy-c7i4xlarge-2]dy-c7i4xlarge-2] [2025-09-03T14:06:06.0000 sched:": "job_allocate",
  "2025-09-03T18:30:16.0000] [2025-09-03T18:30:16.0000 Gres4] [2025-09-03T18:30:16.0000 Gres]] [2025-09-03T18:": "gpu_inventory_update",
  "5-09-03T15:14:40.40.0000 _slurm_rpc_submit_batch_job: [2025-09-03T15:14:40.40.0000 _slurm_rpc_submit": "job_submit_batch",
  "[2025-09-03T23:37:29.0000 Node cpu-8c-std-dy-c7i4xlarge-2 now responding now now]-c7i4xlarge-2 now responding now-c7i4x": "node_now_responding",
  "] [2025-09T07:07:34.0000 DISK: /var is 98% full on node cpu-8c-std]]] [2025-09T07:07:34.0000 DISK:]": "disk_usage_high",
  "[2025-09-03T05:11:11.0000 DISK: scrubber error while cleaning temp files.] [2025-09-03T05:11:11.0000 DISK: scrubber error while cleaning temp files.": "disk_scrubber_error",
  "[2025-09-03T16:53:03.0000 _slurm_rpc_submit_batch_job: [2025-09-03T16:53:03.0000 _slurm_rpc_submit_batch_job": "job_submit_batch",
  "[2025-09-03T04:53:28.0000 Running as primary controller.] [2025-09-03T04:53:28.0000 Running as primary controller.": "controller_primary",
  "[2025-09-03T12:14:49.0000 AUTH: user root failed password authentication.": "auth_failure",
  "cpu=8c-std-dy-c7i4xlarge-4] [2025-09-03T14:39:32.0000]]--.00005-09-03T14:39:32.0000": "cluster_power_node_online",
  "cpu 8c-std-dy-c7i4xlarge-2 now responding.]-c7i4xlarge-2 now responding-c7i4-c7i4x5-09-03T07:55:21.": "node_now_responding",
  "2025-09-03T12:08:31.0000 Gres] [2025-09-03T12:08:31.0000 Gres] [2025-09-03T12:08:31.0000 Gres] [2025-09-03T12:08:31": "gpu_inventory_update",
  "[2025-09-03T03:10:23.0000 POWER: Power save mode: 339 nodes] [2025-09-03T03:10:23.0000 POWER: Power save mode: 339 nodes] [2025-09-03T03:10:": "cluster_power_save_mode",
  "[2025-09-03T01:28:39.0000 _slurm_rpc_submit_batch_job: [2025-09-03T01:28:39.0000 _slurm_rpc_submit_batch": "job_submit_batch",
  "cpu-8c-std-dy-c7i4xlarge-1] [2025-09-03T01:12:49.0000 sched:-dy-c7i4xlarge-1]] [2025-09-03T01": "job_allocate",
  "[2025-09-03T20:18:44.0000 Running as primary controller.] [2025-09-03T20:18:44.0000 Running as primary controller.": "controller_primary",
  "cpu-8c-std-dy-c7i4xlarge-3 now responding.]-c7i4xlarge-3] [2025-09T14:03:34.0000 Node cpu-8c-std-dy": "node_now_responding",
  "[2025-09-03T00:35:17.0000 POWER: Power save mode: 333 nodes] [2025-09-03T00:35:17.0000 POWER: Power save mode: 333 nodes] [2025-09-03T00:35:": "cluster_power_save_mode",
  "2025-09-03T09:39:35.0000.2025-09-03T09:39:35.0000 State of 0 reservations recovered recovered.0] [2025-09-03T09:39:35.0000 State of 0 reservations recovered recovered.": "cluster_state_recovered_no_reservations",
  "cpu-8c-std-dy-c7i4xlarge-2 now responding.]-c7i4xlarge-2] [2025-09-03T13:07:06.0000 Node cpu-8c-std-d": "node_now_responding",
  "[2025-09-03T11:43:27.0000 POWER: Power save mode: 333 nodes [2025-09-03T11:43:27.0000 POWER: Power save mode: 333 nodes [2025-09-03T11:43:27.": "cluster_power_save_mode",
  "cpu-8c-std-dy-c7i4xlarge-2 now responding.]-c7i4xlarge-2 now responding.] [2025-09-03T13:39:36.0000-c7i4xlarge-2": "node_now_responding",
  "2025-09-03T13:57:49.0000 State of 0 reservations recovered recovered recovered recovered2025-09-03T13:57:49.0000 State of0000 State of 0 reservations recovered recovered0:57:49.00005-09-03": "cluster_state_recovered_no_reservations",
  "] [2025-09-03T15:09:13.0000 DISK: /var is 97% full on node cpu-8c-std]]]].0000] [2025-09-03T15:09:13": "disk_usage_high",
  "[2025-09-03T21:59:53.0000 AUTH: user root failed password authentication.] [2025-09-03T21:59:53.0000 AUTH: user root failed password authentication.": "auth_failure",
  "] [2025-09-03T23:35:38.0000 POWER: power_save: waking nodes]] [2025-09-03T23:35:38.0000 POWER: power_save: waking nodes]": "cluster_power_save_wake",
  "[2025-09-03T03:55:19.0000 AUTH: authentication failure for user admin] [2025-09-03T03:55:19.0000 AUTH: authentication failure for user admin] [2025-09-03T03:55:19.0000": "auth_failure",
  "cpu-8c-std-dy-c7i4xlarge-3 now responding now now]-c7i4xlarge-3] [2025-09-03T00:41:18.0000 Node cpu-c7i4": "node_now_responding",
  "[2025-09-03T04:02:49.0000 POWER: Power save mode: 339 nodes: [2025-09-03T04:02:49.0000 POWER: [2025-09-03T04:02:49.0000 POWER: Power save mode": "cluster_power_save_mode",
  "2025-09-03T18:30:30.0000 Gres] [2025-09-03T18:30:30.0000 Gres] [2025-09-03T18:30:30.0000 Gres] [2025-09-03T18:30:30": "gpu_inventory_update",
  "[2025-09-03T22:02:04.0000 _slurm_rpc_submit_batch_job: [2025-09-03T22:02:04.0000 _slurm_rpc_submit_batch_job": "job_submit_batch",
  "2025-09-03T21:26:42.0000 Gres] [2025-09-03T21:26:42.0000 Gres] [2025-09-03T21:26:42.0000 Gres] [2025-09-03T21:26:42": "gpu_inventory_update",
  "2025-09-03T14:02:57.0000 Gres] [2025-09-03T14:02:57.0000 Gres] [2025-09-03T14:02:57.0000 Gres] [2025-09-03T14:02:57": "gpu_inventory_update",
  "] [2025-09-03T17:24:32.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding..] [2025-09-03T17:24:32.0000 Node-": "node_now_responding",
  "2025-09-03T02:48:54.0000] [2025-09-03T02:48:54.0000 Gres Name=gpu Type=l4 Count=74]]] [2025-09-03T02:48:54.": "gpu_inventory_update",
  "2025-09-03T21:27:08.0000 Gres] [2025-09-03T21:27:08.0000 Gres] [2025-09-03T21:27:08.0000 Gres] [2025-09-03T21:27:08.0000 Gres": "gpu_inventory_update",
  "] [2025-09-03T06:54:17.0000 POWER: power_save: waking nodes]] [2025-09-03T06:54:17.0000 POWER: power_save: waking nodes]": "cluster_power_save_wake",
  "[2025-09-03T13:13:02.0000 Running as primary controller.] [2025-09-03T13:13:02.0000 Running as primary controller.": "controller_primary",
  "2025-09-03T12:51:29.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T12:51:29.0000] [": "scheduler_parameters",
  "[2025-09-03T15:47:22.0000 AUTH: user root failed password authentication.] [2025-09-03T15:47:22.0000 AUTH: user root failed password authentication.": "auth_failure",
  "2025-09-03T06:56:56.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T06:56:56.0000 [202": "scheduler_parameters",
  "[2025-09-03T22:18:31.0000 AUTH: authentication failure for user admin] [2025-09-03T22:18:31.0000 AUTH: authentication failure for user admin] [2025-09-03T22:18:31.0000": "auth_failure",
  "cpu=8c-std-dy-c7i4xlarge-4 powered up with instance_id=i-1985197..]]-c7i4xlarge-4-x2025-09-03T10:": "cluster_power_node_online",
  "[2025-09-03T16:50:42.0000 POWER: Power save mode: 322 nodes. [2025-09-03T16:50:42.0000 POWER: Power save mode: 322 nodes.": "cluster_power_save_mode",
  "2025-09-03T12:55:55.0000] [2025-09-03T12:55:55.0000 Gres Name=gpu Type=l4 Count=7 [2025-09-03T12:55:55.0000 Gres Name=gpu": "gpu_inventory_update",
  "5-09-03T15:29:00.0000 _slurm_rpc_submit_batch_job: [2025-09-03T15:29:00.0000 _slurm_rpc_submit_batch_": "job_submit_batch",
  "-] [2025-09-03T20:06:46.0000 _slurm_rpc_submit_batch_job:] [2025-09-03T20:06:46.0000 [2025-09-03T20:06": "job_submit_batch",
  "[2025-09-03T21:27:46.0000 POWER: Power save mode: 333 nodes [2025-09-03T21:27:46.0000 POWER: Power save mode: 333 nodes [2025-09-03T21:27:46.": "cluster_power_save_mode",
  "2025-09-03T11:10:04.0000 sched: Allocate JobId=963-dy-c7i4xlarge-3]-dy-c7i4xlarge-3] [2025-09-03T11:10": "job_allocate",
  "] [2025-09T01:07:41.0000 DISK: scrubber error while cleaning temp files..]..]. [2025-09T01:07:41.0000 DISK: scrubber error while cleaning temp files.": "disk_scrubber_error",
  "[2025-09-03T16:33:06.0000 POWER: Power save mode: 330 nodes [2025-09-03T16:33:06.0000 POWER: Power save mode: 330 nodes [2025-09-03T16:33:06.0000 POWER": "cluster_power_save_mode",
  "] [2025-09-03T11:42:35.0000 DISK:]]] [2025-09-03T11:42:35.0000 DISK:: /var is 86% full on node cpu-8c": "disk_usage_high",
  "cpu=8c-std-dy-c7i4xlarge-4] [2025-09T17:17:47.0000]-c7i4xlarge-4---5-09T17:17:47.": "cluster_power_node_online",
  "] [2025-09-03T15:06:12.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding.] [2025-09-03T15:06:12.0000-c7i4x": "node_now_responding",
  "[2025-09-03T00:41:41.0000 _slurm_rpc_submit_batch_job: JobId=979 InitPrio=31554 usec=5855 [2025-09-03T00:41:": "job_submit_batch",
  "[2025-09-03T00:22:34.0000 DISK: scrubber error while cleaning temp files. [2025-09-03T00:22:34.0000 DISK: scrubber error while cleaning temp files.": "disk_scrubber_error",
  "] [2025-09-03T00:58:58.0000 POWER: power_save: waking nodes]]-c7i4xlarge-4] [2025-09-03T00:58:58.0000 POWER: power_s": "cluster_power_save_wake",
  "-] [2025-09-03T21:04:51.0000 _slurm_rpc_submit_batch_job:] [2025-09-03T21:04:51.0000 [2025-09-03T21:04": "job_submit_batch",
  "2025-09-03T15:08:44.0000 sched:-dy-c7i4xlarge-5]d-dy-c7i4xlarge-5] [2025-09-03T15:08:44.0000 sched": "job_allocate",
  "cpu-8c-std-dy-c7i4xlarge-2 now responding now now]-c7i4xlarge-2 now responding now-c7i4xlarge-2-c7i4x-5-09T18": "node_now_responding",
  "2025-09-03T10:25:26.0000.2025-09-03T10:25:26.0000 State of 0 reservations recovered recovered.0000.-03T10:25:26.00005-09-03T10:25:": "cluster_state_recovered_no_reservations",
  "] [2025-09-03T11:31:43.0000 DISK: scrubber error while cleaning temp files..]] [2025-09-03T11:31:43.0000 DISK: scrubber error while cleaning temp files..": "disk_scrubber_error",
  "[2025-09-03T02:36:48.0000 _slurm_rpc_submit_batch_job: [2025-09-03T02:36:48.0000 _slurm_rpc_submit_batch": "job_submit_batch",
  "[2025-09-03T01:08:52.0000 _slurm_rpc_submit_batch_job: [2025-09-03T01:08:52.0000 _slurm_rpc_submit_batch": "job_submit_batch",
  "2025-09-03T17:47:31.0000 sched:-dy-c7i4xlarge-1]d-dy-c7i4xlarge-1] [2025-09-03T17:47:31.0000 sched": "job_allocate",
  "[2025-09-03T10:14:24.0000 _slurm_rpc_submit_batch_job: [2025-09-03T10:14:24.0000 _slurm_rpc_submit_batch": "job_submit_batch",
  "] [2025-09-03T19:07:07.0000 POWER: power_save: waking nodes]] [2025-09-03T19:07:07.0000 POWER:-c7i4xlarge-4] [2025-09-": "cluster_power_save_wake",
  "[2025-09-03T05:00:32.0000 POWER: Power save mode: 339 nodes. [2025-09-03T05:00:32.0000 POWER: Power save mode: 339 nodes.": "cluster_power_save_mode",
  "--] [2025-09-03T14:42:05.0000 [2025-09-03T14:42:05.0000 [2025-09-03T14:42:05.0000 _slurm_rpc_submit_b": "job_submit_batch",
  "[2025-09-03T22:51:19.0000 Running as primary controller.] [2025-09-03T22:51:19.0000 Running as primary controller.": "controller_primary",
  "2025-09-03T02:08:43.0000.2025-09-03T02:08:43.0000 State of0000] [2025-09-03T02:08:43.0000 State of 0 reservations recovered recovered.5-09-03T": "cluster_state_recovered_no_reservations",
  "latency to storage cluster exceeded threshold.....2025-09-03T10:47:02.0000 NETWORK: latency to storage cluster exceeded threshold..2025-09-03T10:47:02.00005-09-03T": "network_latency_storage_cluster",
  "[2025-09-03T23:04:11.0000 NETWORK: latency to storage cluster exceeded threshold.] [2025-09-03T23:04:11.0000 NETWORK: latency to storage cluster exceeded threshold.": "network_latency_storage_cluster",
  "cpu-8c-std-dy-c7i4xlarge-3]-dy-c7i4xlarge-3]d-dy-c7i4xlarge-3] [2025-09-03T20:34:53.0000": "job_allocate",
  "[2025-09-03T01:47:24.0000 DISK: scrubber error while cleaning temp files.] [2025-09-03T01:47:24.0000 DISK: scrubber error while cleaning temp files.": "disk_scrubber_error",
  "cpu-8c-std-dy-c7i4xlarge-1-dy-c7i4xlarge-1]-dy-c7i4xlarge-1] [2025-09-03T03:18:22.0000 s": "job_allocate",
  "latency to storage cluster exceeded threshold..].. latency to storage cluster exceeded threshold..2025-09-03T08:12:32.0000] [2025-09-03T08:12:32.00005-09-03T": "network_latency_storage_cluster",
  "2025-09-03T19:25:09.0000 AUTH: failed password for ubuntu] [2025-09-03T19:25:09.0000 AUTH: failed password for ubuntu]] [2025-09-03T": "auth_failure",
  "[2025-09-03T12:25:45.0000 POWER: Power save mode: 322 nodes [2025-09-03T12:25:45.0000 POWER: Power save mode: 322 nodes [2025-09-03T12:25:45.": "cluster_power_save_mode",
  "to storage cluster exceeded threshold.. latency to storage cluster exceeded threshold... latency to storage cluster exceeded threshold.2025-09T01:38:04.0000 NETWORK: latency to storage cluster exceeded threshold..2025-09T01:38": "network_latency_storage_cluster",
  "[2025-09-03T18:01:24.0000 POWER: Power save mode: 323 nodes: [2025-09-03T18:01:24.0000 POWER: [2025-09-03T18:01:24.0000 POWER: Power save mode": "cluster_power_save_mode",
  "] [2025-09-03T02:53:30.0000 POWER: no more nodes resume for job]]5-09-03T02:53:30.0000 POWER: no more nodes resume for job JobId=986] [2025-09-": "cluster_power_save_complete",
  "[2025-09-03T09:42:15.0000 POWER: Power save mode: 337 nodes. [2025-09-03T09:42:15.0000 POWER: Power save mode: 337 nodes.": "cluster_power_save_mode",
  "2025-09-03T18:28:19.0000 State of 0 reservations recovered recovered recovered recovered2025-09-03T18:28:19.0000 State of 0 reservations recovered recovered0000 State of 0 reservations recovered recovered]-03T18:28": "cluster_state_recovered_no_reservations",
  "cpu-8c-std-dy-c7i4xlarge-3 #CPUs=8-dy-c7i4xlarge-3]d-dy-c7i4xlarge-3] [2025-09-03T18:": "job_allocate",
  "[2025-09-03T22:23:18.0000 POWER: Power save mode: 329 nodes. [2025-09-03T22:23:18.0000 POWER: Power save mode: 329 nodes.": "cluster_power_save_mode",
  "cpu-8c-std-dy-c7i4xlarge-3 now responding.]-c7i4xlarge-3] [2025-09T03:02:09.0000 Node cpu-8c-std-dy-": "node_now_responding",
  "cpu=8c-std-dy-c7i4xlarge-4]].0000] [2025-09-03T13:22:53.0000--.0000] [2025-09-03T13:22:": "cluster_power_node_online",
  "cpu-8c-std-dy-c7i4xlarge-3-dy-c7i4xlarge-3]dy-c7i4xlarge-3] [2025-09-03T21:48:18.0000 sched": "job_allocate",
  "[2025-09-03T00:52:00.0000 POWER: Power save mode: 332 nodes: [2025-09-03T00:52:00.0000 POWER: [2025-09-03T00:52:00.0000 POWER: Power save mode: 332": "cluster_power_save_mode",
  "2025-09-03T06:17:21.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T06:17:21.0000] [": "scheduler_parameters",
  "2025-09T00:06:48.0000] [2025-09-03T00:06:48.0000 Gres Name=gpu Type=l4 Count=8 Gres Name=gpu] [2025-09-03T00:06:48.0000 Gre": "gpu_inventory_update",
  "] [2025-09-03T16:17:23.0000 POWER: power_save: waking nodes]] [2025-09-03T16:17:23.0000 POWER: power_save: waking nodes]": "cluster_power_save_wake",
  "2025-09-03T07:46:34.0000 Gres] [2025-09-03T07:46:34.0000 Gres] [2025-09-03T07:46:34.0000 Gres] [2025-09-03T07:46:34": "gpu_inventory_update",
  "[2025-09-03T11:52:42.0000 _slurm_rpc_submit_batch_job: [2025-09-03T11:52:42.0000 _slurm_rpc_submit_batch": "job_submit_batch",
  "2025-09-03T14:46:29.0000 Gres] [2025-09-03T14:46:29.0000 Gres] [2025-09-03T14:46:29.0000 Gres] [2025-09-03T14:46:29": "gpu_inventory_update",
  "2025-09-03T11:29:15.0000.2025-09-03T11:29:15.00000000] [2025-09-03T11:29:15.0000 State of 0 reservations recovered recovered.]:29:15.": "cluster_state_recovered_no_reservations",
  "2025-09-03T21:31:33.0000.2025-09-03T21:31:33.0000 State of 0 reservations recovered recovered.0000] [2025-09-03T21:31:33.0000 State of 0 reservations recovered recovered.": "cluster_state_recovered_no_reservations",
  "5-09-03T15:41:40.00 _slurm_rpc_submit_batch_job: [2025-09-03T15:41:40.00 _slurm_rpc_submit_batch_job:": "job_submit_batch",
  "[2025-09-03T14:15:05.0000 POWER: Power save mode: 335 nodes [2025-09-03T14:15:05.0000 POWER: Power save mode: 335 nodes [2025-09-03T14:15:05.": "cluster_power_save_mode",
  "2025-09-03T18:13:13.0000] [2025-09-03T18:13:13.0000 [2025-09-03T18:13:13.0000 SchedulerParameters=preempt_youngest_first,bf_re": "scheduler_parameters",
  "[2025-09-03T03:00:54.0000 DISK: scrubber error while cleaning temp files.] [2025-09-03T03:00:54.0000 DISK: scrubber error while cleaning temp files.": "disk_scrubber_error",
  "5-09-03T10:29:37.0000 _slurm_rpc_submit_batch_job:] [2025-09-03T10:29:37.0000 _slurm_rpc_submit_job": "job_submit_batch",
  "[2025-09-03T05:21:26.0000 POWER: Power save mode: 337 nodes: [2025-09-03T05:21:26.0000 POWER: [2025-09-03T05:21:26.0000 POWER: Power save mode": "cluster_power_save_mode",
  "[2025-09-03T01:23:28.0000 _slurm_rpc_submit_batch_job: [2025-09-03T01:23:28.0000 _slurm_rpc_submit_batch": "job_submit_batch",
  "[2025-09-03T06:58:29.0000 Running as primary controller.] [2025-09-03T06:58:29.0000 Running as primary controller.": "controller_primary",
  "[2025-09-03T00:43:48.0000 Running as primary controller.] [2025-09-03T00:43:48.0000 Running as primary controller.": "controller_primary",
  "std-dy-c7i4xlarge-2-dy-c7i4xlarge-2]-dy-c7i4xlarge-2] [2025-09-03T19:40:03.0000 sched: Allocate Job": "job_allocate",
  "[2025-09-03T21:19:31.0000 POWER: Power save mode: 338 nodes. [2025-09-03T21:19:31.0000 POWER: Power save mode: 338 nodes.": "cluster_power_save_mode",
  "[2025-09-03T22:24:43.0000 POWER: Power save mode: 323 nodes: [2025-09-03T22:24:43.0000 POWER: [2025-09-03T22:24:43.0000 POWER: Power save mode": "cluster_power_save_mode",
  "2025-09-03T11:29:34.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T11:29:34.0000 [202": "scheduler_parameters",
  "cpu-8c-std-dy-c7i4xlarge-3 now responding.]-c7i4xlarge-3] [2025-09-03T12:58:04.0000 Node cpu-8c-std-d": "node_now_responding",
  "[2025-09-03T10:06:01.0000 POWER: Power save mode: 331 nodes [2025-09-03T10:06:01.0000 POWER: Power save mode: 331 nodes [2025-09-03T10:06:01.0000 POWER": "cluster_power_save_mode",
  "2025-09-03T05:04:53.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T05:04:53.0000] [": "scheduler_parameters",
  "5-09-03T18:10:10.0000 _slurm_rpc_submit_batch_job: [2025-09-03T18:10:10.0000 _slurm_rpc_submit_batch_job": "job_submit_batch",
  "[2025-09-03T21:10:29.0000 AUTH: authentication failure for user admin] [2025-09-03T21:10:29.0000 AUTH: authentication failure for user admin] [2025-09-03T21:10:29.0000": "auth_failure",
  "[2025-09-03T23:42:06.0000 State of 0 reservations recovered. [2025-09-03T23:42:06.0000 State of 0 reservations recovered.": "cluster_state_recovered_no_reservations",
  "] [2025-09-03T23:46:31.0000 POWER: no more nodes resume for job] [2025-09-03T23:46:31.0000 POWER: no more nodes resume for job] POWER: no more nodes:": "cluster_power_save_complete",
  "[2025-09-03T23:15:08.0000 _slurm_rpc_submit_batch_job: JobId=972 InitPrio=32933 usec=6989] [2025-09-03T23:15:": "job_submit_batch",
  "[2025-09-03T10:58:32.0000 DISK: scrubber error while cleaning temp files.] [2025-09-03T10:58:32.0000 DISK: scrubber error while cleaning temp files.": "disk_scrubber_error",
  "cpu=8c-std-dy-c7i4xlarge-4.]]-c7i4xlarge-4]-.00005-09-03T05:19:35.0000 POWER: Node:": "cluster_power_node_online",
  "cpu-8c-std-dy-c7i4xlarge-1-dy-c7i4xlarge-1]d-dy-c7i4xlarge-1] [2025-09-03T05:25:16.0000": "job_allocate",
  "[2025-09-03T02:29:18.0000 POWER: Power save mode: 335 nodes. [2025-09-03T02:29:18.0000 POWER: Power save mode: 335 nodes.": "cluster_power_save_mode",
  "] [2025-09-03T16:28:28.0000 POWER: Power save mode: 325 nodes]] [2025-09-03T16:28:28.0000 POWER: Power save mode: 325 nodes] [2025-09-03": "cluster_power_save_mode",
  "2025-09T03:27:06.0000 Gres] [2025-09-03T03:27:06.0000 Gres] [2025-09-03T03:27:06.0000 Gres] [2025-09-03T03:27:06.0000 Gres] [": "gpu_inventory_update",
  "[2025-09-03T08:10:00.0000 POWER: Power save mode: 324 nodes [2025-09-03T08:10:00.0000 POWER: Power save mode: 324 nodes [2025-09-03T08:10:00.0000 POWER": "cluster_power_save_mode",
  "[2025-09-03T12:16:05.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding now now]-c7i4xlarge-1 now responding now-c7i4x": "node_now_responding",
  "2025-09-03T04:17:39.0000.2025-09-03T04:17:39.00000000] [2025-09-03T04:17:39.0000 [2025-09-03T04:17:39.0000 State of": "cluster_state_recovered_no_reservations",
  "[2025-09-03T22:00:44.0000 Node cpu-8c-std-dy-c7i4xlarge-1 now responding. [2025-09-03T22:00:44.0000 Node cpu-8c-std-": "node_now_responding",
  "to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage cluster exceeded threshold to storage": "network_latency_storage_cluster",
  "cpu-8c-std-dy-c7i4xlarge-3] [2025-09-03T22:03:29.0000 sched: Allocate JobId=1013-dy-c7i4xlarge-3]": "job_allocate",
  "cpu-8c-std-dy-c7i4xlarge-1 now responding.]-c7i4xlarge-1 now responding-c7i4xlarge-1]-c7i4x5-09-03T20:": "node_now_responding",
  "2025-09-03T06:33:13.0000 Gres] [2025-09-03T06:33:13.0000 Gres] [2025-09-03T06:33:13.0000 Gres] [2025-09-03T06:33:13": "gpu_inventory_update",
  "[2025-09-03T16:49:02.0000 AUTH: authentication failure for user admin] [2025-09-03T16:49:02.0000 AUTH: authentication failure for user admin] [2025-09-03T16:49:02.0000 AUTH:": "auth_failure",
  "[2025-09-03T15:19:45.0000 AUTH: user root failed password authentication.] [2025-09-03T15:19:45.0000 AUTH: user root failed password authentication.": "auth_failure",
  "cpu-8c-std-dy-c7i4xlarge-4 now responding..c7i4xlarge-4 now responding.c7i4xlarge-4 now responding.-c7i4xlarge-4-c7i4": "node_now_responding",
  "] [2025-09-03T04:19:35.0000 AUTH: failed password for ubuntu.]. [2025-09-03T04:19:35.0000 AUTH: failed password for ubuntu..": "auth_failure",
  "[2025-09-03T17:21:20.0000 POWER: Power save mode: 335 nodes [2025-09-03T17:21:20.0000 POWER: Power save mode: 335 nodes [2025-09-03T17:21:20.": "cluster_power_save_mode",
  "latency to storage cluster exceeded threshold..].] [2025-09-03T16:42:53.0000 [2025-09-03T16:42:53.0000] [2025-09-03T16:42:53.0000.]": "network_latency_storage_cluster",
  "2025-09-03T05:34:28.0000 sched:-dy-c7i4xlarge-2]d-dy-c7i4xlarge-2] [2025-09-03T05:34:28.0000 sched": "job_allocate",
  "5-09-03T20:18:25.0000 [2025-09-03T20:18:25.0000 _slurm_rpc_submit_batch_job: [2025-09-03T20:18:25.0000 _s": "job_submit_batch",
  "[2025-09-03T01:53:48.0000 AUTH: authentication failure for user admin] [2025-09-03T01:53:48.0000 AUTH: authentication failure for user admin] [2025-09-03T01:53:48.0000": "auth_failure",
  "[2025-09-03T02:58:35.0000 POWER: Power save mode: 323 nodes: [2025-09-03T02:58:35.0000 POWER: [2025-09-03T02:58:35.0000 POWER: Power save mode": "cluster_power_save_mode",
  "latency to storage cluster exceeded threshold...].] [2025-09-03T17:50:44.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T17:50:44.0000 NETWORK": "network_latency_storage_cluster",
  "] [2025-09-03T06:18:38.0000 POWER: power_save: waking nodes]] [2025-09-03T06:18:38.0000 POWER: power_save: waking nodes]": "cluster_power_save_wake",
  "[2025-09-03T01:21:25.0000 Running as primary controller.] [2025-09-03T01:21:25.0000 Running as primary controller.": "controller_primary",
  "[2025-09-03T05:20:06.0000 DISK: scrubber error while cleaning temp files.] [2025-09-03T05:20:06.0000 DISK: scrubber error while cleaning temp files.": "disk_scrubber_error",
  "] [2025-09-03T14:39:39.0000 _slurm_rpc_submit_batch_job: [2025-09-03T14:39:39.0000 _slurm_rpc_submit_": "job_submit_batch",
  "[2025-09-03T11:01:49.0000 POWER: Power save mode: 323 nodes: [2025-09-03T11:01:49.0000 POWER: [2025-09-03T11:01:49.0000 POWER: Power save mode": "cluster_power_save_mode",
  "[2025-09-03T04:25:43.0000 POWER: Power save mode: 320 nodes. [2025-09-03T04:25:43.0000 POWER: [2025-09-03T04:25:43.0000 POWER: Power save mode": "cluster_power_save_mode",
  "2025-09-03T20:47:56.0000] [2025-09-03T20:47:56.0000 Gres4]]] [2025-09-03T20:47:56.0000 Gres=gpu] [2025-0": "gpu_inventory_update",
  "2025-09-03T19:22:47.0000.2025-09-03T19:22:47.0000 State of 0 reservations recovered recovered.0000] [2025-09-03T19:22:47.0000 State of 0 reservations recovered recovered.": "cluster_state_recovered_no_reservations",
  "cpu=8c-std-dy-c7i4xlarge-4]]]-c7i4xlarge-4]-.00005-09-03T10:51:33.00005-09-03T10": "cluster_power_node_online",
  "[2025-09-03T09:51:25.0000 Node cpu-8c-std-dy-c7i4xlarge-4 now responding. [2025-09-03T09:51:25.0000 Node cpu-8c-st": "node_now_responding",
  "cpu=8c-std-dy-c7i4xlarge-4.]]-c7i4xlarge-4]-i42025-09-03T15:24:01.0000]5:005-09-": "cluster_power_node_online",
  "5-09-03T11:04:36.0000 [2025-09-03T11:04:36.0000 _slurm_rpc_submit_batch_job: [2025-09-03T11:04:36.0000 _s": "job_submit_batch",
  ".]..].] [2025-09T13:50:13.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T13:50:13.0000 NETWORK:].] [202": "network_latency_storage_cluster",
  "latency to storage cluster exceeded threshold...].] [2025-09-03T09:57:45.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T09:57:45.0000 NETWORK": "network_latency_storage_cluster",
  "...].] [2025-09-03T20:27:35.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T20:27:35.0000 NETWORK: latency to storage cluster exceeded": "network_latency_storage_cluster",
  "2025-09-03T09:22:20.0000.2025-09-03T09:22:20.0000 State of 0 reservations recovered recovered.0000] [2025-09-03T09:22:20.0000 State of 0 reservations recovered recovered.": "cluster_state_recovered_no_reservations",
  "2025-09-03T20:58:12.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T20:58:12.0000 [2025-09": "scheduler_parameters",
  "[2025-09-03T18:19:30.0000 State of 0 reservations recovered recovered. [2025-09-03T18:19:30.0000 State of 0 reservations recovered.": "cluster_state_recovered_no_reservations",
  "[2025-09-03T06:37:04.0000 POWER: Power save mode: 340 nodes [2025-09-03T06:37:04.0000 POWER: Power save mode: 340 nodes [2025-09-03T06:37:04.0000 POWER": "cluster_power_save_mode",
  "[2025-09-03T23:22:29.0000 Running as primary controller.] [2025-09-03T23:22:29.0000 Running as primary controller.": "controller_primary",
  "2025-09-03T05:12:23.0000 State of 0 reservations recovered recovered recovered recovered2025-09-03T05:12:23.0000 State of 0 reservations recovered recovered recovered0000]-03T05:12:23.00005-0": "cluster_state_recovered_no_reservations",
  "[2025-09-03T13:48:22.0000 AUTH: authentication failure for user admin] [2025-09-03T13:48:22.0000 AUTH: authentication failure for user admin] [2025-09-03T13:48:22.0000": "auth_failure",
  "2025-09-03T05:37:02.0000 sched:-dy-c7i4xlarge-2]dy-c7i4xlarge-2] [2025-09-03T05:37:02.0000 sched: Allocate": "job_allocate",
  "2025-09-03T05:45:09.0000 sched: Allocate JobId=979-dy-c7i4xlarge-4]c7i4xlarge-4] [2025-09-03T05:45:09.0000": "job_allocate",
  "[2025-09-03T09:17:02.0000 _slurm_rpc_submit_batch_job: [2025-09-03T09:17:02.0000 _slurm_rpc_submit_batch_job": "job_submit_batch",
  "2025-09-03T12:08:19.0000 Gres] [2025-09-03T12:08:19.0000 Gres] [2025-09-03T12:08:19.0000 Gres] [2025-09-03T12:08:19": "gpu_inventory_update",
  "2025-09T08:03:33.0000] [2025-09-03T08:03:33.0000 Gres Name=gpu Type=l4 Count=74]2025-09T08:03:33.0000 Gres]202": "gpu_inventory_update",
  "node cpu=8c-std-dy-c7i4xlarge-4]]] POWER: Node cpu-8c-std-dy-c7i4xlarge-4--.0000": "cluster_power_node_online",
  "2025-09-03T10:58:42.0000-dy-c7i4xlarge-3]dy-c7i4xlarge-3] [2025-09-03T10:58:42.0000 sched: Allocate JobI": "job_allocate",
  "] [2025-09-03T05:59:34.0000 POWER: no more nodes resume for job] [2025-09-03T05:59:34.0000 POWER: no more nodes resume for job]:59:34.0000 POWER": "cluster_power_save_complete",
  "2025-09-03T06:22:38.0000 Gres] [2025-09-03T06:22:38.0000 Gres] [2025-09-03T06:22:38.0000 Gres] [2025-09-03T06:22:38": "gpu_inventory_update",
  "cpu-8c-std-dy-c7i4xlarge-5-dy-c7i4xlarge-5]dy-c7i4xlarge-5] [2025-09-03T11:56:30.0000 sched": "job_allocate",
  "2025-09-03T20:02:06.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T20:02:06.0000] [2025-0": "scheduler_parameters",
  "[2025-09-03T01:23:06.0000 NETWORK: latency to storage cluster exceeded threshold. [2025-09-03T01:23:06.0000 NETWORK: latency to storage cluster exceeded threshold.": "network_latency_storage_cluster",
  "latency to storage cluster exceeded threshold..].] [2025-09-03T04:55:53.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T04:55:53.0000 to storage cluster": "network_latency_storage_cluster",
  "latency to storage cluster exceeded threshold..].] [2025-09-03T05:08:16.0000 [2025-09-03T05:08:16.0000 NETWORK: latency to storage cluster exceeded threshold..].0000": "network_latency_storage_cluster",
  "] [2025-09-03T16:54:41.0000 DISK:] [2025-09-03T16:54:41.0000 DISK:]] [2025-09-03T16:54:41.0000 DISK:": "disk_usage_high",
  "[2025-09-03T00:34:37.0000 AUTH: user root failed password authentication.": "auth_failure",
  "2025-09-03T16:18:04.0000 AUTH: user root failed password authentication..: user root failed password authentication]] [2025-09-03T16:18:04.0000 AUTH: user root failed password authentication..2025-0": "auth_failure",
  "[2025-09-03T01:31:19.0000 AUTH: authentication failure for user admin] [2025-09-03T01:31:19.0000 AUTH: authentication failure for user admin] [2025-09-03T01:31:19.0000": "auth_failure",
  "[2025-09-03T06:56:09.0000 POWER: Power save mode: 339 nodes [2025-09-03T06:56:09.0000 POWER: Power save mode: 339 nodes] [2025-09-03T06:56:09.0000": "cluster_power_save_mode",
  "std-dy-c7i4xlarge-3-dy-c7i4xlarge-3]d-dy-c7i4xlarge-3-std-dy-c7i4xlarge-3]cpu": "job_allocate",
  "[2025-09-03T08:12:04.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding.]-c7i4xlarge-5] [2025-09-03T08:12:": "node_now_responding",
  "2025-09-03T14:27:13.0000 Gres] [2025-09-03T14:27:13.0000 Gres] [2025-09-03T14:27:13.0000 Gres] [2025-09-03T14:27:13": "gpu_inventory_update",
  "2025-09-03T13:33:35.0000.2025-09-03T13:33:35.0000 State of 0 reservations recovered recovered.0000] [2025-09-03T13:33:35.0000 State of 0 reservations recovered recovered.": "cluster_state_recovered_no_reservations",
  "cpu-8c-std-dy-c7i4xlarge-4-dy-c7i4xlarge-4]-dy-c7i4xlarge-4] [2025-09-03T03:12:32.0000 s": "job_allocate",
  "] [2025-09-03T05:02:10.0000 DISK: /var is 89% full on node cpu-8c-std]]] [2025-09-03T05:02:10.0000 DISK:]": "disk_usage_high",
  "[2025-09-03T19:21:53.0000 Node cpu-8c-std-dy-c7i4xlarge-2 now responding.]-c7i4xlarge-2 now responding-c7i4xlarge-2": "node_now_responding",
  "2025-09-03T08:18:14.0000.2025-09-03T08:18:14.0000 State of 0 reservations recovered recovered.0000] [2025-09-03T08:18:14.0000 State of 0 reservations recovered recovered.": "cluster_state_recovered_no_reservations",
  "2025-09-03T19:02:54.0000 sched:-dy-c7i4xlarge-3]d-dy-c7i4xlarge-3] [2025-09-03T19:02:54.0000 sched": "job_allocate",
  "2025-09-03T13:57:28.0000 Gres] [2025-09-03T13:57:28.0000 Gres] [2025-09-03T13:57:28.0000 Gres] [2025-09-03T13:57:28": "gpu_inventory_update",
  "cpu-8c-std-dy-c7i4xlarge-2-dy-c7i4xlarge-2]-dy-c7i4xlarge-2] [2025-09-03T03:38:25.0000 s": "job_allocate",
  "[2025-09-03T09:53:15.0000 AUTH: authentication failure for user admin] [2025-09-03T09:53:15.0000 AUTH: authentication failure for user admin] [2025-09-03T09:53:15.0000": "auth_failure",
  "cpu-8c-std-dy-c7i4xlarge-1 now responding now now]-c7i4xlarge-1 now responding now-c7i4xlarge-1]-c7i4xlarge-1-5-0": "node_now_responding",
  "cpu=8c-std-dy-c7i4xlarge-4.]]-c7i4xlarge-4]-i4xlarge-42025-09-03T21:22:08.0000] [5-0": "cluster_power_node_online",
  "[2025-09-03T06:40:40.0000 _slurm_rpc_submit_batch_job: JobId=982 InitPrio=30833 usec=6290 [2025-09-03T06:40:": "job_submit_batch",
  "2025-09-03T15:29:03.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T15:29:03.0000 SchedulerPara": "scheduler_parameters",
  "2025-09-03T17:13:55.0000 State of.2025-09-03T17:13:55.0000 State of 0 reservations recovered recovered.0000] [2025-09-03T17:13:55.0000 State of 0 reservations recovered": "cluster_state_recovered_no_reservations",
  "] [2025-09T01:52:53.0000 DISK: scrubber error while cleaning temp files..]] [2025-09-03T01:52:53.0000 DISK: scrubber error while cleaning temp files..]": "disk_scrubber_error",
  "[2025-09-03T18:45:37.0000 AUTH: authentication failure for user admin] [2025-09-03T18:45:37.0000 AUTH: authentication failure for user admin] [2025-09-03T18:45:37.0000": "auth_failure",
  "[2025-09-03T14:55:01.0000 _slurm_rpc_submit_batch_job: JobId=963 InitPrio=31283 usec=6299 [2025-09-03T14:55:01.": "job_submit_batch",
  "2025-09-03T18:06:06.0000 Gres] [2025-09-03T18:06:05.0000 Gres] [2025-09-03T18:06:06.0000 Gres] [2025-09-03T18:06:05.0000": "gpu_inventory_update",
  "[2025-09-03T00:51:55.0000 POWER: Power save mode: 325 nodes: [2025-09-03T00:51:55.0000 POWER: [2025-09-03T00:51:55.0000 POWER: Power save mode": "cluster_power_save_mode",
  "[2025-09-03T17:41:32.0000 Node cpu-8c-std-dy-c7i4xlarge-3 now responding now now]-c7i4xlarge-3]-c7i4x-": "node_now_responding",
  "2025-09-03T08:52:35.0000 sched:-dy-c7i4xlarge-5]d-dy-c7i4xlarge-5] [2025-09-03T08:52:35.0000 sched": "job_allocate",
  "2025-09-03T04:24:19.0000 State of 0 reservations recovered recovered recovered recovered2025-09-03T04:24:19.0000 State of0000] [2025-09-03T04:24:19.0000 State of 0 reservations recovered": "cluster_state_recovered_no_reservations",
  "2025-09-03T16:00:00.0000] [2025-09-03T16:00:00.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320,bf_": "scheduler_parameters",
  "[2025-09-03T20:05:42.0000 sched: Allocate JobId=966-dy-c7i4xlarge-3]d-dy-c7i4xlarge-3] [2025-09-03T": "job_allocate",
  "/var is 86% full on node cpu-8c-std] [2025-09-03T21:35:04.0000 DISK:] /var is 86% full on node cpu-8c-std]": "disk_usage_high",
  "latency to storage cluster exceeded threshold..].] [2025-09-03T17:51:05.0000 NETWORK: latency to storage cluster exceeded threshold..] [2025-09-03T17:51:05.0000 NETWORK:": "network_latency_storage_cluster",
  "2025-09-03T21:06:52.0000-dy-c7i4xlarge-2]dy-c7i4xlarge-2] [2025-09-03T21:06:52.0000 sched: Allocate JobI": "job_allocate",
  "] [2025-09-03T23:56:12.0000 POWER: power_save: waking nodes]] [2025-09-03T23:56:12.0000 POWER: power_save: waking nodes]-": "cluster_power_save_wake",
  "2025-09-03T15:42:01.0000 SchedulerParameters=preempt_youngest_first,bf_resolution=60,bf_window=4320] [2025-09-03T15:42:01.0000 SchedulerPara": "scheduler_parameters",
  "[2025-09-03T17:06:53.0000 POWER: Power save mode: 329 nodes. [2025-09-03T17:06:53.0000 POWER: Power save mode: 329 nodes.": "cluster_power_save_mode",
  "] [2025-09-03T10:10:21.0000 sched: Allocate JobId=999-dy-c7i4xlarge-1]-dy-c7i4xlarge-1] [2025-09-03T10": "job_allocate",
  "latency to storage cluster exceeded threshold.....2025-09-03T19:15:02.0000] [2025-09-03T19:15:02.0000 NETWORK: latency to storage cluster exceeded threshold..2025-09-": "network_latency_storage_cluster",
  "[2025-09-03T15:23:44.0000 AUTH: authentication failure for user admin] [2025-09-03T15:23:44.0000 AUTH: authentication failure for user admin] [2025-09-03T15:23:44.0000": "auth_failure",
  "] [2025-09-03T13:28:31.0000 [2025-09-03T13:28:31.0000 _slurm_rpc_submit_batch_job: [2025-09-03T13:28:31.0000": "job_submit_batch",
  "[2025-09-03T05:06:11.0000 Node cpu-8c-std-dy-c7i4xlarge-5 now responding now now]-c7i4xlarge-5] [2025-09-03T05:06": "node_now_responding",
  "] [2025-09-03T23:04:11.0000 POWER: power_save: waking nodes]] POWER: power_save: waking nodes] POWER: power_save: waking nodes-": "cluster_power_save_wake",
  "POWER: no more nodes to resume for job [2025-09-03T02:09:33.0000] POWER: no more nodes to resume for job JobId=986 [2025-09-03T02:09:33.0000] POWER:": "cluster_power_save_complete",
  ". [2025-09-03T15:09:31.0000] NETWORK:... [2025-09-03T15:09:31.0000] [2025-09-03T15:09:31.0000] [2025-09-03": "network_latency_storage_cluster",
  "[2025-09-03T15:00:00.0000],bf_resolution=60,bf_window=4320 [2025-09-03T15:00:03.0000]] [2025-09-03T15:00:03.0000] [": "scheduler_parameters",
  "[2025-09-03T11:46:13.0000] _slurm_rpc_submit_batch_job: [2025-09-03T11:46:13.0000] _slurm_rpc_submit_": "job_submit_batch",
  "[2025-09-03T09:38:11.413] _slurm_rpc_submit_batch_job: JobId=958 InitPrio=35869 usec=6139 [2025-09-03T09:38": "job_submit_batch",
  "[2025-09-03T09:38:12.000] sched: [2025-09-03T09:38:12.000] sched: [2025-09-03T09:38:12.000] sched: [2025-09-03T09": "job_allocate",
  "[2025-09-03T09:38:42.013] POWER: no more nodes to resume for job JobId=958 JobId=958 JobId=958 JobId=958 JobId=958 JobId=95": "cluster_power_save_complete",
  "cpu_save: waking nodes cpu-std-dy-c7i4xlarge-1_save: waking nodes-dy-c7i4xlarge-1--- POWER": "cluster_power_save_wake",
  "-c7i4xlarge-1 now responding. [2025-09-03T09:42:37.720]. [2025-09-03T09:42:37.720]--c7i4xlarge-1 now responding. [2025-09-": "node_now_responding",
  "-1/192.168.13.70-1 and node_name=c7i.4xlarge-1. [2025-09-03T09:42:37.720] POWER: Node0] [2025-09-03T09:42:37.720": "cluster_power_node_online",
  "[2025-09-03T09:42:01.001] job_time_limit: Configuration for JobId=958 complete [2025-09-03T09:42:02.001] job_time_limit: Configuration for JobId=958 complete [2025-0": "job_time_limit_configured",
  "[2025-09-03T10:50:31.527] POWER: Power save mode: 333 nodes [2025-09-03T10:50:31.527] POWER: Power save mode: 333 nodes [2025-09-03T10:50:": "cluster_power_save_mode",
  "[2025-09-03T11:00:51.602] [POWER: Power save mode: 333 nodes:::: [2025-09-03T11:00:51.602] POWER: Power save mode: 333 nodes: 333 nodes": "cluster_power_save_mode",
  "[2025-09-03T11:11:11.672] POWER: Power save mode: 333 nodes [2025-09-03T11:11:11.672] POWER: Power save mode: 333 nodes [2025-09-03T11:11:": "cluster_power_save_mode",
  "[2025-09-03T11:21:31.748] POWER: Power save mode: 333 nodes [2025-09-03T11:21:31.748] POWER: Power save mode: 333 nodes [2025-09-03T11:21:": "cluster_power_save_mode",
  "[2025-09-03T11:31:51.820] POWER: Power save mode: 333 nodes [2025-09-03T11:31:51.820] POWER: Power save mode: 333 nodes [2025-09-03T11:31:": "cluster_power_save_mode",
  "[2025-09-03T11:36:02.442] _slurm_rpc_submit_batch_job: JobId=959 InitPrio=32608 usec=3722 [2025-09-03T11:36": "job_submit_batch"
}